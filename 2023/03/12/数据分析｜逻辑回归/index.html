<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="分类和回归Logistic Regression 和 Linear Regression Linear Regression：输出一个标量wx+b，是连续值，用以处理回归问题； Logistic Regression：将标量wx+b通过sigmoid函数映射到(0,1)上，并划分一个阈值，大于阈值的分为一类，其他归为另一类，可处理二分分类问题； 对于N分类问题，先得到N组w值不同的wx+b，然后利">
<meta property="og:type" content="article">
<meta property="og:title" content="数据分析｜逻辑回归">
<meta property="og:url" content="https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="Syaoran :)">
<meta property="og:description" content="分类和回归Logistic Regression 和 Linear Regression Linear Regression：输出一个标量wx+b，是连续值，用以处理回归问题； Logistic Regression：将标量wx+b通过sigmoid函数映射到(0,1)上，并划分一个阈值，大于阈值的分为一类，其他归为另一类，可处理二分分类问题； 对于N分类问题，先得到N组w值不同的wx+b，然后利">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-e5118cf19e52cb3ccd832f1f07a1433a_1440w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-7e323dac66fca1f5733c5a417081f436_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-f1d63db4853f166c846626c1bd7b64cd_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-61117b8b8c172e696692a69fe3ce062d_1440w.webp">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-247219e9dabefadfeed7933474eaebbb_1440w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-30b960d57411222d79b41842a3529ea2_1440w.webp">
<meta property="article:published_time" content="2023-03-12T01:02:11.000Z">
<meta property="article:modified_time" content="2023-03-12T01:46:02.469Z">
<meta property="article:author" content="Syaoran Li">
<meta property="article:tag" content="数据分析">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic3.zhimg.com/80/v2-e5118cf19e52cb3ccd832f1f07a1433a_1440w.webp">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>数据分析｜逻辑回归</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Syaoran :)" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E7%BB%9F%E8%AE%A1%E5%AD%A6/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2023/03/12/%E5%95%86%E4%B8%9A%E5%88%86%E6%9E%90%EF%BD%9C%E8%B6%85%E5%B8%82%E5%88%86%E6%9E%90/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&text=数据分析｜逻辑回归"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&is_video=false&description=数据分析｜逻辑回归"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=数据分析｜逻辑回归&body=Check out this article: https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&name=数据分析｜逻辑回归&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&t=数据分析｜逻辑回归"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">分类和回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-%E5%92%8C-Linear-Regression"><span class="toc-number">1.1.</span> <span class="toc-text">Logistic Regression 和 Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Support-Vector-Regression-%E5%92%8C-Support-Vector-Machine"><span class="toc-number">1.2.</span> <span class="toc-text">Support Vector Regression 和 Support Vector Machine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%BA%94%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">相应有哪些常用方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">逻辑回归分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="toc-number">2.1.</span> <span class="toc-text">主要思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.2.</span> <span class="toc-text">原理介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.3.</span> <span class="toc-text">评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87ACC%E3%80%81%E7%9C%9F%E6%AD%A3%E7%8E%87TPR%E5%8F%8A%E5%81%87%E6%AD%A3%E7%8E%87FPR"><span class="toc-number">2.3.2.</span> <span class="toc-text">准确率ACC、真正率TPR及假正率FPR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">2.3.3.</span> <span class="toc-text">ROC曲线</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归的Python实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.</span> <span class="toc-text">提出问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE"><span class="toc-number">3.2.</span> <span class="toc-text">理解数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">构建模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">3.4.</span> <span class="toc-text">模型评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        数据分析｜逻辑回归
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Syaoran Li</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-03-12T01:02:11.000Z" class="dt-published" itemprop="datePublished">2023-03-12</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/Python/" rel="tag">Python</a>, <a class="p-category" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" rel="tag">数据分析</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h1><h2 id="Logistic-Regression-和-Linear-Regression"><a href="#Logistic-Regression-和-Linear-Regression" class="headerlink" title="Logistic Regression 和 Linear Regression"></a>Logistic Regression 和 Linear Regression</h2><ul>
<li>Linear Regression：<strong>输出一个标量wx+b</strong>，是连续值，用以处理回归问题；</li>
<li>Logistic Regression：<strong>将标量wx+b通过sigmoid函数映射到(0,1)上</strong>，并划分一个<strong>阈值</strong>，大于阈值的分为一类，其他归为另一类，可处理<strong>二分分类问题</strong>；</li>
<li>对于N分类问题，先得到<strong>N组w值不同的wx+b</strong>，然后<strong>利用softmax函数归一化</strong>，最后得到<strong>N个类上的概率</strong>，可处理<strong>多分类</strong>问题。</li>
</ul>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic3.zhimg.com/80/v2-e5118cf19e52cb3ccd832f1f07a1433a_1440w.webp"
                      alt="img"
                ></p>
<blockquote>
<p>因此，逻辑回归属于分类方法，其本质是将回归方程映射到了(0,1)上。</p>
</blockquote>
<h2 id="Support-Vector-Regression-和-Support-Vector-Machine"><a href="#Support-Vector-Regression-和-Support-Vector-Machine" class="headerlink" title="Support Vector Regression 和 Support Vector Machine"></a>Support Vector Regression 和 Support Vector Machine</h2><ul>
<li>SVR：输出wx+b，即某个样本点到分类面的距离，是连续值，用以处理<strong>回归</strong>问题；</li>
<li>SVM：将该距离通过sign(·)函数映射，距离为正的样本点为一类，为负的是另一类，故为<strong>分类</strong>问题。</li>
</ul>
<h2 id="相应有哪些常用方法"><a href="#相应有哪些常用方法" class="headerlink" title="相应有哪些常用方法"></a><strong>相应有哪些常用方法</strong></h2><ul>
<li><p>常见的<strong>分类方法：</strong>逻辑回归、决策树分类、KNN(K-近邻)分类、贝叶斯分类、人工神经网络、支持向量机(SVM)等</p>
</li>
<li><p>常见的<strong>回归方法：</strong>线性回归、多项式回归、逐步回归等</p>
</li>
</ul>
<p>（常见的聚类方法：K-Means(K均值)聚类等）</p>
<h1 id="逻辑回归分析"><a href="#逻辑回归分析" class="headerlink" title="逻辑回归分析"></a>逻辑回归分析</h1><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>根据现有数据对决策边界建立回归方程，然后将回归方程映射到分类函数上实现分类。</p>
<h2 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h2><blockquote>
<p>具体见参考[1]</p>
</blockquote>
<p>Logistic回归的原理可以理解为以下四步：</p>
<ol>
<li>利用回归方程表示决策边界</li>
<li>利用Sigmod函数对回归关系进行映射</li>
<li>在得到拟合函数后，利用损失函数来评价模型与实际值之间的差异大小</li>
<li>求出损失函数取得极小值时对应的W，从而得到拟合函数</li>
</ol>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>常见的分类模型性能指标有准确率(accuracy)、召回率(recall)、ROC曲线等。</p>
<h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>包括分类器预测结果：真正TP(true positive)、真负TN(true negative)、假正FP(false positive)、假负FN(false negative)的数量，其中真正和真负均为正确分类的结果。</p>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic3.zhimg.com/80/v2-7e323dac66fca1f5733c5a417081f436_1440w.webp"
                      alt="img"
                ></p>
<p>1代表正类，0代表负类</p>
<h3 id="准确率ACC、真正率TPR及假正率FPR"><a href="#准确率ACC、真正率TPR及假正率FPR" class="headerlink" title="准确率ACC、真正率TPR及假正率FPR"></a>准确率ACC、真正率TPR及假正率FPR</h3><p><strong>注意区别准确率Accuracy与精确率(查准率)Precision区别</strong>：后者是预测与实际均为正类别样本数量 与 预测正样本数量的比值，即 $TP&#x2F;(TP+FP)$ 。一般而言，回归模型中默认的Score都是Accuracy值。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线由变量1-Specificity和Sensitivity绘制，其中横轴1-Specificity&#x3D;假正率(FPR)、纵轴Sensitivity&#x3D;真正率(TPR)，ROC曲线的对角线表示随机猜测，若ROC曲线在对角线下表示分类器性能比随机猜测还差，ROC曲线下的区域面积(area under the curve,AUC)表示分类模型的性能。</p>
<p><strong>意义：</strong></p>
<ul>
<li>有助于选择最佳阈值：ROC曲线越靠近左上角，模型查全率越高，最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少。</li>
<li>可以比较不同学习器的性能：将各个学习器的ROC曲线绘制在同一坐标中，直观比较，越靠近左上角的ROC曲线代表的学习器准确性越高。</li>
<li>AUC同时考虑了学习器对于正例和负例的分类能力，在样本不平衡的情况下，依然能对分类器做出合理评价。</li>
</ul>
<h1 id="逻辑回归的Python实现"><a href="#逻辑回归的Python实现" class="headerlink" title="逻辑回归的Python实现"></a>逻辑回归的Python实现</h1><p>利用Python中<strong>sklearn</strong>包进行逻辑回归分析。</p>
<h2 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a><strong>提出问题</strong></h2><p>根据已有数据探究“学习时长”与“是否通过考试”之间关系，并建立预测模型。</p>
<h2 id="理解数据"><a href="#理解数据" class="headerlink" title="理解数据"></a><strong>理解数据</strong></h2><p>1、导入包和数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.导入包</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.创建数据（学习时间与是否通过考试）</span></span><br><span class="line">dataDict=&#123;<span class="string">&#x27;学习时间&#x27;</span>:<span class="built_in">list</span>(np.arange(<span class="number">0.50</span>,<span class="number">5.50</span>,<span class="number">0.25</span>)),</span><br><span class="line">        <span class="string">&#x27;考试成绩&#x27;</span>:[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line">dataOrDict=OrderedDict(dataDict)</span><br><span class="line">dataDf=pd.DataFrame(dataOrDict)</span><br><span class="line">dataDf.head()</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">       学习时间	考试成绩</span><br><span class="line"><span class="number">0</span>	<span class="number">0.50</span>	<span class="number">0</span></span><br><span class="line"><span class="number">1</span>	<span class="number">0.75</span>	<span class="number">0</span></span><br><span class="line"><span class="number">2</span>	<span class="number">1.00</span>	<span class="number">0</span></span><br><span class="line"><span class="number">3</span>	<span class="number">1.25</span>	<span class="number">0</span></span><br><span class="line"><span class="number">4</span>	<span class="number">1.50</span>	<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>2、查看数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看数据具体形式</span></span><br><span class="line">dataDf.head()</span><br><span class="line"><span class="comment">#查看数据类型及缺失情况</span></span><br><span class="line">dataDf.info()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;</span><br><span class="line">RangeIndex: <span class="number">20</span> entries, <span class="number">0</span> to <span class="number">19</span></span><br><span class="line">Data columns (total <span class="number">2</span> columns):</span><br><span class="line">学习时间    <span class="number">20</span> non-null float64</span><br><span class="line">考试成绩    <span class="number">20</span> non-null int64</span><br><span class="line">dtypes: float64(<span class="number">1</span>), int64(<span class="number">1</span>)</span><br><span class="line">memory usage: <span class="number">400.0</span> <span class="built_in">bytes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看描述性统计信息</span></span><br><span class="line">dataDf.describe()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">        学习时间	 考试成绩</span><br><span class="line">count	<span class="number">20.00000</span> <span class="number">20.000000</span></span><br><span class="line">mean	<span class="number">2.87500</span>	<span class="number">0.500000</span></span><br><span class="line">std	<span class="number">1.47902</span>	<span class="number">0.512989</span></span><br><span class="line"><span class="built_in">min</span>	<span class="number">0.50000</span>	<span class="number">0.000000</span></span><br><span class="line"><span class="number">25</span>%	<span class="number">1.68750</span>	<span class="number">0.000000</span></span><br><span class="line"><span class="number">50</span>%	<span class="number">2.87500</span>	<span class="number">0.500000</span></span><br><span class="line"><span class="number">75</span>%	<span class="number">4.06250</span>	<span class="number">1.000000</span></span><br><span class="line"><span class="built_in">max</span>	<span class="number">5.25000</span>	<span class="number">1.000000</span></span><br></pre></td></tr></table></figure>

<p>3、绘制散点图查看数据分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提取特征和标签</span></span><br><span class="line">exam_X=dataDf[<span class="string">&#x27;学习时间&#x27;</span>]</span><br><span class="line">exam_y=dataDf[<span class="string">&#x27;考试成绩&#x27;</span>]</span><br><span class="line"><span class="comment">#绘制散点图</span></span><br><span class="line">plt.scatter(exam_X,exam_y,color=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;考试数据&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;学习时间&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;考试成绩&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic2.zhimg.com/80/v2-f1d63db4853f166c846626c1bd7b64cd_1440w.webp"
                      alt="img"
                ></p>
<p>从图中可以看出当学习时间高于某一阈值时，一般都能够通过考试，因此我们利用逻辑回归方法建立模型。</p>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a><strong>构建模型</strong></h2><p>1、拆分训练集并利用散点图观察</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.拆分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">exam_X=exam_X.values.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">exam_y=exam_y.values.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">train_X,test_X,train_y,test_y=train_test_split(exam_X,exam_y,train_size=<span class="number">0.8</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集数据大小为&#x27;</span>,train_X.size,train_y.size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集数据大小为&#x27;</span>,test_X.size,test_y.size)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">训练集数据大小为 <span class="number">16</span> <span class="number">16</span></span><br><span class="line">测试集数据大小为 <span class="number">4</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.散点图观察</span></span><br><span class="line">plt.scatter(train_X,train_y,color=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;train data&#x27;</span>)</span><br><span class="line">plt.scatter(test_X,test_y,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;test data&#x27;</span>)</span><br><span class="line"><span class="comment">#plt.plot(test_X,pred_y,color=&#x27;r&#x27;)</span></span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Scores&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic2.zhimg.com/80/v2-61117b8b8c172e696692a69fe3ce062d_1440w.webp"
                      alt="img"
                ></p>
<p>2、导入模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">modelLR=LogisticRegression()</span><br></pre></td></tr></table></figure>

<p>3、训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelLR.fit(train_X,train_y)</span><br></pre></td></tr></table></figure>

<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a><strong>模型评估</strong></h2><p>1、模型评分（即准确率）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">modelLR.score(test_X,test_y)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">0.75</span></span><br></pre></td></tr></table></figure>

<p>2、指定某个点的预测情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#学习时间确定时，预测为0和1的概率分别为多少？</span></span><br><span class="line">modelLR.predict_proba(<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">array([[<span class="number">0.36720478</span>, <span class="number">0.63279522</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#学习时间确定时，预测能否通过考试？</span></span><br><span class="line">modelLR.predict(<span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>3、求出逻辑回归函数并绘制曲线</p>
<p><em>逻辑回归函数</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先求出回归函数y=a+bx，再代入逻辑函数中pred_y=1/(1+np.exp(-y))</span></span><br><span class="line">b=modelLR.coef_</span><br><span class="line">a=modelLR.intercept_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;该模型对应的回归函数为:1/(1+exp-(%f+%f*x))&#x27;</span>%(a,b))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">该模型对应的回归函数为:<span class="number">1</span>/(<span class="number">1</span>+exp-(-<span class="number">1.527106</span>+<span class="number">0.690444</span>*x))</span><br></pre></td></tr></table></figure>

<p><em>逻辑回归曲线</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画出相应的逻辑回归曲线</span></span><br><span class="line">plt.scatter(train_X,train_y,color=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;train data&#x27;</span>)</span><br><span class="line">plt.scatter(test_X,test_y,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;test data&#x27;</span>)</span><br><span class="line">plt.plot(test_X,<span class="number">1</span>/(<span class="number">1</span>+np.exp(-(a+b*test_X))),color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.plot(exam_X,<span class="number">1</span>/(<span class="number">1</span>+np.exp(-(a+b*exam_X))),color=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Scores&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic4.zhimg.com/80/v2-247219e9dabefadfeed7933474eaebbb_1440w.webp"
                      alt="img"
                ></p>
<p>黄色S形曲线代表利用数据集拟合得到的逻辑回归曲线</p>
<p>4、得到模型混淆矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="comment">#数值处理</span></span><br><span class="line">pred_y=<span class="number">1</span>/(<span class="number">1</span>+np.exp(-(a+b*test_X)))</span><br><span class="line">pred_y=pd.DataFrame(pred_y)</span><br><span class="line">pred_y=<span class="built_in">round</span>(pred_y,<span class="number">0</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"><span class="comment">#混淆矩阵</span></span><br><span class="line">confusion_matrix(test_y.astype(<span class="built_in">str</span>),pred_y.astype(<span class="built_in">str</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p>从混淆矩阵可以看出：</p>
<ul>
<li>该模型的准确率ACC为0.75；</li>
<li>真正率TPR和假正率FPR分别为0.50和0.00，说明该模型对负例的甄别能力更强（如果数据量更多，该指标更有说服性，而本案例中数据较少，因此受随机影响较大）。</li>
</ul>
<p>5、绘制模型ROC曲线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc  <span class="comment">###计算roc和auc</span></span><br><span class="line"><span class="comment"># Compute ROC curve and ROC area for each class</span></span><br><span class="line">fpr,tpr,threshold = roc_curve(test_y, pred_y) <span class="comment">###计算真正率和假正率</span></span><br><span class="line">roc_auc = auc(fpr,tpr) <span class="comment">###计算auc的值</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">         lw=lw, label=<span class="string">&#x27;ROC curve (area = %0.2f)&#x27;</span> % roc_auc) <span class="comment">###假正率为横坐标，真正率为纵坐标做曲线</span></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver operating characteristic example&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img  
                     lazyload
                     alt="image"
                     data-src="https://pic3.zhimg.com/80/v2-30b960d57411222d79b41842a3529ea2_1440w.webp"
                      alt="img"
                ></p>
<p>红线以下部分面积等于0.75，即误将一个反例划分为正例。（此处AUC的面积与模型准确率刚好一致，并非总是相等的）</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>理解回归与分类的关系：两者既有区别（三个维度理解），又有联系（将回归方程映射到分类函数）；</li>
<li>逻辑回归的参数及其含义：准确率（ACC：模型预测准确度）、真正率（TPR：模型将正例分类正确的能力）、假正率（FPR：模型将负例分类正确的能力）、ROC曲线（可以反映模型正确识别正&#x2F;负例的能力，也可利用AUC反映模型准确度）</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48981694" >机器学习之利用Python进行逻辑回归分析<i class="fas fa-external-link-alt"></i></a></li>
</ol>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">分类和回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-%E5%92%8C-Linear-Regression"><span class="toc-number">1.1.</span> <span class="toc-text">Logistic Regression 和 Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Support-Vector-Regression-%E5%92%8C-Support-Vector-Machine"><span class="toc-number">1.2.</span> <span class="toc-text">Support Vector Regression 和 Support Vector Machine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%BA%94%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">相应有哪些常用方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">逻辑回归分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="toc-number">2.1.</span> <span class="toc-text">主要思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.2.</span> <span class="toc-text">原理介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.3.</span> <span class="toc-text">评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87ACC%E3%80%81%E7%9C%9F%E6%AD%A3%E7%8E%87TPR%E5%8F%8A%E5%81%87%E6%AD%A3%E7%8E%87FPR"><span class="toc-number">2.3.2.</span> <span class="toc-text">准确率ACC、真正率TPR及假正率FPR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">2.3.3.</span> <span class="toc-text">ROC曲线</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归的Python实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98"><span class="toc-number">3.1.</span> <span class="toc-text">提出问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE"><span class="toc-number">3.2.</span> <span class="toc-text">理解数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">构建模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">3.4.</span> <span class="toc-text">模型评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&text=数据分析｜逻辑回归"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&is_video=false&description=数据分析｜逻辑回归"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=数据分析｜逻辑回归&body=Check out this article: https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&title=数据分析｜逻辑回归"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&name=数据分析｜逻辑回归&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://artemis727.github.io/2023/03/12/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/&t=数据分析｜逻辑回归"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2023
    Syaoran Li
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
