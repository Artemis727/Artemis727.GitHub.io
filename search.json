[{"title":"Data Science R Basic","url":"/2023/02/09/Data-Science-R-Basic/","content":"Words &amp; Expressions\nin a nested way åµŒå¥—\n\nconcatenate è¿æ¥ c() -&gt; all items in â€˜()â€™ to a vector\n\n\nWhy use â€˜[[â€™ instead of â€˜[â€™ to access variables?\\We extract the population like this:\np &lt;- murders$population\n\nIf you instead try to access a column with just one bracket,\nmurders[&quot;population&quot;]\n\n&#x3D;&#x3D;R returns a subset of the original data frame containing just this column. This new object will be of class data.frame rather than a vector.&#x3D;&#x3D; To access the column itself you need to use either the $ accessor or the double square brackets [[:\nmurders[[&quot;population&quot;]]\n\n\n\nR- sorting functions\nsort()\n# sort()æ’åºï¼Œæ’åºç»“æœä¸å¯é€†è½¬# é»˜è®¤æ˜¯å‡åº# decreasingä¸ºTRUEï¼Œè¡¨ç¤ºé™åº# decreasingä¸ºFALSEï¼Œè¡¨ç¤ºå‡åº#æ’åºåå¹¶ä¸ä¼šä¿®æ”¹åŸå¯¹è±¡çš„å€¼#ç¤ºä¾‹å¦‚ä¸‹ï¼š&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; #sort()é»˜è®¤ä¸ºä»å°åˆ°å¤§ï¼ˆå‡åºï¼‰æ’åºï¼Œç­‰åŒäºdecreasing=FALSE&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; sort(a,decreasing = F) [1]  3  4  5  6  7  9 10 13 16 22&gt; #decreasing=TRUE,ä¸ºä»å¤§åˆ°å°ï¼ˆé™åºï¼‰æ’åº&gt; sort(a,decreasing = T) [1] 22 16 13 10  9  7  6  5  4  3#æ’åºå¹¶ä¸ä¼šä¿®æ”¹åŸå¯¹è±¡çš„å€¼ï¼Œaä»ä¸ºåŸæ¥æœªæ’åºçš„a&gt; a [1]  3  9 16  6  7  4 22  5 10 13\n\n\n\nrank()\nç”¨æ³•ï¼šrank(a)å‡½æ•°è¯´æ˜ï¼šæŒ‡å‡ºå½“å‰å‘é‡ä¸­å„å…ƒç´ å¤§å°çš„æ’åï¼Œé»˜è®¤å‡åºå‡½æ•°è¿˜æœ‰å…¶ä»–çš„å‚æ•°ï¼šrank(x &#x3D; data, na.last &#x3D; TRUE)x è¡¨ç¤ºå¾…æ’åºçš„å‘é‡na.last è¡¨ç¤ºæ˜¯å¦æ’åºæ—¶æ˜¯å¦å°†NAæ”¾åœ¨æœ€åé¢ï¼Œé»˜è®¤å¿½ç•¥NA\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; order(a) [1]  1  6  8  4  5  2  9 10  3  7#è¯´æ˜ï¼šåœ¨å‘é‡aä¸­ï¼Œ3æ˜¯ç¬¬ä¸€å°çš„æ•°ï¼Œä½ç½®ä¸‹æ ‡ä¸º1ï¼›4æ˜¯ç¬¬äºŒå°çš„æ•°ï¼Œä½ç½®ä¸‹æ ‡ä¸º6ï¼›æœ€å¤§çš„æ•°æ˜¯22ï¼Œä½ç½®ä¸‹æ ‡ä¸º7#a[order(a)] ç­‰åŒäºsort(a)&gt; a[order(a)]  [1]  3  4  5  6  7  9 10 13 16 22\n\n\n\norder()\nè¯´æ˜ï¼šè¿”å›çš„å€¼è¡¨ç¤ºä½ç½®ï¼Œé»˜è®¤æ˜¯å‡åºï¼Œä¾æ¬¡å¯¹åº”çš„æ˜¯å‘é‡çš„æœ€å°å€¼ã€æ¬¡å°å€¼ã€ç¬¬ä¸‰å°å€¼â€¦æœ€å¤§å€¼\nç”¨æ³•ï¼šorder(a), aä¸ºè¦æ’åºçš„å‘é‡order(â€¦ &#x3D; data, na.last &#x3D; TRUE,decreasing &#x3D; TRUE)â€¦ è¡¨ç¤ºå¾…æ’åºå‘é‡na.last è¡¨ç¤ºæ—¶å€™å°†NAå€¼æ”¾åœ¨æœ€åé¢ï¼ˆé»˜è®¤æ’åºå¿½ç•¥NAï¼‰decreasing è¡¨ç¤ºæ˜¯å¦æŒ‰ç…§é™åºæ’åºï¼Œé»˜è®¤å‡åºã€‚\n&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; rank(a) [1]  1  6  9  4  5  2 10  3  7  8 #è¯´æ˜ï¼šå‘é‡aä¸­çš„ç¬¬ä¸€ä¸ªæ•°ä¸º3ï¼Œæ˜¯æœ€å°çš„ï¼Œæ•…æ’åä¸º1ï¼›ç¬¬äºŒä¸ªæ•°æ˜¯9ï¼Œæ˜¯ç¬¬å…­å°çš„æ•°ï¼Œæ’åä¸º6\n\n\n\næ¯”è¾ƒ\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) #å°†a**ä»å°åˆ°å¤§æ’åºå¹¶åˆ—å‡º** [1]  3  4  5  6  7  9 10 13 16 22&gt; order(a) #è¿”å›ä»å°åˆ°å¤§çš„æ•°çš„**ä½ç½®ä¸‹æ ‡**ï¼Œa[order(a)]=sort(a) [1]  1  6  8  4  5  2  9 10  3  7&gt; rank(a) #è¿”å›aä¸­**æ¯ä¸ªæ•°çš„æ’åï¼ˆä»å°åˆ°å¤§ï¼‰** [1]  1  6  9  4  5  2 10  3  7  8&gt; a [1]  3  9 16  6  7  4 22  5 10 13#æ³¨æ„ï¼šç»è¿‡sort()ã€order()ã€rank()æ’åºåï¼Œaä¸æ”¹å˜\n\norderå’Œrankçš„åŒºåˆ«åœ¨äºä¸€ä¸ªè¿”å›çš„æ˜¯indexï¼Œä¸€ä¸ªæ˜¯rankï¼Œindexä¸ä¸€å®šæ˜¯æ•°å­—ã€‚\n\n\n","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["R"]},{"title":"Kaggle|æˆ¿ä»·é¢„æµ‹","url":"/2023/02/28/HousePricing2/","content":"NotesProfileReport [1]import ydata_profiling as ppfimport pandas as pddf_train = pd.read_csv(&#x27;train.csv&#x27;)ppf.ProfileReport(df_train)\n\nğŸ‘§ğŸ» ydata_profiling å’Œ pandas_profiling?\nğŸ‘§ğŸ» ProfileReport() è¿è¡Œå¡é¡¿ï¼Ÿ\nğŸ¤– è¿™å¯èƒ½ä¸ä¸åŒæµè§ˆå™¨å¯¹äº Jupyter Notebook çš„æ”¯æŒå’Œæ€§èƒ½ä¼˜åŒ–æœ‰å…³ã€‚Jupyter Notebook æ˜¯ä¸€ä¸ªåŸºäº Web çš„åº”ç”¨ç¨‹åºï¼Œå®ƒçš„æ€§èƒ½å’Œç¨³å®šæ€§å¯èƒ½ä¼šå—åˆ°ä¸åŒæµè§ˆå™¨çš„å½±å“ã€‚\nSafari æµè§ˆå™¨åœ¨è¿è¡Œ Jupyter Notebook æ—¶å¯èƒ½ä¼šæ›´åŠ æµç•…ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸º Safari æµè§ˆå™¨å…·æœ‰æ›´å¥½çš„ Web æŠ€æœ¯æ”¯æŒå’Œæ€§èƒ½ä¼˜åŒ–ã€‚åŒæ—¶ï¼ŒSafari æµè§ˆå™¨åœ¨ Mac OS X ç³»ç»Ÿä¸­æ˜¯é»˜è®¤çš„æµè§ˆå™¨ï¼Œå¯èƒ½ä¸æ“ä½œç³»ç»Ÿä¹‹é—´çš„å…¼å®¹æ€§æ›´å¥½ã€‚\nå¦ä¸€æ–¹é¢ï¼ŒChrome æµè§ˆå™¨åœ¨è¿è¡Œ Jupyter Notebook æ—¶å¯èƒ½ä¼šå‡ºç°å¡é¡¿é—®é¢˜ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸º Chrome æµè§ˆå™¨å…·æœ‰æ›´é«˜çš„ç³»ç»Ÿèµ„æºå ç”¨ç‡ï¼Œä»¥åŠæ›´å¤šçš„æ’ä»¶å’Œæ‰©å±•åŠŸèƒ½ï¼Œå¯¼è‡´å…¶åœ¨è¿è¡Œ Jupyter Notebook æ—¶æ›´å®¹æ˜“å‡ºç°æ€§èƒ½é—®é¢˜ã€‚\nå¦‚æœä½ åœ¨ä½¿ç”¨ Chrome æµè§ˆå™¨æ—¶é‡åˆ°äº†æ€§èƒ½é—®é¢˜ï¼Œä½ å¯ä»¥å°è¯•ä¼˜åŒ–æµè§ˆå™¨çš„è®¾ç½®æˆ–å‡å°‘æµè§ˆå™¨æ’ä»¶å’Œæ‰©å±•çš„ä½¿ç”¨ï¼Œæˆ–è€…ä½¿ç”¨å…¶ä»–æµè§ˆå™¨æ¥è¿è¡Œ Jupyter Notebookï¼Œä¾‹å¦‚ Safariã€Firefox ç­‰ç­‰ã€‚åŒæ—¶ï¼Œä½ ä¹Ÿå¯ä»¥è€ƒè™‘åœ¨æœ¬åœ°ä½¿ç”¨ Jupyter Notebookï¼Œè€Œä¸æ˜¯åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ Jupyter Notebookï¼Œè¿™å¯ä»¥æä¾›æ›´å¥½çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚\nEncoding\næ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œobjectç±»çš„ç‰¹å¾ä¼šæ ¹æ®ç±»åˆ«ä½¿ç”¨ä¸åŒçš„ç¼–ç æ–¹å¼å¤„ç†ï¼ˆlabel encodingå’ŒOne-hot encodingï¼‰\n\nLabel encodingOne-hot encodingReference[1] kaggleæ¡ˆä¾‹-Pythonå®ç°æˆ¿ä»·é¢„æµ‹-å®Œæ•´åˆ†ææµç¨‹\n[2] [Kaggleç«èµ›ä¸¨æˆ¿ä»·é¢„æµ‹ï¼ˆHouse Pricesï¼‰][https://zhuanlan.zhihu.com/p/137076292]\n[3] \n","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["Python","kaggle","data prediction"]},{"title":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?","url":"/2023/02/08/How-is-Hadley-Wickham-able-to-contribute-so-much-to-R-particularly-in-the-form-of-packages/","content":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?\nDavid Robinson:\nFrom following Hadleyâ€™s work, it seems to me that along with being an exceptional programmer and data scientist, and having the advantage of developing R packages as part of his job, Hadley follows a few strategies that serve as useful wisdom for all developers:\n\nHe writes packages that make himself more productive. Three of Hadleyâ€™s popular packages, devtools, Roxygen2, and testthat, make it very easy to (respectively) develop, document and test R packages. He recognized that the time spent to create and maintain those was small compared to the time it would save him (and others!) in developing future packages. This extends beyond those package development tools: packages like stringr and lubridate are designed to make working with strings and dates easier. This also extends beyond his own packages: he takes advantage of packages like Rcpp (http://www.rcpp.org/) that make writing R C++ extensions fast and intuitive.\nHe takes full advantage of social coding. Heâ€™s a prolific GitHub user (hadley (Hadley Wickham)), which makes it efficient to receive and respond to bug reports and feature requests, and to collaborate with others (for instance, with Romain Francois on dplyr).\nHe works to simplify his packages rather than complicate them. In his announcement of the tidyr package (Introducing tidyr) he notes that â€œJust as reshape2 did less than reshape, tidyr does less than reshape2.â€ When packages are simpler (doing a few things well instead of hundreds of things poorly), theyâ€™re easier to develop and maintain.\n\n**Hadley Wickham: **\nI like Davidâ€™s answer, but here are a few more thoughts from a personal perspective ;)\n\nWriting. I have worked really hard to build a solid writing habit - &#x3D;&#x3D;I try and write for 60-90 minutes every morning. Itâ€™s the first thing I do after I get out of bed.&#x3D;&#x3D; I think writing is really helpful to me for a few reasons. First, &#x3D;&#x3D;I often use my writing as a reference&#x3D;&#x3D; - I donâ€™t program in C++ every day, so Iâ€™m constantly referring to @Rcpp every time I do. Writing also makes me aware of gaps in my knowledge and my tools, and filling in those gaps tends to make me more efficient at tackling new problems.\nReading. I read a lot. I follow about 300 blogs, and keep a pretty close eye on the R tags on Twitter and Stack Overflow. I donâ€™t read most things deeply - &#x3D;&#x3D;the majority of content I only briefly skim. But this wide exposure helps me keep up with changes in technology, interesting new programming languages, and what others are doing with data.&#x3D;&#x3D; Itâ€™s also helpful that if when youâ€™re tackling a new problem you can recognise the basic name - then googling for it will suggest possible solutions. If you donâ€™t know the name of a problem, itâ€™s very hard to research it.\nChunking. Context-switching is expensive, so if I worked on many packages at the same time, Iâ€™d never get anything done. Instead, at any point in time, most of my packages are lying fallow, steadily accumulating issues and ideas for new feature. Once a critical mass has accumulated, Iâ€™ll spend a couple of days on the package.\n\nFinally, itâ€™s hard to over-emphasise the impact that working full-time on R makes. Since Iâ€™ve left Rice, I now spend well over 90% of my work time thinking about and programming in R. This has a compounding effect because as I built better tools (cognitive and computational) it becomes even easier to build new tools. I can create a new package in seconds, and I have many techniques on-hand (in-brain) for solving new problems.\n","categories":["Quora"]},{"title":"SQL-åˆ·é¢˜ç¬”è®°","url":"/2023/02/07/SQL%20%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","content":"1. æŸ¥è¯¢åˆ—åˆ—æŸ¥è¯¢ã€å¤šåˆ—æŸ¥è¯¢ã€é™åˆ¶è¿”å›æ•°é‡ã€inã€likeã€èšåˆå‡½æ•°ã€‚\n# æŸ¥è¯¢æ‰€æœ‰åˆ—SELECT * FROM user_profile# æŸ¥è¯¢å¤šåˆ—SELECT device_id, gender, age, university FROM user_profile# æŸ¥è¯¢ç»“æœå»é‡SELECT DISTINCT university FROM user_profile# æŸ¥è¯¢ç»“æœé™åˆ¶è¿”å›è¡Œæ•°SELECT device_id FROM user_profile LIMIT 2# æŸ¥è¯¢åé‡å‘½åSELECT device_id FROM user_profile AS user_infos_example LIMIT 2# å¦å®šæŸ¥è¯¢select device_id, gender, age, universityfrom user_profilewhere not university = &#x27;å¤æ—¦å¤§å­¦&#x27;# è¿‡æ»¤ç©ºå€¼select device_id, gender, age, universityfrom user_profilewhere not age is null# inselect device_id, gender, age, university, gpa from user_profilewhere university in (&#x27;åŒ—äº¬å¤§å­¦&#x27;, &#x27;å¤æ—¦å¤§å­¦&#x27;, &#x27;å±±ä¸œå¤§å­¦&#x27;)# likeselect device_id, age, universityfrom user_profilewhere university like &#x27;%åŒ—äº¬%&#x27;# max() min()select max(gpa)from user_praofilewhere university = &#x27;å¤æ—¦å¤§å­¦&#x27;# avg() count()select count(gender) as male_num, avg(gpa) as avg_gpafrom user_profilewhere gender = &#x27;male&#x27;# havingselect university, avg(question_cnt) avg_question_cnt, avg(answer_cnt) avg_answer_cntfrom user_profilegroup by universityhaving avg_question_cnt &lt; 5 or avg_answer_cnt &lt; 20# ç”Ÿæˆæ–°å­—æ®µåä¸èƒ½ç”¨whereè¦ç”¨having\n\n\n\n2. æ¶‰åŠåˆ°å¤šä¸ªè¡¨çš„æƒ…å†µselect university, (count(q.question_id)/count(distinct(q.device_id))) avg_answer_cntfrom user_profile ujoin question_practice_detail qon u.device_id = q.device_idgroup by universityorder by university asc\n\nselect u.university, q.difficult_level, count(qp.question_id)/count(distinct qp.device_id) avg_answer_cntfrom user_profile u, question_practice_detail qp, question_detail qwhere u.university = &#x27;å±±ä¸œå¤§å­¦&#x27; and u.device_id = qp.device_id and q.question_id = qp.question_idgroup by u.university, difficult_levelorder by avg_answer_cnt\n\n\n\nSQL25 æŸ¥æ‰¾å±±ä¸œå¤§å­¦æˆ–è€…æ€§åˆ«ä¸ºç”·ç”Ÿçš„ä¿¡æ¯é¢˜ç›®ï¼šç°åœ¨è¿è¥æƒ³è¦åˆ†åˆ«æŸ¥çœ‹å­¦æ ¡ä¸ºå±±ä¸œå¤§å­¦æˆ–è€…æ€§åˆ«ä¸ºç”·æ€§çš„ç”¨æˆ·çš„device_idã€genderã€ageå’Œgpaæ•°æ®ï¼Œè¯·å–å‡ºç›¸åº”ç»“æœï¼Œç»“æœä¸å»é‡ã€‚\nç¤ºä¾‹ï¼šuser_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\nåŒ—äº¬å¤§å­¦\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\nå¤æ—¦å¤§å­¦\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\nåŒ—äº¬å¤§å­¦\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\næµ™æ±Ÿå¤§å­¦\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\nå±±ä¸œå¤§å­¦\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\nå±±ä¸œå¤§å­¦\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\nå¤æ—¦å¤§å­¦\n3.6\n9\n6\n52\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼ˆæ³¨æ„è¾“å‡ºçš„é¡ºåºï¼Œå…ˆè¾“å‡ºå­¦æ ¡ä¸ºå±±ä¸œå¤§å­¦å†è¾“å‡ºæ€§åˆ«ä¸ºç”·ç”Ÿçš„ä¿¡æ¯ï¼‰ï¼š\n\n\n\ndevice_id\ngender\nage\ngpa\n\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n2138\nmale\n21\n3.4\n\n\n3214\nmale\nNone\n4\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n4321\nmale\n28\n3.6\n\n\nselect device_id, gender, age, gpafrom user_profilewhere university = &#x27;å±±ä¸œå¤§å­¦&#x27;union all select device_id, gender, age, gpafrom user_profilewhere gender = &#x27;male&#x27;\n\n\n\n3. ifSQL26 è®¡ç®—25å²ä»¥ä¸Šå’Œä»¥ä¸‹çš„ç”¨æˆ·æ•°é‡é¢˜ç›®ï¼šç°åœ¨è¿è¥æƒ³è¦å°†ç”¨æˆ·åˆ’åˆ†ä¸º25å²ä»¥ä¸‹å’Œ25å²åŠä»¥ä¸Šä¸¤ä¸ªå¹´é¾„æ®µï¼Œåˆ†åˆ«æŸ¥çœ‹è¿™ä¸¤ä¸ªå¹´é¾„æ®µç”¨æˆ·æ•°é‡\næœ¬é¢˜æ³¨æ„ï¼šageä¸ºnull ä¹Ÿè®°ä¸º 25å²ä»¥ä¸‹\nç¤ºä¾‹ï¼šuser_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\nåŒ—äº¬å¤§å­¦\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\nå¤æ—¦å¤§å­¦\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\nåŒ—äº¬å¤§å­¦\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\næµ™æ±Ÿå¤§å­¦\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\nå±±ä¸œå¤§å­¦\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\nå±±ä¸œå¤§å­¦\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\nå¤æ—¦å¤§å­¦\n3.6\n9\n6\n52\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š\n\n\n\nage_cut\nnumber\n\n\n\n25å²ä»¥ä¸‹\n4\n\n\n25å²åŠä»¥ä¸Š\n3\n\n\nselect age_cut, count(device_id) numberfrom(select if(age &gt;= 25, &#x27;25å²åŠä»¥ä¸Š&#x27;, &#x27;25å²ä»¥ä¸‹&#x27;) as age_cut, device_id from user_profile) t1group by age_cut\n\n\n\n4. caseSQL27 æŸ¥çœ‹ä¸åŒå¹´é¾„æ®µçš„ç”¨æˆ·æ˜ç»†é¢˜ç›®ï¼šç°åœ¨è¿è¥æƒ³è¦å°†ç”¨æˆ·åˆ’åˆ†ä¸º20å²ä»¥ä¸‹ï¼Œ20-24å²ï¼Œ25å²åŠä»¥ä¸Šä¸‰ä¸ªå¹´é¾„æ®µï¼Œåˆ†åˆ«æŸ¥çœ‹ä¸åŒå¹´é¾„æ®µç”¨æˆ·çš„æ˜ç»†æƒ…å†µï¼Œè¯·å–å‡ºç›¸åº”æ•°æ®ã€‚ï¼ˆæ³¨ï¼šè‹¥å¹´é¾„ä¸ºç©ºè¯·è¿”å›å…¶ä»–ã€‚ï¼‰\nç¤ºä¾‹ï¼šuser_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\nåŒ—äº¬å¤§å­¦\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\nå¤æ—¦å¤§å­¦\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\nåŒ—äº¬å¤§å­¦\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\næµ™æ±Ÿå¤§å­¦\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\nå±±ä¸œå¤§å­¦\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\nå±±ä¸œå¤§å­¦\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\nå¤æ—¦å¤§å­¦\n3.6\n9\n6\n52\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š\n\n\n\ndevice_id\ngender\nage_cut\n\n\n\n2138\nmale\n20-24å²\n\n\n3214\nmale\nå…¶ä»–\n\n\n6543\nfemale\n20-24å²\n\n\n2315\nfemale\n20-24å²\n\n\n5432\nmale\n25å²åŠä»¥ä¸Š\n\n\n2131\nmale\n25å²åŠä»¥ä¸Š\n\n\n4321\nmale\n25å²åŠä»¥ä¸Š\n\n\nselect device_id, gender, case    when age &lt; 20 then &#x27;20å²ä»¥ä¸‹&#x27;    when age &lt; 25 then &#x27;20-24å²&#x27;    when age &gt;= 25 then &#x27;25å²åŠä»¥ä¸Š&#x27;    else &#x27;å…¶ä»–&#x27;end age_cutfrom user_profile\n\n\n\n5. day()\nday()\n\nmonth()\n\nyear()\n\n\n6. ç”¨æˆ·ç•™å­˜ç‡è®¡ç®—SQL-è®¡ç®—ç”¨æˆ·æ¬¡æ—¥ç•™å­˜ç‡.md\n7. å­—ç¬¦ä¸²ç›¸å…³SQL-å­—ç¬¦ä¸²ç›¸å…³.md\n","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["SQL"]},{"title":"SQL-å­—ç¬¦ä¸²ç›¸å…³","url":"/2023/02/07/SQL-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3/","content":"SQL30 ç»Ÿè®¡æ¯ç§æ€§åˆ«çš„äººæ•°æè¿°é¢˜ç›®ï¼šç°åœ¨è¿è¥ä¸¾åŠäº†ä¸€åœºæ¯”èµ›ï¼Œæ”¶åˆ°äº†ä¸€äº›å‚èµ›ç”³è¯·ï¼Œè¡¨æ•°æ®è®°å½•å½¢å¼å¦‚ä¸‹æ‰€ç¤ºï¼Œç°åœ¨è¿è¥æƒ³è¦ç»Ÿè®¡æ¯ä¸ªæ€§åˆ«çš„ç”¨æˆ·åˆ†åˆ«æœ‰å¤šå°‘å‚èµ›è€…ï¼Œè¯·å–å‡ºç›¸åº”ç»“æœ\nç¤ºä¾‹ï¼šuser_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;url&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;urlsydney\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š\n\n\n\ngender\nnumber\n\n\n\nmale\n2\n\n\nfemale\n3\n\n\n\nselect substring_index(profile, &#x27;,&#x27;, -1) as gender, count(device_id)from user_submitgroup by gender\n\n\n\nSQL31 æå–åšå®¢URLä¸­çš„ç”¨æˆ·åæè¿°é¢˜ç›®ï¼šå¯¹äºç”³è¯·å‚ä¸æ¯”èµ›çš„ç”¨æˆ·ï¼Œblog_urlå­—æ®µä¸­urlå­—ç¬¦åçš„å­—ç¬¦ä¸²ä¸ºç”¨æˆ·ä¸ªäººåšå®¢çš„ç”¨æˆ·åï¼Œç°åœ¨è¿è¥æƒ³è¦æŠŠç”¨æˆ·çš„ä¸ªäººåšå®¢ç”¨æˆ·å­—æ®µæå–å‡ºå•ç‹¬è®°å½•ä¸ºä¸€ä¸ªæ–°çš„å­—æ®µï¼Œè¯·å–å‡ºæ‰€éœ€æ•°æ®ã€‚\nç¤ºä¾‹ï¼šuser_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bisdgboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;dkittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;ur&#x2F;tigaer\n\n\n4321\n171 cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š\n\n\n\ndevice_id\nuser_name\n\n\n\n2138\nbisdgboy777\n\n\n3214\ndkittycc\n\n\n6543\ntigaer\n\n\n4321\nuhsksd\n\n\n2131\nsydney\n\n\n\næå–æŸä¸ªå­—ç¬¦ä¸€èˆ¬æœ‰å››ç§åšæ³•ï¼š\ntrim()\ntrim() æ˜¯ç›´æ¥æ›´æ”¹ç›¸åŒæ ¼å¼çš„ä¸€åˆ—ï¼Œåˆ é™¤è¿™ä¸€åˆ—å†…å®¹ä¸­çš„ç»Ÿä¸€éƒ¨åˆ†ï¼Œç„¶åé‡å‘½åï¼š\nselect device_id, trim(&#x27;http:/url/&#x27; from blog_url) as user_namefrom user_submit\n\nâ€‹\t\n\nsubstring_index()\nsubstring_index() æ˜¯å°†å­—ç¬¦ä¸²åˆ‡å‰²ï¼Œ1è¡¨ç¤ºä¿ç•™å­—ç¬¦ä¸²çš„å·¦è¾¹ğŸ‘ˆï¼Œ-1è¡¨ç¤ºä¿ç•™å­—ç¬¦ä¸²çš„å³è¾¹ğŸ‘‰ï¼š\nselect device_id, substring_index(blog_url, &#x27;/url/&#x27;, -1) as user_namefrom user_submit\n\nè¿˜æœ‰ä¸€ä¸ªç”¨æ³•æ˜¯è®¡æ•°ï¼š\nSUBSTRING_INDEX(str,delim,count) è¿”å›ä»å­—ç¬¦ä¸²stråˆ†éš”ç¬¦ delim åœ¨è®¡æ•°å‘ç”Ÿå‰çš„å­å­—ç¬¦ä¸²ã€‚å¦‚æœè®¡æ•°æ˜¯æ­£çš„ï¼Œåˆ™è¿”å›ä¸€åˆ‡åˆ°æœ€ç»ˆå®šç•Œç¬¦(ä»å·¦è¾¹ç®—èµ·)çš„å·¦ä¾§ã€‚å¦‚æœcountæ˜¯è´Ÿæ•°ï¼Œåˆ™è¿”å›ä¸€åˆ‡æœ€ç»ˆå®šç•Œç¬¦(ä»å³è¾¹ç®—èµ·)çš„å³ä¾§ã€‚SUBSTRING_INDEX() æœå¯»åœ¨delimæ—¶è¿›è¡ŒåŒºåˆ†å¤§å°å†™çš„åŒ¹é…ã€‚\nSELECT SUBSTRING_INDEX(&#x27;www.somewebsite.com&#x27;,&#x27;.&#x27;,2);\n\nOutput: &#39;www.somewebsite&#39;\n\nsubstr()\nsubstr() æ˜¯ç”¨å…·ä½“ä½ç½®ï¼ˆæ•°å­—ï¼‰æ¥è¡¨ç¤ºä»å“ªå¼€å§‹æˆªå–çš„ï¼Œå‚æ•°é‡Œè¿˜åŒ…æ‹¬æˆªå–çš„é•¿åº¦ï¼š\nselect device_id, substr(blog_url, 11, length(blog_url)-10) as user_namefrom user_submit\n\n\n\nreplace()\nreplace() å°±æ˜¯æ›¿æ¢å‡½æ•°ï¼š\nsleect device_id, replace(blog_url, &#x27;http:/url/&#x27;, &#x27;&#x27;) as user_namefrom user_submit\n\nSQL32 æˆªå–å‡ºå¹´é¾„æè¿°é¢˜ç›®ï¼šç°åœ¨è¿è¥ä¸¾åŠäº†ä¸€åœºæ¯”èµ›ï¼Œæ”¶åˆ°äº†ä¸€äº›å‚èµ›ç”³è¯·ï¼Œè¡¨æ•°æ®è®°å½•å½¢å¼å¦‚ä¸‹æ‰€ç¤ºï¼Œç°åœ¨è¿è¥æƒ³è¦ç»Ÿè®¡æ¯ä¸ªå¹´é¾„çš„ç”¨æˆ·åˆ†åˆ«æœ‰å¤šå°‘å‚èµ›è€…ï¼Œè¯·å–å‡ºç›¸åº”ç»“æœ\nç¤ºä¾‹ï¼šuser_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\næ ¹æ®ç¤ºä¾‹ï¼Œä½ çš„æŸ¥è¯¢åº”è¿”å›ä»¥ä¸‹ç»“æœï¼š\n\n\n\nage\nnumber\n\n\n\n27\n1\n\n\n26\n1\n\n\n25\n1\n\n\n23\n1\n\n\n22\n1\n\n\nselect substring_index(substring_index(profile, &#x27;,&#x27;, -2), &#x27;,&#x27;, 1) as age, count(device_id)from user_submitgroup by age\n\n","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["SQL","å­—ç¬¦ä¸²"]},{"title":"Kaggle|House Pricing|Comprehensive data exploration with python","url":"/2023/02/23/HousePricing/","content":"\nFrom kaggle: Comprehensive data exploration with python\n\nNotesååº¦(skewness)å’Œå³°åº¦(kurtosisï¼‰\nskewness è¡¡é‡æ•°æ®åˆ†å¸ƒçš„éå¯¹ç§°ç¨‹åº¦\næ­£æ€åˆ†å¸ƒ skewness &#x3D; 0\nå³ååˆ†å¸ƒ skewness &gt; 0\nå·¦ååˆ†å¸ƒ skewness &lt; 0\n\n\n\n\n\nkurtosis è¡¨ç¤ºæ¦‚ç‡å¯†åº¦æ›²çº¿çš„å³°å€¼é«˜ä½ï¼ˆå³°çš„å°–åº¦ï¼‰\n\næ­£æ€åˆ†å¸ƒï¼ˆ&#x3D; 3ï¼‰\nåšå°¾ï¼ˆ&gt; 3ï¼‰æœ€ä½\nç˜¦å°¾ï¼ˆ&lt; 3ï¼‰æœ€å°–\n\n\n\n\nheaptmapæ€ä¹ˆçœ‹ç›¸å…³æ€§ä¸­é—´å¯¹è§’çº¿æ°¸è¿œæ˜¯æœ€ç›¸å…³çš„\næ ‡å‡†åŒ–from sklearn.preprocessing import StandardScaler\nğŸ¤–ï¸ å½“è¿›è¡Œå•ä¸ªå˜é‡çš„æ ‡å‡†åŒ–å¤„ç†æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¯¹äºå•ä¸ªå˜é‡æ¥è¯´ï¼Œå®ƒåªæœ‰ä¸€ä¸ªç»´åº¦ï¼Œè€Œè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†æ—¶éœ€è¦åœ¨æŸä¸ªè½´ä¸Šè¿›è¡Œè¿ç®—ï¼Œå› æ­¤éœ€è¦å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå…·æœ‰å¤šä¸ªç»´åº¦çš„æ•°ç»„ï¼Œä»¥ä¾¿è¿›è¡Œè¿ç®—ã€‚\nå¯ä»¥ä½¿ç”¨ print() å‡½æ•°è¾“å‡º area_2d çš„å€¼ï¼Œä¾‹å¦‚:\nimport numpy as np# create a 1D numpy array with 5 valuesarea = np.array([1000, 1500, 1200, 1800, 2000])# reshape the 1D array to a 2D array with 5 rows and 1 columnarea_2d = area.reshape(-1, 1)print(area_2d)\n\nè¾“å‡ºç»“æœåº”è¯¥æ˜¯ï¼š\n[[1000] [1500] [1200] [1800] [2000]]\n\nå¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªäºŒç»´æ•°ç»„åŒ…å«5è¡Œ1åˆ—ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåªæœ‰ä¸€ä¸ªç‰¹å¾ï¼‰ã€‚\nå¦‚ä½•è§£é‡Šprobplotçš„ç»“æœçº¢è‰²çº¿æ¡è¡¨ç¤ºæ­£æ€åˆ†å¸ƒï¼Œè“è‰²çº¿æ¡è¡¨ç¤ºæ ·æœ¬æ•°æ®ï¼Œè“è‰²è¶Šæ¥è¿‘çº¢è‰²å‚è€ƒçº¿ï¼Œè¯´æ˜è¶Šç¬¦åˆé¢„æœŸåˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ã€‚è¿™ä¸ªå›¾é€šå¸¸ä¸sns.distplotç›´æ–¹å›¾ä¸è¿ç»­æ¦‚ç‡å¯†åº¦ä¼°è®¡å›¾ä¸€èµ·ä½¿ç”¨ã€‚![image-20230224124152190](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230224124152190.png)\nç¦æ­¢æ—¥å¿—ä¿¡æ¯from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)\n\nè¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ç¦æ­¢ matplotlib è¾“å‡ºä¸€äº›ä¸å¿…è¦çš„æ—¥å¿—ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½ä¼šå¹²æ‰°æˆ‘ä»¬çš„ç»“æœã€‚åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬å°† matplotlib.axes._axes æ¨¡å—çš„æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º ERRORï¼Œè¿™æ · matplotlib å°±ä¸ä¼šè¾“å‡ºè¯¥æ¨¡å—çš„æ—¥å¿—ä¿¡æ¯äº†ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ä»£ç ä¼šç¦æ­¢ matplotlib è¾“å‡ºä¸€äº›ä¸å¯¹æ•°è½´æœ‰å…³çš„è­¦å‘Šä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯é€šå¸¸åœ¨æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œå¯è§†åŒ–æ—¶å‡ºç°ã€‚\nå›¾åƒè¾“å‡ºå‰ç©ºç™½from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)data.plot.scatter(x = var, y = &#x27;SalePrice&#x27;, ylim = (0,800000))\n\nğŸ‘§ğŸ» ä¸ºä»€ä¹ˆæˆ‘è¿è¡Œå®Œè¿™ä¸€æ®µä»£ç ä¹‹ååœ¨å›¾åƒè¾“å‡ºä¹‹å‰ä¼šæœ‰ä¸€å¤§æ®µç©ºç™½ï¼Œå¹¶ä¸”æ˜¾ç¤º&lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;\tylabel=&#39;SalePrice&#39;&gt; No results.å‘¢ï¼Ÿ\nğŸ¤–ï¸ è¿™æ®µä»£ç çš„è¾“å‡ºåŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯å›¾åƒï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯è¾“å‡ºçš„å¯¹è±¡ç±»å‹å’Œç©ºç»“æœã€‚è¾“å‡ºå¯¹è±¡ç±»å‹çš„ä¿¡æ¯æ˜¯å› ä¸º data.plot.scatter() å‡½æ•°è¿”å›äº†ä¸€ä¸ª AxesSubplot å¯¹è±¡ï¼Œè€Œç©ºç»“æœæ˜¯å› ä¸ºåœ¨ä»£ç çš„æœ€åæ²¡æœ‰åŠ ä¸Š plt.show() å‡½æ•°ã€‚\nå› æ­¤ï¼Œä½ å¯ä»¥å°†ä»£ç ä¿®æ”¹ä¸ºå¦‚ä¸‹å½¢å¼æ¥é¿å…è¾“å‡ºç©ºç»“æœï¼š\nfrom matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)ax = data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0, 800000))plt.show()\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº† plt.show() å‡½æ•°æ¥æ˜¾ç¤ºå›¾åƒï¼Œå¹¶ä¸”å°† data.plot.scatter() çš„è¾“å‡ºèµ‹å€¼ç»™äº†ä¸€ä¸ªå˜é‡ axï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¯¹å›¾åƒè¿›è¡Œæ›´å¤šçš„æ“ä½œã€‚\ncorrcoef() + è½¬ç½®cm = np.corrcoef(df_train[cols].values.T)\nğŸ¤–ï¸ åœ¨Numpyä¸­ï¼Œcorrcoefå‡½æ•°ç”¨äºè®¡ç®—ä¸¤ä¸ªæˆ–å¤šä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µï¼Œè¯¥çŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å¦‚æœæœ‰ä¸¤ä¸ªå˜é‡ï¼Œé‚£ä¹ˆcorrcoefè¿”å›ä¸€ä¸ª2x2çš„çŸ©é˜µï¼Œå…¶ä¸­ç¬¬(i,j)ä¸ªå…ƒç´ ä¸ºç¬¬iä¸ªå˜é‡å’Œç¬¬jä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°ã€‚å¦‚æœæœ‰å¤šä¸ªå˜é‡ï¼Œåˆ™è¿”å›ä¸€ä¸ªnÃ—nçš„çŸ©é˜µï¼Œå…¶ä¸­nä¸ºå˜é‡çš„ä¸ªæ•°ã€‚corrcoefå‡½æ•°çš„å‚æ•°æ˜¯éœ€è¦è®¡ç®—ç›¸å…³ç³»æ•°çš„å˜é‡çŸ©é˜µæˆ–æ•°ç»„ï¼Œè¿™äº›å˜é‡å¯ä»¥å­˜å‚¨åœ¨Numpyæ•°ç»„æˆ–è€…Pandasçš„DataFrameæˆ–Seriesä¸­ã€‚å…¶ä¸­ï¼Œå‚æ•°rowvaré»˜è®¤ä¸ºTrueï¼Œè¡¨ç¤ºå¯¹æ¯ä¸€è¡Œè¿›è¡Œå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§è®¡ç®—ï¼Œå‚æ•°biasé»˜è®¤ä¸ºFalseï¼Œè¡¨ç¤ºæ— åä¼°è®¡ã€‚\nç¤ºä¾‹ä»£ç ï¼š\nimport numpy as npx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])corr_matrix = np.corrcoef(x)print(corr_matrix)\n\nè¾“å‡ºç»“æœä¸ºï¼š\n[[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]\n\nä¸Šè¿°ä»£ç ä¸­ï¼Œxæ˜¯ä¸€ä¸ª3x3çš„çŸ©é˜µï¼Œè¡¨ç¤ºä¸‰ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µã€‚corr_matrixä¸ºcorrcoefå‡½æ•°è®¡ç®—å‡ºçš„ç›¸å…³ç³»æ•°çŸ©é˜µï¼Œç”±äºæ¯ä¸ªå˜é‡ä¸è‡ªèº«çš„ç›¸å…³æ€§ç³»æ•°éƒ½ä¸º1ï¼Œå› æ­¤è¿”å›çš„ç»“æœä¸ºä¸€ä¸ªç”±1ç»„æˆçš„çŸ©é˜µã€‚\nğŸ¤–ï¸ df_train[cols]è¡¨ç¤ºåœ¨DataFrameå¯¹è±¡df_trainä¸­æŒ‘é€‰å‡ºç›¸å…³æ€§æœ€é«˜çš„10ä¸ªå˜é‡å¯¹åº”çš„åˆ—ï¼Œå…¶ä¸­colsæ˜¯ä¸€ä¸ªåŒ…å«è¿™äº›åˆ—åçš„åˆ—è¡¨ã€‚valueså°†è¿™äº›åˆ—è½¬æ¢ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªå˜é‡ã€‚.Tè¡¨ç¤ºå¯¹è¿™ä¸ªäºŒç»´æ•°ç»„è¿›è¡Œè½¬ç½®ï¼Œå³å°†è¡Œä¸åˆ—äº¤æ¢ï¼Œå˜æˆæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå˜é‡ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªæ ·æœ¬çš„å½¢å¼ã€‚è¿™æ ·çš„ç»“æœæ˜¯ä¸€ä¸ª10è¡Œnåˆ—çš„äºŒç»´æ•°ç»„ï¼Œå…¶ä¸­næ˜¯è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°ã€‚\nä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä»¥ä¸‹æ•°æ®é›†ï¼š\n#scatterplotsns.set()cols = [&#x27;SalePrice&#x27;, &#x27;OverallQual&#x27;, &#x27;GrLivArea&#x27;, &#x27;GarageCars&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;FullBath&#x27;, &#x27;YearBuilt&#x27;]sns.pairplot(df_train[cols], size = 2.5)plt.show();\n\nå¦‚æœé€‰æ‹©ç›¸å…³æ€§æœ€é«˜çš„3ä¸ªå˜é‡â€™Aâ€™, â€˜Bâ€™, â€˜Câ€™ï¼Œåˆ™df_train[cols]å°†è¿”å›ä¸€ä¸ªåŒ…å«3ä¸ªåˆ—çš„DataFrameå¯¹è±¡ï¼Œå³ï¼š\n    A   B   C0   1   4   71   2   5   82   3   6   9\n\né€šè¿‡.valuesè½¬æ¢æˆNumpyæ•°ç»„ï¼Œå³ä¸ºï¼š\narray([[1, 4, 7],       [2, 5, 8],       [3, 6, 9]])\n\nåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œ.Tæ“ä½œï¼Œå°†å¾—åˆ°ä¸€ä¸ª3è¡Œçš„äºŒç»´æ•°ç»„ï¼Œå³ï¼š\narray([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])\n\nè¿™ä¸ªäºŒç»´æ•°ç»„åŒ…å«äº†3ä¸ªå˜é‡çš„å–å€¼ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå˜é‡ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ã€‚è¿™ä¸ªæ•°ç»„å¯ç”¨äºè®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µã€‚\nâ—ï¸ ğŸ¤–ï¸ä¸è½¬ç½®ä¹Ÿå¯ä»¥ç”Ÿæˆç›¸å…³ç³»æ•°çŸ©é˜µå’Œç›¸å…³æ€§çƒ­å›¾ï¼Œä½†æ˜¯åœ¨æ•°æ®é›†è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œç”±äºç›¸å…³ç³»æ•°çŸ©é˜µæ˜¯ä¸€ä¸ªæ–¹é˜µï¼Œå¦‚æœæ ·æœ¬æ•°å¾ˆå¤šï¼Œåˆ™ç›¸å…³ç³»æ•°çŸ©é˜µçš„å¤§å°å°†éå¸¸å¤§ï¼Œè®¡ç®—é‡ä¹Ÿä¼šå˜å¾—å¾ˆå¤§ï¼Œå› æ­¤è¿è¡Œé€Ÿåº¦ä¼šå˜æ…¢ã€‚\n&#x3D;&#x3D;è€Œè½¬ç½®æ“ä½œå¯ä»¥å°†å˜é‡æ•°ä½œä¸ºçŸ©é˜µçš„è¡Œæ•°ï¼Œè¿™æ ·å°±é¿å…äº†çŸ©é˜µéå¸¸å¤§çš„é—®é¢˜ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œæ‰€ä»¥åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿›è¡Œè½¬ç½®æ“ä½œæ˜¯ä¸€ç§æœ‰æ•ˆçš„ä¼˜åŒ–æ–¹å¼ï¼Œå¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚åŒæ—¶ï¼Œè½¬ç½®åçš„çƒ­å›¾å¯ä»¥æ›´ç›´è§‚åœ°å±•ç¤ºå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ›´ä¾¿äºè§‚å¯Ÿå’Œåˆ†æã€‚&#x3D;&#x3D;\nheaatmap ç»˜åˆ¶#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\nğŸ‘§ğŸ» få’Œaxä»£è¡¨ä»€ä¹ˆï¼Ÿ\nğŸ¤–ï¸ åœ¨ä»£ç  f, ax = plt.subplots(figsize=(12, 9)) ä¸­ï¼Œf å’Œ ax æ˜¯ä¸¤ä¸ªå˜é‡åï¼Œåˆ†åˆ«è¡¨ç¤ºå›¾å½¢ç”»å¸ƒå’Œå›¾å½¢è½´å¯¹è±¡ã€‚\nå…·ä½“æ¥è¯´ï¼Œplt.subplots() å‡½æ•°è¿”å›äº†ä¸€ä¸ªåŒ…å«ç”»å¸ƒå’Œè½´å¯¹è±¡çš„å…ƒç»„ (fig, ax)ï¼Œå…¶ä¸­ fig è¡¨ç¤ºæ•´ä¸ªå›¾å½¢ç”»å¸ƒå¯¹è±¡ï¼Œè€Œ ax è¡¨ç¤ºåæ ‡è½´å¯¹è±¡ã€‚è¿™ä¸ªå…ƒç»„å¯ä»¥é€šè¿‡ Python çš„è§£æ„èµ‹å€¼è¯­æ³•æ¥æ‹†åˆ†æˆå•ç‹¬çš„å˜é‡ã€‚\nåœ¨è¿™æ®µä»£ç ä¸­ï¼Œé€šè¿‡è®¾ç½® figsize=(12, 9) å‚æ•°ï¼ŒæŒ‡å®šç”»å¸ƒå¤§å°ä¸º 12x9ï¼Œç„¶åå°†è¿”å›çš„å…ƒç»„åˆ†é…ç»™å˜é‡ f å’Œ axï¼Œä»¥ä¾¿åœ¨åç»­ä»£ç ä¸­ä½¿ç”¨å®ƒä»¬æ¥ç»˜åˆ¶çƒ­åŠ›å›¾ã€‚\nğŸ‘§ğŸ» vmax &#x3D; .8æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ\nğŸ¤–ï¸ vmax = .8 æ˜¯æŒ‡åœ¨ç»˜åˆ¶çƒ­åŠ›å›¾æ—¶ï¼Œå°†æ•°æ®æ˜ å°„åˆ°é¢œè‰²å›¾è°±çš„æœ€å¤§å€¼ä¸º 0.8ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™ä¸ªå‚æ•°ç”¨äºè®¾ç½®çƒ­åŠ›å›¾ä¸­é¢œè‰²çš„äº®åº¦èŒƒå›´ï¼Œæœ€å¤§å€¼ä¸º 0.8 è¡¨ç¤ºæ•°æ®ä¸­æœ€å¤§å€¼å¯¹åº”çš„é¢œè‰²åœ¨é¢œè‰²å›¾è°±ä¸­çš„äº®åº¦ä¸º 0.8ï¼Œè€Œå…¶ä»–å€¼å¯¹åº”çš„é¢œè‰²äº®åº¦å°†æ ¹æ®æ•°æ®çš„åˆ†å¸ƒæƒ…å†µè‡ªåŠ¨è°ƒæ•´ã€‚\né€šè¿‡è®¾ç½® vmax å‚æ•°ï¼Œå¯ä»¥åœ¨çƒ­åŠ›å›¾ä¸­å¼ºè°ƒæ•°æ®ä¸­çš„æç«¯å€¼å’Œé‡è¦ç‰¹å¾ã€‚å¦‚æœä¸è®¾ç½®è¿™ä¸ªå‚æ•°ï¼Œé¢œè‰²å›¾è°±çš„æœ€å¤§å€¼å°†è‡ªåŠ¨åŒ¹é…æ•°æ®ä¸­çš„æœ€å¤§å€¼ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é¢œè‰²æ˜ å°„èŒƒå›´å¤ªå¤§æˆ–å¤ªå°ï¼Œå½±å“çƒ­åŠ›å›¾çš„å¯è¯»æ€§ã€‚\nåˆ é™¤åŸæ¥dataframeä¸­çš„æŸäº›è¡Œï¼ˆ2ç§æ–¹å¼ï¼‰ğŸ¤–ï¸ è¿™ç§æƒ…å†µä¸‹ç›´æ¥dropï¼Œå¿…é¡»å†™inplace &#x3D; Trueæ‰ä¼šä¿®æ”¹åŸæ•°æ®æ¡†ï¼š\ntop_2_index = df_train.sort_values(by=&#x27;GrLivArea&#x27;, ascending=False)[:2].indexdf_train.drop(top_2_index, inplace=True)\n\nğŸ¤– ä¹Ÿå¯ä»¥ä¸ä½¿ç”¨ inplace=True å‚æ•°æ¥ä¿®æ”¹ df_trainï¼Œè€Œæ˜¯å°† drop() å‡½æ•°çš„è¿”å›å€¼èµ‹ç»™ä¸€ä¸ªæ–°çš„ DataFrameï¼š\ndf_train = df_train.drop(df_train[df_train[&#x27;Id&#x27;] == 1299].index)\n\nè¿™æ ·ï¼Œå°†åˆ é™¤ Id ä¸º 1299 çš„è¡Œä¹‹åçš„ DataFrame èµ‹å€¼ç»™äº† df_trainã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¼šè¿”å›ä¸€ä¸ªä¿®æ”¹åçš„å‰¯æœ¬ï¼Œè€Œä¸ä¼šç›´æ¥ä¿®æ”¹åŸå§‹ DataFrameã€‚\nğŸ‘§ğŸ» ä¿®æ”¹åçš„å‰¯æœ¬å’ŒåŸæ¥çš„dataframeä¸æ˜¯éƒ½å«df_trainå—ï¼Ÿè¿™æ ·è·Ÿç›´æ¥ä¿®æ”¹åŸæ¥çš„dataframeæœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿ\nğŸ¤–ï¸ ç¡®å®ï¼Œä¿®æ”¹åçš„å‰¯æœ¬å’ŒåŸå§‹çš„ DataFrame éƒ½å«åš df_trainï¼Œå®ƒä»¬çš„åç§°æ˜¯ä¸€æ ·çš„ï¼Œä½†å®ƒä»¬çš„å†…å­˜åœ°å€ä¸åŒï¼Œå®ƒä»¬æ˜¯ä¸¤ä¸ªä¸åŒçš„å¯¹è±¡ã€‚å½“ä½ æ‰§è¡Œ df_train = df_train.drop(df_train[df_train[&#39;Id&#39;] == 1299].index) æ—¶ï¼Œdrop() å‡½æ•°è¿”å›ä¸€ä¸ªæ–°çš„ DataFrame å¯¹è±¡ï¼Œç„¶åå°†è¿™ä¸ªæ–°çš„å¯¹è±¡èµ‹å€¼ç»™åŸå§‹çš„ df_trainï¼Œå› æ­¤è¿™ä¸ªè¯­å¥ä¼šæ”¹å˜åŸå§‹çš„ df_trainã€‚è€Œå¦‚æœä½ åœ¨ drop() å‡½æ•°ä¸­ä½¿ç”¨ inplace=True å‚æ•°ï¼Œå°±ä¼šç›´æ¥ä¿®æ”¹åŸå§‹çš„ DataFrameï¼Œä¸ä¼šè¿”å›æ–°çš„å¯¹è±¡ã€‚è¿™ä¸¤ç§æ–¹æ³•çš„æœ€ç»ˆæ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯ä¿®æ”¹å‰¯æœ¬å’Œç›´æ¥ä¿®æ”¹åŸå§‹ DataFrame çš„æ–¹å¼å¯èƒ½å¯¹ä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§æœ‰ä¸€å®šçš„å½±å“ã€‚\nğŸ¤– å¦‚æœä½¿ç”¨äº†èµ‹å€¼æ“ä½œç¬¦ â€œ&#x3D;â€ï¼Œå°†åŸå§‹æ•°æ®æ¡†èµ‹å€¼ç»™å‰¯æœ¬ï¼Œé‚£ä¹ˆå®ƒä»¬çš„å˜é‡åä¼šç›¸åŒï¼Œå¾ˆå®¹æ˜“è®©äººæ··æ·†ã€‚ä¸ºäº†é¿å…æ··æ·†ï¼Œå¯ä»¥åœ¨å˜é‡åä¸Šæ·»åŠ åç¼€æˆ–å‰ç¼€ä»¥ç¤ºåŒºåˆ«ï¼Œä¾‹å¦‚åœ¨å‰¯æœ¬çš„å˜é‡åä¸­åŠ å…¥â€_copyâ€æˆ–â€_modifiedâ€ç­‰åç¼€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µå†³å®šå¦‚ä½•å‘½åå˜é‡ä»¥ä¾¿æ›´å¥½åœ°åŒºåˆ†å®ƒä»¬ã€‚\nå››ç§assumptionsæœ€ç»ˆå¾—å‡ºç»“æœè¦çœ‹å› å˜é‡æ˜¯å¦ç¬¦åˆå¤šå˜é‡åˆ†æä¸­çš„å‡è®¾ã€‚\n\nNormality æ­£æ€åˆ†å¸ƒï¼šç”¨ histogram + normal probability plot\nRemember that &#x3D;&#x3D;univariate normality doesnâ€™t ensure multivariate normality&#x3D;&#x3D;.\nIn big samples (&gt;200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so thatâ€™s the main reason why we are doing this analysis.\nå¼‚è´¨æ€§å°±æ˜¯è¯´ç ”ç©¶çš„æ ·æœ¬çš„é‡è¦å±æ€§ä¸Šå­˜åœ¨å·®å¼‚ï¼Œæ¯”å¦‚äººå’Œäººä¹‹é—´çš„æ¶ˆè´¹ä¹ æƒ¯å¯èƒ½å¤§ç›¸å¾„åº­ï¼Œè¿™æ ·ä½ è®°å½•1000ä¸ªäºº10å¹´çš„æœˆæ¶ˆè´¹æ•°æ®ï¼Œå³ä¾¿ä»–ä»¬æ”¶å…¥æµå’Œèµ„äº§å®Œå…¨ç›¸åŒï¼Œæ¶ˆè´¹æµä¹Ÿå¯èƒ½æˆªç„¶ä¸åŒã€‚åœ¨ç»Ÿè®¡æ€§è´¨ä¸Šï¼Œè¿™ç§ä¸åŒè¡¨ç°ä¸ºå¼‚æ–¹å·®ã€‚æ‰€ä»¥åœ¨è®¡é‡æ¨¡å‹ä¸Šï¼Œæ¨ªæˆªé¢æ•°æ®å’Œé¢æ¿æ•°æ®ç»å¸¸å‡ºç°ï¼Œä¹Ÿå¯ä»¥è¯´æ€»ä¼šå­˜åœ¨å¼‚è´¨æ€§é—®é¢˜ã€‚\n\n\n\n\nHomoscedasticity åŒæ–¹å·®æ€§ï¼šç”¨ graphically\nHomoscedasticity refers to the â€˜assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)â€™ (Hair et al., 2013). \nHomoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n\nLinearity çº¿æ€§\nThe most common way to assess linearity is to &#x3D;&#x3D;examine scatter plots and search for linear patterns&#x3D;&#x3D;. If patterns are not linear, it would be worthwhile to explore data transformations.\n\n\nAbsence of correlated errors æ— ç›¸å…³é”™è¯¯\nCorrelated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that thereâ€™s a relationship between these variables. This occurs often in time series, where some patterns are &#x3D;&#x3D;time related&#x3D;&#x3D;. Weâ€™ll also not get into this. \nHowever, &#x3D;&#x3D;if you detect something, try to add a variable that can explain the effect youâ€™re getting&#x3D;&#x3D;. Thatâ€™s the most common solution for correlated errors.\n\n\n\npyplotå’Œmatplotlibçš„å…³ç³»ğŸ¤– pyplot æ˜¯ Matplotlib åº“ä¸­çš„ä¸€ä¸ªæ¨¡å—ï¼Œç”¨äºåˆ›å»ºå›¾å½¢å’Œå›¾è¡¨ã€‚åœ¨ Matplotlib ä¸­ï¼Œ&#x3D;&#x3D;pyplot æ¨¡å—è¢«ç”¨ä½œç»˜å›¾å·¥å…·çš„æ¥å£&#x3D;&#x3D;ï¼Œæä¾›äº†ä¸€äº›æ–¹ä¾¿çš„å‡½æ•°å’Œæ–¹æ³•æ¥åˆ›å»ºå’Œæ“ä½œå›¾å½¢ã€è½´ã€å›¾ä¾‹å’Œæ³¨é‡Šç­‰å…ƒç´ ã€‚\nMatplotlib æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–åº“ï¼Œæä¾›äº†å„ç§å„æ ·çš„ç»˜å›¾å‡½æ•°å’Œå·¥å…·ï¼Œå¯ä»¥ç”¨äºåˆ›å»ºæŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€æŸ±çŠ¶å›¾ã€é¥¼å›¾ç­‰å„ç§ç±»å‹çš„å›¾è¡¨ã€‚è€Œ pyplot æ¨¡å—åˆ™æ˜¯ Matplotlib ä¸­æœ€å¸¸ç”¨çš„ç»˜å›¾å·¥å…·ä¹‹ä¸€ï¼Œå®ƒæä¾›äº†å¾ˆå¤šæ–¹ä¾¿çš„å‡½æ•°å’Œæ–¹æ³•ï¼Œä½¿å¾—ç»˜åˆ¶å„ç§ç±»å‹çš„å›¾è¡¨å˜å¾—æ›´åŠ ç®€å•æ˜“ç”¨ã€‚å› æ­¤ï¼Œpyplot å¯ä»¥è¢«è®¤ä¸ºæ˜¯ Matplotlib åº“ä¸­æœ€ä¸»è¦çš„ä¸€ä¸ªæ¨¡å—ã€‚\nåŒæ–¹å·®æ€§(Homoskedasticity)å’Œå¼‚æ–¹å·®æ€§(Heteroskedasticity)ğŸ‘¦ å†™ä¸€ä¸ªé€šä¿—çš„è§£é‡Šï¼šæ¯”å¦‚è¯´ $income&#x3D;b*education+e$ è¿™ä¸ªè®¡é‡æ¨¡å‹,æè¿°äº†æ•™è‚²æ°´å¹³ä¸æ”¶å…¥çš„å…³ç³»ï¼Œeä¸ºæ®‹å·®ï¼ŒOLSç­‰ä¼ ç»Ÿè®¡é‡æ¡†æ¶è¦æ±‚æ®‹å·®eä¸ä¸ä»»ä½•å˜é‡ç›¸å…³,å³: $E(e|x)&#x3D;0$ã€‚\nä¸€ç§å¸¸è§çš„æ®‹å·®eä¸xç›¸å…³çš„æ–¹å¼æ˜¯: æ®‹å·®eçš„æ–¹å·®ä¸å˜é‡xç›¸å…³ï¼Œè¿™æ—¶å€™ï¼Œæ®‹å·®eçš„æ–¹å·®ä¼šéšç€xå˜åŠ¨è€Œå˜åŠ¨ï¼Œå› æ­¤æ–¹å·®æ˜¯å¼‚è´¨æ€§çš„. è¿™è¢«ç§°ä¸ºå¼‚æ–¹å·®é—®é¢˜ã€‚\nå¼‚æ–¹å·®é—®é¢˜ä¼šå¯¼è‡´ä»€ä¹ˆ?\næ‹¿ä¸Šè¾¹é‚£ä¸ªæ•™è‚²-å·¥èµ„çš„ä¾‹å­æ¥çœ‹ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå—æ•™è‚²æ°´å¹³è¶Šé«˜çš„äººç¾¤æ”¶å…¥å˜åŠ¨è¶Šå¤§ï¼Œè€Œæ•™è‚²æ°´å¹³è¾ƒä½çš„äººç¾¤æ”¶å…¥ç›¸å·®ä¸ä¼šå¤ªå¤§ (æ¯”å¦‚è¯´ï¼Œæœ‰æœ€ä½å·¥èµ„æ³•)ï¼Œè¿™æ—¶å°±å‡ºç°å¼‚æ–¹å·®é—®é¢˜äº†, å› ä¸ºeçš„æ–¹å·®ä¼šéšç€æ•™è‚²æ°´å¹³xçš„å¢å¤§è€Œå¢å¤§ã€‚\n\nå¼‚æ–¹å·®å­˜åœ¨çš„æ—¶å€™,å¤§å¤šæ•°æƒ…å†µä¸‹,OLSä¼°è®¡å‡ºçš„æ–¹å·®ä¼šæ¯”å®é™…çš„æ–¹å·®è¦å°(å½“ç„¶, å°éƒ¨åˆ†æƒ…å†µä¸‹ä¼°è®¡å€¼ä¼šæ¯”å®é™…çš„å¤§)ï¼Œ&#x3D;&#x3D;å› æ­¤ä¼šè¿‡é«˜åœ°ä¼°è®¡ç³»æ•°bçš„æ˜¾è‘—æ€§&#x3D;&#x3D; (å› ä¸º, ç³»æ•°çš„tå€¼&#x3D;ç³»æ•°&#x2F;æ ‡å‡†å·®)ï¼Œ**&#x3D;&#x3D;è€Œä¸ä¼šå½±å“ç³»æ•°ä¼°è®¡å€¼çš„å¤§å°&#x3D;&#x3D;**(è€ƒè™‘å¼‚æ–¹å·®å½±å“ç³»æ•°å¤§å°çš„é—®é¢˜æ˜¯è¿‘å‡ å¹´çš„ç„¦ç‚¹å’Œéš¾ç‚¹ï¼Œç•¥è¿‡ä¸æ)ã€‚\nğŸ‘¨ åŒæ–¹å·®æ€§æ˜¯æˆ‘ä»¬å¯¹OLSå›å½’æ®‹å·®çš„ä¸€ä¸ªè¦æ±‚ï¼Œå³ &#x3D;&#x3D;$\\epsilon \\sim N(0, \\sigma^2)$&#x3D;&#x3D; ã€‚ç®€å•çš„è¯´å°±æ˜¯æ®‹å·®å¿…é¡»æ˜¯éšæœºçš„ï¼Œè¿™ä¸ªåˆ†å¸ƒæ˜¯æˆ‘ä»¬ç”¨æ¥æè¿°éšæœºåˆ†å¸ƒçš„ä¸€ç§æ–¹æ³• â€“ å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸ºsigmaæ–¹ã€‚\nå¼‚æ–¹å·®åˆ™è¯´æ˜æ®‹å·®ä¸æ»¡è¶³è¿™ä¸ªæ­£æ€åˆ†å¸ƒã€‚\næ¦‚æ‹¬è€Œè¨€ï¼ŒåŒæ–¹å·®è¯´æ˜å›å½’çš„æ®‹å·®é¡¹æ˜¯éšæœºçš„ï¼Œå¼‚æ–¹å·®è¯´æ˜æ®‹å·®é¡¹ä¸éšæœºï¼Œä½ çš„å›å½’è®¾ç½®æˆ–è€…å˜é‡çš„é€‰å–æœ‰é—®é¢˜ï¼Œéœ€è¦ä¿®æ­£ã€‚\nget_dummies()#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\nğŸ¤– è¿™æ®µä»£ç ä½¿ç”¨äº†get_dummies()å‡½æ•°å°†æ•°æ®æ¡†ä¸­çš„åˆ†ç±»å˜é‡è½¬æ¢æˆäº†è™šæ‹Ÿå˜é‡ï¼ˆdummy variableï¼‰ã€‚\nè™šæ‹Ÿå˜é‡æ˜¯æŒ‡å°†ä¸€ä¸ªåˆ†ç±»å˜é‡æ‹†åˆ†æˆå¤šä¸ªäºŒå…ƒå˜é‡çš„è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªåä¸ºcolorçš„åˆ†ç±»å˜é‡ï¼Œå…¶ä¸­åŒ…æ‹¬çº¢è‰²ã€è“è‰²å’Œç»¿è‰²ä¸‰ä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆå°†colorè½¬æ¢æˆè™šæ‹Ÿå˜é‡çš„è¿‡ç¨‹å°±æ˜¯åˆ›å»ºä¸‰ä¸ªæ–°çš„äºŒå…ƒå˜é‡ï¼Œåˆ†åˆ«ä»£è¡¨çº¢è‰²ã€è“è‰²å’Œç»¿è‰²ï¼Œå¦‚æœæ ·æœ¬å±äºçº¢è‰²ç±»åˆ«ï¼Œåˆ™å¯¹åº”çš„çº¢è‰²è™šæ‹Ÿå˜é‡å€¼ä¸º1ï¼Œå…¶ä½™ä¸¤ä¸ªè™šæ‹Ÿå˜é‡çš„å€¼ä¸º0ï¼Œä»¥æ­¤ç±»æ¨ã€‚\nget_dummies()å‡½æ•°ä¼šè‡ªåŠ¨è¯†åˆ«å‡ºæ‰€æœ‰çš„åˆ†ç±»å˜é‡ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢æˆè™šæ‹Ÿå˜é‡ã€‚å®ƒè¿˜æœ‰ä¸€äº›å…¶ä»–çš„å‚æ•°ï¼Œä¾‹å¦‚drop_firstå¯ä»¥æŒ‡å®šæ˜¯å¦è¦åˆ é™¤æ¯ä¸ªå˜é‡çš„ç¬¬ä¸€ä¸ªè™šæ‹Ÿå˜é‡ï¼Œä»¥é¿å…å¤šé‡å…±çº¿æ€§é—®é¢˜ã€‚\nProcedureâ€œç†è§£é—®é¢˜ -&gt; å•å˜é‡åˆ†æ -&gt; å¤šå˜é‡åˆ†æ -&gt; æ•°æ®æ¸…æ´— -&gt; éªŒè¯å‡è®¾â€\n1âƒ£ï¸ Understand the problem. Weâ€™ll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\nè§‚å¯Ÿ columns\n\n2âƒ£ï¸ Univariable study. Weâ€™ll just focus on the dependent variable (â€˜SalePriceâ€™) and try to know a little bit more about it.\n\nä½¿ç”¨ describe\nä½¿ç”¨ç›´æ–¹å›¾\nåˆ†æååº¦å’Œå³°åº¦\nä½¿ç”¨æ•£ç‚¹å›¾å’Œç®±å›¾è§‚å¯Ÿç›®æ ‡å˜é‡å’Œå…¶ä»–å˜é‡ä¹‹é—´çš„å…³ç³»\n\n3âƒ£ï¸ Multivariate study. Weâ€™ll try to understand how the dependent variable and independent variables relate.\n\nçƒ­å›¾çœ‹æ‰€æœ‰å˜é‡ä¹‹é—´çš„å…³ç³»\nçƒ­å›¾çœ‹è·Ÿç›®æ ‡å˜é‡æœ€ç›¸å…³çš„å˜é‡ä¹‹é—´çš„å…³ç³»\nscatterplot çœ‹ç›®æ ‡å˜é‡åŠæœ€ç›¸å…³çš„å˜é‡ä¹‹é—´çš„å…³ç³»\n\n4âƒ£ï¸ Basic cleaning. Weâ€™ll clean the dataset and handle the missing data, outliers and categorical variables.\n\nç¼ºå¤±å€¼\n\nçœ‹æ‰€æœ‰çš„å˜é‡ä¸­ç¼ºå¤±å€¼å æ¯”\n\nå‰”é™¤ç¼ºå¤±å˜é‡ï¼šå¯¹å› å˜é‡ä¸é‡è¦çš„å¯ä»¥ç›´æ¥å‰”é™¤æ•´ä¸ªå˜é‡ï¼›æœ‰æ›¿ä»£å˜é‡çš„å¯ä»¥ç›´æ¥å‰”é™¤æ•´ä¸ªå˜é‡ï¼›ä¸å‰”é™¤æ•´ä¸ªå˜é‡çš„å¯ä»¥ä»…å°†åŒ…å«ç¼ºå¤±å€¼çš„å•ä¸ªæ ·æœ¬å‰”é™¤\n\næ£€æŸ¥æ— ç¼ºå¤±å€¼\n\n\n\nç¦»ç¾¤å€¼\n\n[æ•°æ®æ ‡å‡†åŒ–][4]ï¼Œè®¾ç½®ä¸€ä¸ªé˜ˆå€¼æ¥åˆ¤æ–­ç¦»ç¾¤å€¼ï¼Œè¾“å‡ºè¿‡é«˜&#x2F;è¿‡ä½çš„ç¦»ç¾¤å€¼\nä½¿ç”¨æ•£ç‚¹å›¾è§‚å¯ŸæŸä¸ªè‡ªå˜é‡å’Œå› å˜é‡çš„å…³ç³»ï¼Œæœ‰äº†ä¸Šä¸€æ­¥å¾—åˆ°çš„ç¦»ç¾¤å€¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•£ç‚¹å›¾ä¸­ä¸€çœ¼çœ‹å‡ºç¦»ç¾¤å€¼åœ¨å“ª\nä¿ç•™éµå¾ªè¶‹åŠ¿çš„ç¦»ç¾¤å€¼ï¼Œåˆ é™¤è¿èƒŒè¶‹åŠ¿çš„ç¦»ç¾¤å€¼\n\n\n\n5âƒ£ï¸ Test assumptions. Weâ€™ll check if our data meets the assumptions required by most multivariate techniques.\n\næ­£æ€åˆ†å¸ƒ\nçœ‹histogramï¼šååº¦å’Œå³°åº¦\nçœ‹å¸¸æ€æœºç‡å›¾ï¼šæ•°æ®åˆ†å¸ƒåº”ç´§è·Ÿä»£è¡¨æ­£æ€åˆ†å¸ƒçš„å¯¹è§’çº¿\n\n\nå¼‚æ–¹å·®æ€§\næ•£ç‚¹å›¾å¯è§†åŒ–ï¼Œä¸€å¤´ç´§å‡‘ä¸€å¤´åˆ†æ•£ï¼Œæˆ–ä¸­é—´ç´§å‡‘ä¸¤å¤´åˆ†æ•£ï¼Œæˆ–ä¸­é—´åˆ†æ•£ä¸¤å¤´ç´§å‡‘\n\n\nè™šæ‹ŸåŒ–å˜é‡\n\nStep1: Understand our data#check the decorationdf_train.columns\n\n\n Look as each variable and try to understand their meaning and relvance to this problem. Although itâ€™s time-comsuming, it gives us the flavour of our dataset.\n\n\nWe should focus 5 aspects. Create an Excel spreadsheet with following columns:\n\nVariable\nType -&gt; â€˜numericalâ€™ or â€˜categoricalâ€™\nSegment -&gt; Define the segments of all variables then identify them. (eg. building&#x2F; space &#x2F;location)\nExpectation -&gt; the variable infulence on the dependent variable. (high&#x2F; medium&#x2F; low)\nConclusion -&gt; the importance of the variable \nComments -&gt; any general comments that occured to us\n\n\nExpectation will give us â€˜sixth senseâ€™. In order to fill it, we should read the description of all the variables one by one, ask ourselves:\n\nDo we care this variable when we are buying a house?\nHow important would this variable is?\nIs this information already decribed in any other variable?\n\n\nFocus the variables with â€˜highâ€™ expectation. Generate some scatter plots between those variables and the dependant variable(SalePrice), filling the conclusion column which is the correction of our expectations.\n\nYou may give up some of your expecting variables after vasualizing them.\n\n\n\nStep2: Univariable analysis: â€˜SalePriceâ€™\ndescribe()\nhistogram- normal distribution? (positive) skewness? peakedness?\n[skewness and kurtosis][1]\n\n#data#descriptive statistics summarydf_train[&#x27;SalePrice&#x27;].describe()#graph#histogramsns.distplot(df_train[&#x27;SalePrice&#x27;]);#data#skewness and kurtosisprint(&quot;Skewness: %f&quot; % df_train[&#x27;SalePrice&#x27;].skew())print(&quot;Kurtosis: %f&quot; % df_train[&#x27;SalePrice&#x27;].kurt())\n\nRelationship with numerical variables\nscatter plots - if there is a linear (exponential) reaction\n\n#graph#scatter plot grlivarea/salepricevar = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));#scatter plot totalbsmtsf/salepricevar = &#x27;TotalBsmtSF&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));\n\nRelationship with categorical fratures\nbox plots\n\n#box plot overallqual/salepricevar = &#x27;OverallQual&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(8, 6))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);var = &#x27;YearBuilt&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);plt.xticks(rotation=90);\n\nğŸ¤–ï¸ è¿™æ®µä»£ç ä½¿ç”¨äº†Pythonä¸­çš„Pandaså’ŒSeabornåº“ï¼Œç”¨äºç»˜åˆ¶æˆ¿ä»·æ•°æ®é›†ä¸­æ•´ä½“è´¨é‡ï¼ˆOverallQualï¼‰ä¸æˆ¿ä»·ï¼ˆSalePriceï¼‰ä¹‹é—´çš„ç®±å‹å›¾ã€‚å…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š\n\nvar = &#39;OverallQual&#39;ï¼šå°†å˜é‡åOverallQualèµ‹å€¼ç»™å˜é‡varï¼Œè¡¨ç¤ºè¦ç»˜åˆ¶çš„xè½´å˜é‡æ˜¯æ•´ä½“è´¨é‡ã€‚\ndata = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1)ï¼šå°†è®­ç»ƒæ•°æ®é›†df_trainä¸­çš„SalePriceå’ŒOverallQualåˆ—æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¹¶å­˜å‚¨åœ¨åä¸ºdataçš„å˜é‡ä¸­ã€‚è¿™é‡Œä½¿ç”¨äº†Pandasåº“ä¸­çš„concatå‡½æ•°ã€‚\nf, ax = plt.subplots(figsize=(8, 6))ï¼šåˆ›å»ºä¸€ä¸ª8x6è‹±å¯¸å¤§å°çš„ç»˜å›¾å¯¹è±¡ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨åä¸ºfçš„å˜é‡ä¸­ï¼Œå°†å…¶è½´å¯¹è±¡å­˜å‚¨åœ¨åä¸ºaxçš„å˜é‡ä¸­ã€‚è¿™é‡Œä½¿ç”¨äº†Matplotlibåº“ã€‚\nfig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)ï¼šä½¿ç”¨Seabornåº“ä¸­çš„boxplotå‡½æ•°ç»˜åˆ¶ç®±å‹å›¾ï¼Œxè½´ä¸ºOverallQualï¼Œyè½´ä¸ºSalePriceï¼Œæ•°æ®æ¥æºä¸ºdataã€‚å°†ç»˜åˆ¶ç»“æœå­˜å‚¨åœ¨åä¸ºfigçš„å˜é‡ä¸­ã€‚\nfig.axis(ymin=0, ymax=800000)ï¼šå°†yè½´çš„èŒƒå›´è®¾å®šä¸º0åˆ°800000ï¼Œä½¿å¾—ç®±å‹å›¾çš„çºµåæ ‡èŒƒå›´æ›´åŠ åˆé€‚ã€‚è¿™é‡Œä½¿ç”¨äº†Matplotlibåº“ä¸­çš„axiså‡½æ•°ã€‚\n\nStep3: Multivariable analysis\n[heatmap][2] style - from a universla perspective\n\n#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\nğŸ¤–ï¸ è¿™æ®µä»£ç ç”¨äºç»˜åˆ¶æ•°æ®é›†ä¸­çš„ç›¸å…³æ€§çŸ©é˜µçš„çƒ­åŠ›å›¾ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸€è¡Œçš„è§£é‡Šï¼š\n\ncorrmat = df_train.corr(): è®¡ç®—æ•°æ®é›† df_train ä¸­æ‰€æœ‰æ•°å€¼å‹å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µï¼Œå…¶ä¸­ corr() æ˜¯ pandas åº“ä¸­è®¡ç®—ç›¸å…³ç³»æ•°çš„å‡½æ•°ã€‚\nf, ax = plt.subplots(figsize=(12, 9)): åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º 12x9 çš„å›¾å½¢ç”»å¸ƒï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨å˜é‡ f å’Œ ax ä¸­ï¼Œç”¨äºç»˜åˆ¶ç›¸å…³æ€§çŸ©é˜µçš„çƒ­åŠ›å›¾ã€‚è¿™é‡Œç”¨åˆ°äº† matplotlib åº“ä¸­çš„ subplots() å‡½æ•°ã€‚\nsns.heatmap(corrmat, vmax=.8, square=True): ä½¿ç”¨ seaborn åº“ä¸­çš„ heatmap() å‡½æ•°å°†ç›¸å…³æ€§çŸ©é˜µç»˜åˆ¶æˆçƒ­åŠ›å›¾ï¼Œå…¶ä¸­ corrmat æ˜¯ç›¸å…³æ€§çŸ©é˜µçš„æ•°æ®ï¼Œvmax æ˜¯çƒ­åŠ›å›¾ä¸­é¢œè‰²çš„æœ€å¤§å€¼ï¼Œsquare å‚æ•°ç”¨äºæŒ‡å®šçƒ­åŠ›å›¾çš„å½¢çŠ¶æ˜¯å¦ä¸ºæ­£æ–¹å½¢ã€‚\n\né€šè¿‡ç»˜åˆ¶ç›¸å…³æ€§çŸ©é˜µçš„çƒ­åŠ›å›¾ï¼Œå¯ä»¥æ›´ç›´è§‚åœ°äº†è§£æ•°æ®é›†ä¸­å„ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥ä¾¿è¿›è¡Œæ•°æ®åˆ†æå’Œå»ºæ¨¡ã€‚\n&#x3D;&#x3D;â—ï¸Notice if some of the independent variables have a similar color distribution in the heat map! This proves that they play a similar role in the overall model, and perhaps it is possible to keep only the one variable with the least missing values in the cleaning.&#x3D;&#x3D;\n#saleprice correlation matrixk = 10 #number of variables for heatmapcols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].indexcm = np.corrcoef(df_train[cols].values.T)sns.set(font_scale=1.25)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#x27;.2f&#x27;, annot_kws=&#123;&#x27;size&#x27;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)plt.show()\n\nğŸ¤–ï¸ è¿™æ®µä»£ç ä½¿ç”¨äº†Pythonä¸­çš„Numpyã€Pandaså’ŒSeabornåº“ï¼Œç”¨äºç»˜åˆ¶æˆ¿ä»·æ•°æ®é›†ä¸­å„å˜é‡ä¹‹é—´çš„ç›¸å…³ç³»æ•°çƒ­å›¾ã€‚å…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š\n\nk = 10ï¼šå°†çƒ­å›¾ä¸­å±•ç¤ºçš„ç›¸å…³ç³»æ•°æœ€é«˜çš„10ä¸ªå˜é‡æŒ‘é€‰å‡ºæ¥ã€‚\ncols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].indexï¼šä»ç›¸å…³ç³»æ•°çŸ©é˜µcorrmatä¸­é€‰æ‹©SalePriceåˆ—ä¸å…¶ä»–åˆ—çš„ç›¸å…³ç³»æ•°æœ€é«˜çš„10ä¸ªå˜é‡ï¼Œå°†å…¶ç´¢å¼•å­˜å‚¨åœ¨colså˜é‡ä¸­ã€‚\ncm = np.corrcoef(df_train[cols].values.T)ï¼šä»è®­ç»ƒæ•°æ®é›†df_trainä¸­æŒ‘é€‰å‡ºç›¸å…³ç³»æ•°æœ€é«˜çš„10ä¸ªå˜é‡ï¼Œè®¡ç®—å…¶ç›¸å…³ç³»æ•°çŸ©é˜µï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨cmå˜é‡ä¸­ã€‚è¿™é‡Œä½¿ç”¨äº†Numpyåº“ä¸­çš„[corrcoefå‡½æ•°][3]ã€‚\nsns.set(font_scale=1.25)ï¼šè®¾ç½®Seabornåº“ä¸­çš„å­—ä½“å¤§å°ã€‚\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.2f&#39;, annot_kws=&#123;&#39;size&#39;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)ï¼šç»˜åˆ¶çƒ­å›¾ã€‚å…¶ä¸­ï¼Œå‚æ•°cmæ˜¯ä¸€ä¸ªç›¸å…³ç³»æ•°çŸ©é˜µï¼Œå‚æ•°cbarè¡¨ç¤ºæ˜¯å¦æ˜¾ç¤ºé¢œè‰²æ¡ï¼Œå‚æ•°annotè¡¨ç¤ºæ˜¯å¦æ˜¾ç¤ºæ¯ä¸ªæ ¼å­ä¸­çš„æ•°å€¼ï¼Œå‚æ•°squareè¡¨ç¤ºæ˜¯å¦å°†æ¯ä¸ªæ ¼å­è®¾ç½®ä¸ºæ­£æ–¹å½¢ï¼Œå‚æ•°fmtè¡¨ç¤ºæ˜¾ç¤ºæ•°å€¼æ—¶ä¿ç•™ä¸¤ä½å°æ•°ï¼Œå‚æ•°annot_kwsè¡¨ç¤ºæ³¨é‡Šçš„å‚æ•°ï¼Œè¿™é‡Œè®¾ç½®å­—ä½“å¤§å°ä¸º10ï¼Œyticklabelså’Œxticklabelsè¡¨ç¤ºè¡Œå’Œåˆ—çš„æ ‡ç­¾åï¼Œè¿™é‡Œéƒ½ä¸ºcols.valuesã€‚å°†ç»˜åˆ¶ç»“æœå­˜å‚¨åœ¨åä¸ºhmçš„å˜é‡ä¸­ã€‚\nplt.show()ï¼šæ˜¾ç¤ºç»˜åˆ¶å‡ºçš„çƒ­å›¾ã€‚è¿™é‡Œä½¿ç”¨äº†Matplotlibåº“ä¸­çš„showå‡½æ•°ã€‚\n\nsns.set()cols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].index.tolist()sns.pairplot(df_train[cols], height = 2.5)plt.show()\n\n\n\nStep4: Missing dataMissing data#missing datatotal = df_train.isnull().sum().sort_values(ascending=False)percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)missing_data = pd.concat([total, percent], axis=1, keys=[&#x27;Total&#x27;, &#x27;Percent&#x27;])missing_data.head(20)\n\nğŸ¤–ï¸\n\ndf_train.isnull()ï¼šåˆ¤æ–­æ•°æ®é›† df_train ä¸­æ¯ä¸ªå…ƒç´ æ˜¯å¦ä¸ºç¼ºå¤±å€¼ï¼Œè¿”å›ä¸€ä¸ªä¸ df_train å¤§å°ç›¸åŒçš„å¸ƒå°”çŸ©é˜µï¼Œå†…å®¹æ˜¯True&#x2F;Falseã€‚\ndf_train.isnull().sum()ï¼šå°†å¸ƒå°”çŸ©é˜µä¸­æ¯åˆ—ï¼ˆå˜é‡ï¼‰ä¸­çš„ç¼ºå¤±å€¼æ•°é‡ç›¸åŠ ï¼Œå¾—åˆ°æ¯åˆ—ç¼ºå¤±å€¼çš„æ€»æ•°ã€‚\ndf_train.isnull().count()ï¼šå°†å¸ƒå°”çŸ©é˜µä¸­æ¯åˆ—ï¼ˆå˜é‡ï¼‰ä¸­çš„å…ƒç´ ï¼ˆæ— è®ºæ˜¯å¦ç¼ºå¤±ï¼‰æ•°é‡ç›¸åŠ ï¼Œå¾—åˆ°æ¯åˆ—å…ƒç´ çš„æ€»æ•°ã€‚\n\n#dealing with missing datadf_train = df_train.drop((missing_data[missing_data[&#x27;Total&#x27;] &gt; 1]).index, axis = 1)df_train = df_train.drop(df_train.loc[df_train[&#x27;Electrical&#x27;].isnull()].index)df_train.isnull().sum().max() #just checking that there&#x27;s no missing data missing...\n\nOutliers#standardizing datafrom sklearn.preprocessing import StandardScalersaleprice_scaled = StandardScaler().fit_transform(np.array(df_train[&#x27;SalePrice&#x27;])[:, np.newaxis]);low_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][:10]high_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][-10:]print(&#x27;outer range (low) of the distrubution:&#x27;)print(low_range)print(&#x27;\\n outer range (high) of the distribution:&#x27;)print(high_range)\n\n\n\nStep5: Getting hard coreNormality: histogram + normal probability plot#histogram and normal probability plotfrom scipy.stats import normsns.distplot(df_train[&#x27;SalePrice&#x27;], kde = True,fit = norm);fig = plt.figure()res = stats.probplot(df_train[&#x27;SalePrice&#x27;], plot = plt)\n\n\n ğŸ¤–ï¸ histplotä¸æ”¯æŒç›´æ¥æ˜¾ç¤ºæ‹Ÿåˆçš„æ­£æ€åˆ†å¸ƒæ›²çº¿ã€‚å¦‚æœè¦åŒæ—¶æ˜¾ç¤ºæ­£æ€åˆ†å¸ƒæ›²çº¿ï¼Œå¯ä»¥ä½¿ç”¨displotã€‚\n\n&#x3D;&#x3D;In case of positive skewness, log transformations usually works well.&#x3D;&#x3D;\ndef dist_probplot(x):    sns.distplot(df_train[x], fit = norm)    fig = plt.figure()    res = stats.probplot(df_train[x], plot = plt)\n\n\n#applying log transformationdf_train[&#x27;SalePrice&#x27;] = np.log(df_train[&#x27;SalePrice&#x27;])dist_probplot(&#x27;SalePrice&#x27;)\n\n\ndist_probplot(&#x27;TotalBsmtSF&#x27;)\n\n\n\nOk, now we are dealing with the big boss. What do we have here?\n\nSomething that, in general, presents skewness.\nA significant number of observations with value zero (houses without basement).\nA big problem because the value zero doesnâ€™t allow us to do log transformations.\n\n\nå¤„ç†æ–¹æ³•ï¼šå°†éé›¶å€¼åšå¯¹æ•°è½¬æ¢ï¼Œé›¶å€¼è¿˜æ˜¯é›¶ï¼Œä½†æ˜¯è‹¥æ‰§è¡Œï¼š\n![image-20230228093921098](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230228093921098.png)\nä¼šå‡ºç°ä»¥ä¸ŠæŠ¥é”™ã€‚\nğŸ¤–ï¸ å‡ºç°RuntimeWarning: divide by zero encountered in logçš„åŸå› æ˜¯åœ¨å¯¹æ•°å˜æ¢æ—¶é‡åˆ°äº† 0ï¼Œå› ä¸º log(0) æ˜¯æ²¡æœ‰å®šä¹‰çš„ï¼Œæ‰€ä»¥ä¼šäº§ç”Ÿè­¦å‘Šã€‚å¯ä»¥å°†å€¼ä¸º 0 çš„è§‚æµ‹åœ¨å˜æ¢å‰ç”¨ä¸€ä¸ªæå°çš„æ•°ä»£æ›¿ï¼Œæ¯”å¦‚å¯ä»¥ç”¨ 1e-6 ä»£æ›¿ 0ã€‚\nğŸ‘§ğŸ» ä½†æ˜¯è¿™æ®µä»£ç ä¸æ˜¯è¯´åªæœ‰ä¸ç­‰äº0æ—¶æ‰ä¼šè¿›è¡Œå¯¹æ•°å˜æ¢å—ï¼Ÿä¸ºä»€ä¹ˆè¿˜ä¼šé‡åˆ°log(0)çš„æƒ…å†µå‘¢ï¼Ÿ\nğŸ¤–ï¸ è¿™æ˜¯ç”±äºæµ®ç‚¹æ•°çš„ç²¾åº¦é—®é¢˜å¯¼è‡´çš„ã€‚å¯èƒ½æœ‰ä¸€äº›å€¼åœ¨å¤„ç†ä¹‹åéå¸¸æ¥è¿‘0ï¼Œä½†å®é™…ä¸Šå¹¶ä¸ç­‰äº0ï¼Œå› æ­¤åœ¨è¿›è¡Œnp.log()å‡½æ•°è®¡ç®—æ—¶ï¼Œä¼šè¿”å›ä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ— ç©·å€¼ï¼Œæ¯”å¦‚-log(1e-15)&#x3D;-34.538776394910684ã€‚è¿™ä¹Ÿå°±æ˜¯æŠ¥é”™ä¸­æåˆ°çš„â€œdivide by zero encountered in logâ€é”™è¯¯ï¼Œå…¶å®å¹¶éæ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„é™¤é›¶æ“ä½œï¼Œè€Œæ˜¯ä¸€ä¸ªæ•°å­¦ä¸Šçš„çº¦å®šã€‚\nğŸ¤–ï¸ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯åœ¨å¯¹æ•°å˜æ¢å‰ï¼Œå…ˆå¯¹æ•°æ®ä¸­çš„æ‰€æœ‰éé›¶å€¼åŠ ä¸Šä¸€ä¸ªæå°çš„å¸¸æ•°ï¼Œæ¯”å¦‚1e-8ï¼Œé¿å…å‡ºç°log(0)çš„æƒ…å†µï¼ŒåŒæ—¶ç”±äºè¿™ä¸ªå¸¸æ•°å€¼éå¸¸å°ï¼Œå¯¹æ•´ä½“æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾å½±å“ä¹Ÿéå¸¸å°ã€‚å› æ­¤å¯ä»¥å°†ä»£ç ä¿®æ”¹ä¸ºï¼š\nepsilon = 1e-8  # å®šä¹‰ä¸€ä¸ªæå°å€¼df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] + epsilon)\n\nğŸ‘§ğŸ» æ‰€ä»¥å…¶å®åº”è¯¥æ˜¯è¿™æ ·ï¼š\n#transform datadf_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;])\n\n#histogram and normal probability plotsns.distplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], fit = norm)fig = plt.figure()res = stats.probplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], plot = plt)\n\nğŸ‘§ğŸ» plot &#x3D; pltæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ\nğŸ¤– plot=pltæ˜¯å°†&#x3D;&#x3D;Matplotlibåº“çš„å½“å‰pyplotå¯¹è±¡&#x3D;&#x3D;ä¼ é€’ç»™probplotå‡½æ•°ä¸­çš„å‚æ•°plotã€‚è¿™æ ·åšæ˜¯ä¸ºäº†è®©probplotå‡½æ•°èƒ½å¤Ÿåœ¨å½“å‰çš„Matplotlibå›¾å½¢å¯¹è±¡ä¸­ç»˜åˆ¶æ¦‚ç‡å›¾ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬åœ¨åˆ›å»ºä¸€ä¸ªæ–°çš„Matplotlibå›¾å½¢å¯¹è±¡æ—¶ä¼šä½¿ç”¨plt.figure()å‡½æ•°ï¼Œå¹¶ä¸ºå…¶æŒ‡å®šä¸€ä¸ªæ–°çš„åç§°æˆ–ç¼–å·ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªç°æœ‰çš„å›¾å½¢å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¼ é€’ç»™plotå‚æ•°ï¼Œä½¿å¾—probplotå‡½æ•°åœ¨è¯¥å¯¹è±¡ä¸­ç»˜åˆ¶å›¾å½¢ã€‚\nğŸ‘§ğŸ» æˆ‘åº”è¯¥æ€ä¹ˆè·å–Matplotlibåº“çš„å½“å‰pyplotå¯¹è±¡ï¼Ÿ\nğŸ¤– ä½ å¯ä»¥ä½¿ç”¨matplotlib.pyplot.gcf()å‡½æ•°æ¥è·å–å½“å‰å›¾å½¢å¯¹è±¡ã€‚gcf()ä»£è¡¨â€œget current figureâ€ï¼Œå®ƒå°†è¿”å›æœ€è¿‘åˆ›å»ºçš„Figureå¯¹è±¡ï¼ˆå¦‚æœæ²¡æœ‰ç°æœ‰Figureï¼Œåˆ™ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ï¼‰ã€‚\nHomoscedasticity: graphically\nDepartures from an equal dispersion are shown by such shapes as &#x3D;&#x3D;cones (small dispersion at one side of the graph, large dispersion at the opposite side)&#x3D;&#x3D; or &#x3D;&#x3D;diamonds (a large number of points at the center of the distribution)&#x3D;&#x3D;.\n\n#scatter plotplt.scatter(df_train[&#x27;GrLivArea&#x27;], df_train[&#x27;SalePrice&#x27;]);\n\n#scatter plotplt.scatter(df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;TotalBsmtSF&#x27;], df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;SalePrice&#x27;]);\n\n\n\nDummy variables#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\n","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["Python","kaggle","data exploration"]},{"title":"Hello World","url":"/2023/02/07/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"SQL-è®¡ç®—ç”¨æˆ·ç•™å­˜ç‡","url":"/2023/02/07/SQL-%E8%AE%A1%E7%AE%97%E7%94%A8%E6%88%B7%E6%AC%A1%E6%97%A5%E7%95%99%E5%AD%98%E7%8E%87/","content":"è®¡ç®—ç”¨æˆ·æ¬¡æ—¥ç•™å­˜ç‡å¦‚æœåªæ˜¯è®¡ç®—ç”¨æˆ·çš„æ¬¡æ—¥ç•™å­˜ç‡ï¼Œé‚£ä¹ˆä½¿ç”¨date_sub()å‡½æ•°å°±å¤Ÿäº†ï¼š\nselect avg(if(b.device_id is not null, 1, 0)) as avg_retfrom(    select distinct device_id, date    from question_practice_detail) aleft join(    select distinct device_id, date_sub(date, interval 1 day) as date    from question_practice_detail) bon a.device_id = b.device_id and a.date = b.date\n\n\ndate_sub() å‡½æ•°ï¼š\n\nSELECT DATE_SUB(â€˜2010-08-12â€™, INTERVAL 3 DAY) AS NewDate \nç»“æœï¼š 2010-08-09\n\nSELECT DATE_SUB(â€˜2010-08-12â€™, INTERVAL â€˜3-2â€™ YEAR_MONTH) AS NewDate \nç»“æœï¼š 2007-06-12\n\nSELECT DATE_SUB(â€˜2011-09-14 2:44:36â€™, INTERVAL â€˜2:26â€™ HOUR_MINUTE) AS NewDate \nç»“æœï¼š 2011-09-14 00:18:36\n\n\n\n\nåŸºäºSQLçš„ç•™å­˜ç‡è®¡ç®—\nè½¬è‡ªçŸ¥ä¹ï¼šåŸºäºSQLçš„ç•™å­˜ç‡è®¡ç®—\n\nä¸€ã€ä»€ä¹ˆæ˜¯ç•™å­˜ç‡äº’è”ç½‘è¡Œä¸šé‡Œï¼Œç•™å­˜ç‡æ˜¯ç”¨äºåæ˜ ç½‘ç«™ã€äº’è”ç½‘åº”ç”¨æˆ–ç½‘ç»œæ¸¸æˆçš„è¿è¥æƒ…å†µçš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œå…¶å…·ä½“å«ä¹‰ä¸ºåœ¨ç»Ÿè®¡å‘¨æœŸï¼ˆå‘¨&#x2F;æœˆï¼‰å†…ï¼Œæ¯æ—¥æ´»è·ƒç”¨æˆ·æ•°åœ¨ç¬¬Næ—¥ä»å¯åŠ¨è¯¥Appçš„ç”¨æˆ·æ•°å æ¯”çš„å¹³å‡å€¼ã€‚å…¶ä¸­Né€šå¸¸å–2ã€3ã€7ã€14ã€30ï¼Œåˆ†åˆ«å¯¹åº”æ¬¡æ—¥ç•™å­˜ç‡ã€ä¸‰æ—¥ç•™å­˜ç‡ã€å‘¨ç•™å­˜ç‡ã€åŠæœˆç•™å­˜ç‡å’Œæœˆç•™å­˜ç‡ã€‚\nç•™å­˜ç‡å¸¸ç”¨äºåæ˜ ç”¨æˆ·ç²˜æ€§ï¼Œå½“Nå–å€¼è¶Šå¤§ã€ç•™å­˜ç‡è¶Šé«˜æ—¶ï¼Œç”¨æˆ·ç²˜æ€§è¶Šé«˜ã€‚\näºŒã€ç•™å­˜ç‡çš„è®¡ç®—\nç•™å­˜ç‡ &#x3D; ç™»é™†ç”¨æˆ·æ•°&#x2F;æ–°å¢ç”¨æˆ·æ•° * 100%\n\næ–°å¢ç”¨æˆ·æ•°ï¼šåœ¨å½“å‰æ—¶é—´æ®µæ–°æ³¨å†Œï¼ˆæˆ–æ–°è®¿é—®ï¼‰çš„ç”¨æˆ·æ•°ï¼›\n\nç™»å½•ç”¨æˆ·æ•°ï¼šåœ¨ç»Ÿè®¡çš„æ—¶é—´æ®µè‡³å°‘ç™»å½•è¿‡ä¸€æ¬¡çš„ç”¨æˆ·æ•°ï¼›\n\næ¬¡æ—¥ç•™å­˜ç‡ï¼šåœ¨æ¬¡æ—¥è‡³å°‘ç™»å½•è¿‡ä¸€æ¬¡çš„ç”¨æˆ·æ•°&#x2F;å½“å¤©æ–°å¢çš„ç”¨æˆ·æ•°ï¼›\n\nâ—ï¸3æ—¥ç•™å­˜ç‡ï¼šåœ¨å¾€å3å¤©å†…è‡³å°‘ç™»å½•è¿‡ä¸€æ¬¡çš„ç”¨æˆ·æ•°&#x2F;å½“å¤©æ–°å¢çš„ç”¨æˆ·æ•°ï¼›\n\nâ—ï¸7æ—¥ç•™å­˜ç‡ï¼šåœ¨å¾€å7å¤©å†…è‡³å°‘ç™»å½•è¿‡ä¸€æ¬¡çš„ç”¨æˆ·æ•°&#x2F;å½“å¤©æ–°å¢çš„ç”¨æˆ·æ•°ï¼›\n\nâ—ï¸15æ—¥ç•™å­˜æ•°ï¼šå½“å¤©æ–°å¢çš„ç”¨æˆ·æ•°ï¼Œåœ¨å¾€å7å¤©å†…è‡³å°‘ç™»å½•è¿‡ä¸€æ¬¡çš„ç”¨æˆ·ï¼Œåœ¨å¾€åç¬¬8å¤©åˆ°ç¬¬14å¤©å†…è‡³å°‘å†ç™»é™†è¿‡ä¸€æ¬¡çš„ç”¨æˆ·æ•°\nâ¡ï¸ 3æ—¥å’Œ7æ—¥ï¼Œè‡³å°‘ç™»é™†è¿‡ä¸€æ¬¡ï¼›15æ—¥ï¼Œ7å¤©ä¸ºä¸€æ®µï¼Œåœ¨æ¯æ®µå†…è‡³å°‘ç™»å½•ä¸€æ¬¡ï¼\n\n\nSQLä¸­è®¡ç®—ç”¨æˆ·çš„ç•™å­˜ç‡\næ–°å¢ç”¨æˆ·æ•°\n\nç”±äºæ•°æ®è¿‡å¤§ï¼Œè¿™æˆªå–æ—¶é—´2017.11.26~2017.12.03ä¸ºä¾‹ã€‚\né¦–å…ˆè®¡ç®—åˆ†æ¯ï¼Œè¿™é‡Œæœ‰çš„ç®—æ³•æ˜¯ç”¨æ–°å¢ç”¨æˆ·æ•°ï¼Œæœ‰çš„ç®—æ³•æ˜¯ç”¨æ´»è·ƒç”¨æˆ·æ•°ã€‚\nâš ï¸æ³¨æ„ï¼šæ–°å¢ç”¨æˆ·æ•°ä¸æ´»è·ƒç”¨æˆ·æ•°å¹¶ä¸ç›¸ç­‰ï¼Œæ´»è·ƒç”¨æˆ·æ•°åŒ…å«æ–°å¢ç”¨æˆ·æ•°ã€‚æ´»è·ƒç”¨æˆ·æ•°ï¼Œå½“å¤©çš„è®¿é—®äººæ•°ï¼Œä¹Ÿå°±æ˜¯UVã€‚\n-- æ¯ä½ç”¨æˆ·çš„æœ€æ—©ç™»å½•æ—¥æœŸSELECT ç”¨æˆ·ID, MIN(æ—¥æœŸ) AS æœ€æ—©ç™»å½•æ—¥æœŸFROM userbehaviorWHERE æ—¥æœŸ &gt; &#x27;2017-11-25&#x27;AND æ—¥æœŸ &lt; &#x27;2017-12-04&#x27;GROUP BY ç”¨æˆ·ID\n\n\nå†ä»ä¸Šè¡¨ä¸­è®¡ç®—å‡ºæ¯å¤©çš„æ–°å¢äººæ•°ï¼Œâ—ï¸ç®—æ–°å¢äººæ•°ç”¨çš„æ—¥æœŸæ˜¯æœ€æ—©ç™»å½•æ—¥æœŸï¼\nSELECT æœ€æ—©ç™»å½•æ—¥æœŸ AS æ—¥æœŸ, COUNT(DISTINCT ç”¨æˆ·ID) AS æ–°å¢äººæ•°FROM(SELECT ç”¨æˆ·ID, MIN(æ—¥æœŸ) AS æœ€æ—©ç™»å½•æ—¥æœŸ     FROM userbehavior     WHERE æ—¥æœŸ &gt; &#x27;2017-11-25&#x27; AND æ—¥æœŸ &lt; &#x27;2017-12-04&#x27;     GROUP BY ç”¨æˆ·ID) AS fGROUP BY æœ€æ—©ç™»å½•æ—¥æœŸ\n\n\nä»¥ä¸‹æ˜¯æ´»è·ƒç”¨æˆ·æ•°çš„ç®—æ³•ï¼ŒäºŒè€…ç¡®å®æ•°å€¼ä¸Šå¹¶ä¸ç›¸ç­‰ã€‚â—ï¸ç®—æ¯æ—¥æ´»è·ƒç”¨æˆ·æ•°ç”¨çš„æ—¥æœŸå°±æ˜¯å½“å¤©çš„æ—¥æœŸï¼\n-- æ¯å¤©æ´»è·ƒç”¨æˆ·æ•°=UV è®¿å®¢æ•°SELECT æ—¥æœŸ, COUNT(DISTINCT ç”¨æˆ·ID) AS æ´»è·ƒç”¨æˆ·æ•°FROM userbehaviorwhere æ—¥æœŸ &gt;&#x27;2017-11-25&#x27; AND æ—¥æœŸ &lt; &#x27;2017-12-04&#x27;GROUP BY æ—¥æœŸ\n\n\næŠ½å–7å¤©çš„æ´»è·ƒç”¨æˆ·æ•°ã€‚\n\næ¬¡æ—¥ç•™å­˜çš„ç”¨æˆ·æ•°\n\n\næ¬¡æ—¥çš„æ—¶é—´é—´éš”ä¸º1ï¼Œæ¶‰åŠåˆ°æ—¶é—´é—´éš”é‡‡ç”¨è‡ªè”ç»“ã€‚\n\nSELECT a.ç”¨æˆ·ID, a.`æ—¥æœŸ` AS atime, b.`æ—¥æœŸ` AS btimeFROM userbehavior AS a LEFT JOIN userbehavior AS bON a.`ç”¨æˆ·ID` = b.`ç”¨æˆ·ID`WHERE a.`æ—¥æœŸ` &gt; &#x27;2017-11-25&#x27; AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;\n\n\nå°†ä¸Šè¡¨å­˜ä¸ºè§†å›¾Cï¼Œ\nCREATE VIEW C(ç”¨æˆ·ID, atime, btime)ASSELECT a.ç”¨æˆ·ID, a.`æ—¥æœŸ`, b.`æ—¥æœŸ` from userbehavior AS a LEFT JOIN userbehavior AS bON a.`ç”¨æˆ·ID`=b.`ç”¨æˆ·ID`WHERE a.`æ—¥æœŸ`&gt;&#x27;2017-11-25&#x27;AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;;\n\n\nè®¡ç®—æ—¶é—´é—´éš”ç”¨timestampdiffå‡½æ•°\n\nSELECT `ç”¨æˆ·ID`, TIMESTAMPDIFF(DAY, atime, btime)AS æ—¶é—´é—´éš”FROM (  SELECT a.ç”¨æˆ·ID, a.æ—¥æœŸ as atime, b.æ—¥æœŸ as btime  FROM userbehavior as a LEFT JOIN userbehavior as b  ON a.ç”¨æˆ·ID=b.`ç”¨æˆ·ID`  WHERE a.`æ—¥æœŸ`&gt;&#x27;2017-11-25&#x27;AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;) AS c-- è¿™é‡Œç›´æ¥è®¾ç½®ä¸ºè¡¨c\n\n\nå¾—åˆ°äº†ç”¨æˆ·çš„æ—¶é—´é—´éš”\n3.ç”¨caseè¯­å¥ç­›é€‰å‡ºæ—¶é—´é—´éš”ä¸º1çš„æ•°æ®ï¼Œå¹¶ä¸”è¿›è¡Œè®¡æ•°\nSELECT *, COUNT(DISTINCT case when æ—¶é—´é—´éš” = 1 then `ç”¨æˆ·ID`\t\t\tELSE NULL\t\t\tEND)AS æ¬¡æ—¥ç•™å­˜æ•°FROM( SELECT `ç”¨æˆ·ID`,TIMESTAMPDIFF(DAY,atime,btime)AS æ—¶é—´é—´éš”\t\t\t FROM (\t\t\t\tSELECT a.ç”¨æˆ·ID,a.æ—¥æœŸ as atime,b.æ—¥æœŸ as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.ç”¨æˆ·ID=b.`ç”¨æˆ·ID`\t\t\t\tWHERE a.`æ—¥æœŸ`&gt;&#x27;2017-11-25&#x27;AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\næ¬¡æ—¥ç•™å­˜ç‡\n\næ¬¡æ—¥ç•™å­˜ç‡&#x3D;æ¬¡æ—¥ç•™å­˜ç”¨æˆ·æ•°&#x2F;å½“æ—¥æ´»è·ƒç”¨æˆ·æ•°\nSELECT *,COUNT(DISTINCT case when æ—¶é—´é—´éš” = 1 then `ç”¨æˆ·ID`\t\t\tELSE NULL\t\t\tEND) AS æ¬¡æ—¥ç•™å­˜æ•°/COUNT(DISTINCT ç”¨æˆ·ID) AS æ¬¡æ—¥ç•™å­˜ç‡FROM( SELECT `ç”¨æˆ·ID`,TIMESTAMPDIFF(DAY,atime,btime)AS æ—¶é—´é—´éš”\t\t\t FROM (\t\t\t\tSELECT a.ç”¨æˆ·ID,a.æ—¥æœŸ as atime,b.æ—¥æœŸ as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.ç”¨æˆ·ID=b.`ç”¨æˆ·ID`\t\t\t\tWHERE a.`æ—¥æœŸ`&gt;&#x27;2017-11-25&#x27;AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n\nä¸‰ã€ä¸‰æ—¥ä»¥åŠNæ—¥ç•™å­˜çš„è®¡ç®—åªéœ€è¦ä¿®æ”¹æ—¶é—´é—´éš”&#x3D;Nå³å¯ã€‚\nSELECT æ—¥æœŸ,COUNT(DISTINCT ç”¨æˆ·ID AS æ´»è·ƒç”¨æˆ·æ•°,COUNT(DISTINCT case when æ—¶é—´é—´éš”=1 then `ç”¨æˆ·ID`\t\t\tELSE NULL\t\t\tEND) AS æ¬¡æ—¥ç•™å­˜æ•°/COUNT(DISTINCT ç”¨æˆ·ID) AS æ¬¡æ—¥ç•™å­˜ç‡COUNT(DISTINCT case when æ—¶é—´é—´éš”=3 then `ç”¨æˆ·ID`\t\t\tELSE NULL\t\t\tEND) AS æ¬¡æ—¥ç•™å­˜æ•°/COUNT(DISTINCT ç”¨æˆ·ID) AS ä¸‰æ—¥ç•™å­˜ç‡COUNT(DISTINCT case when æ—¶é—´é—´éš”=7 then `ç”¨æˆ·ID`\t\t\tELSE NULL\t\t\tEND) AS æ¬¡æ—¥ç•™å­˜æ•°/COUNT(DISTINCT ç”¨æˆ·ID) AS ä¸ƒæ—¥ç•™å­˜ç‡FROM( SELECT `ç”¨æˆ·ID`,TIMESTAMPDIFF(DAY,atime,btime)AS æ—¶é—´é—´éš”\t\t\t FROM (\t\t\t\tSELECT a.ç”¨æˆ·ID,a.æ—¥æœŸ as atime,b.æ—¥æœŸ as btime\t\t\t\tFROM userbehavior as a LEFT JOIN æ¯æ—¥æ–°å¢ç”¨æˆ·æ•°è¡¨ as b\t\t\t\tON a.ç”¨æˆ·ID=b.`ç”¨æˆ·ID`\t\t\t\tWHERE a.`æ—¥æœŸ`&gt;&#x27;2017-11-25&#x27;AND a.`æ—¥æœŸ` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d","categories":["åˆ·é¢˜ç¬”è®°"],"tags":["SQL"]}]