[{"title":"Data Science R Basic","url":"/2023/02/09/Data-Science-R-Basic/","content":"Words &amp; Expressions\nin a nested way 嵌套\n\nconcatenate 连接 c() -&gt; all items in ‘()’ to a vector\n\n\nWhy use ‘[[’ instead of ‘[’ to access variables?\\We extract the population like this:\np &lt;- murders$population\n\nIf you instead try to access a column with just one bracket,\nmurders[&quot;population&quot;]\n\n&#x3D;&#x3D;R returns a subset of the original data frame containing just this column. This new object will be of class data.frame rather than a vector.&#x3D;&#x3D; To access the column itself you need to use either the $ accessor or the double square brackets [[:\nmurders[[&quot;population&quot;]]\n\n\n\nR- sorting functions\nsort()\n# sort()排序，排序结果不可逆转# 默认是升序# decreasing为TRUE，表示降序# decreasing为FALSE，表示升序#排序后并不会修改原对象的值#示例如下：&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; #sort()默认为从小到大（升序）排序，等同于decreasing=FALSE&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; sort(a,decreasing = F) [1]  3  4  5  6  7  9 10 13 16 22&gt; #decreasing=TRUE,为从大到小（降序）排序&gt; sort(a,decreasing = T) [1] 22 16 13 10  9  7  6  5  4  3#排序并不会修改原对象的值，a仍为原来未排序的a&gt; a [1]  3  9 16  6  7  4 22  5 10 13\n\n\n\nrank()\n用法：rank(a)函数说明：指出当前向量中各元素大小的排名，默认升序函数还有其他的参数：rank(x &#x3D; data, na.last &#x3D; TRUE)x 表示待排序的向量na.last 表示是否排序时是否将NA放在最后面，默认忽略NA\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; order(a) [1]  1  6  8  4  5  2  9 10  3  7#说明：在向量a中，3是第一小的数，位置下标为1；4是第二小的数，位置下标为6；最大的数是22，位置下标为7#a[order(a)] 等同于sort(a)&gt; a[order(a)]  [1]  3  4  5  6  7  9 10 13 16 22\n\n\n\norder()\n说明：返回的值表示位置，默认是升序，依次对应的是向量的最小值、次小值、第三小值…最大值\n用法：order(a), a为要排序的向量order(… &#x3D; data, na.last &#x3D; TRUE,decreasing &#x3D; TRUE)… 表示待排序向量na.last 表示时候将NA值放在最后面（默认排序忽略NA）decreasing 表示是否按照降序排序，默认升序。\n&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; rank(a) [1]  1  6  9  4  5  2 10  3  7  8 #说明：向量a中的第一个数为3，是最小的，故排名为1；第二个数是9，是第六小的数，排名为6\n\n\n\n比较\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) #将a**从小到大排序并列出** [1]  3  4  5  6  7  9 10 13 16 22&gt; order(a) #返回从小到大的数的**位置下标**，a[order(a)]=sort(a) [1]  1  6  8  4  5  2  9 10  3  7&gt; rank(a) #返回a中**每个数的排名（从小到大）** [1]  1  6  9  4  5  2 10  3  7  8&gt; a [1]  3  9 16  6  7  4 22  5 10 13#注意：经过sort()、order()、rank()排序后，a不改变\n\norder和rank的区别在于一个返回的是index，一个是rank，index不一定是数字。\n\n\n","categories":["刷题笔记"],"tags":["R"]},{"title":"Kaggle|房价预测","url":"/2023/02/28/HousePricing2/","content":"NotesProfileReport [1]import ydata_profiling as ppfimport pandas as pddf_train = pd.read_csv(&#x27;train.csv&#x27;)ppf.ProfileReport(df_train)\n\n👧🏻 ydata_profiling 和 pandas_profiling?\n👧🏻 ProfileReport() 运行卡顿？\n🤖 这可能与不同浏览器对于 Jupyter Notebook 的支持和性能优化有关。Jupyter Notebook 是一个基于 Web 的应用程序，它的性能和稳定性可能会受到不同浏览器的影响。\nSafari 浏览器在运行 Jupyter Notebook 时可能会更加流畅，这可能是因为 Safari 浏览器具有更好的 Web 技术支持和性能优化。同时，Safari 浏览器在 Mac OS X 系统中是默认的浏览器，可能与操作系统之间的兼容性更好。\n另一方面，Chrome 浏览器在运行 Jupyter Notebook 时可能会出现卡顿问题，这可能是因为 Chrome 浏览器具有更高的系统资源占用率，以及更多的插件和扩展功能，导致其在运行 Jupyter Notebook 时更容易出现性能问题。\n如果你在使用 Chrome 浏览器时遇到了性能问题，你可以尝试优化浏览器的设置或减少浏览器插件和扩展的使用，或者使用其他浏览器来运行 Jupyter Notebook，例如 Safari、Firefox 等等。同时，你也可以考虑在本地使用 Jupyter Notebook，而不是在浏览器中运行 Jupyter Notebook，这可以提供更好的性能和稳定性。\nEncoding\n数据预处理阶段，object类的特征会根据类别使用不同的编码方式处理（label encoding和One-hot encoding）\n\nLabel encodingOne-hot encodingReference[1] kaggle案例-Python实现房价预测-完整分析流程\n[2] [Kaggle竞赛丨房价预测（House Prices）][https://zhuanlan.zhihu.com/p/137076292]\n[3] \n","categories":["刷题笔记"],"tags":["Python","kaggle","data prediction"]},{"title":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?","url":"/2023/02/08/How-is-Hadley-Wickham-able-to-contribute-so-much-to-R-particularly-in-the-form-of-packages/","content":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?\nDavid Robinson:\nFrom following Hadley’s work, it seems to me that along with being an exceptional programmer and data scientist, and having the advantage of developing R packages as part of his job, Hadley follows a few strategies that serve as useful wisdom for all developers:\n\nHe writes packages that make himself more productive. Three of Hadley’s popular packages, devtools, Roxygen2, and testthat, make it very easy to (respectively) develop, document and test R packages. He recognized that the time spent to create and maintain those was small compared to the time it would save him (and others!) in developing future packages. This extends beyond those package development tools: packages like stringr and lubridate are designed to make working with strings and dates easier. This also extends beyond his own packages: he takes advantage of packages like Rcpp (http://www.rcpp.org/) that make writing R C++ extensions fast and intuitive.\nHe takes full advantage of social coding. He’s a prolific GitHub user (hadley (Hadley Wickham)), which makes it efficient to receive and respond to bug reports and feature requests, and to collaborate with others (for instance, with Romain Francois on dplyr).\nHe works to simplify his packages rather than complicate them. In his announcement of the tidyr package (Introducing tidyr) he notes that “Just as reshape2 did less than reshape, tidyr does less than reshape2.” When packages are simpler (doing a few things well instead of hundreds of things poorly), they’re easier to develop and maintain.\n\n**Hadley Wickham: **\nI like David’s answer, but here are a few more thoughts from a personal perspective ;)\n\nWriting. I have worked really hard to build a solid writing habit - &#x3D;&#x3D;I try and write for 60-90 minutes every morning. It’s the first thing I do after I get out of bed.&#x3D;&#x3D; I think writing is really helpful to me for a few reasons. First, &#x3D;&#x3D;I often use my writing as a reference&#x3D;&#x3D; - I don’t program in C++ every day, so I’m constantly referring to @Rcpp every time I do. Writing also makes me aware of gaps in my knowledge and my tools, and filling in those gaps tends to make me more efficient at tackling new problems.\nReading. I read a lot. I follow about 300 blogs, and keep a pretty close eye on the R tags on Twitter and Stack Overflow. I don’t read most things deeply - &#x3D;&#x3D;the majority of content I only briefly skim. But this wide exposure helps me keep up with changes in technology, interesting new programming languages, and what others are doing with data.&#x3D;&#x3D; It’s also helpful that if when you’re tackling a new problem you can recognise the basic name - then googling for it will suggest possible solutions. If you don’t know the name of a problem, it’s very hard to research it.\nChunking. Context-switching is expensive, so if I worked on many packages at the same time, I’d never get anything done. Instead, at any point in time, most of my packages are lying fallow, steadily accumulating issues and ideas for new feature. Once a critical mass has accumulated, I’ll spend a couple of days on the package.\n\nFinally, it’s hard to over-emphasise the impact that working full-time on R makes. Since I’ve left Rice, I now spend well over 90% of my work time thinking about and programming in R. This has a compounding effect because as I built better tools (cognitive and computational) it becomes even easier to build new tools. I can create a new package in seconds, and I have many techniques on-hand (in-brain) for solving new problems.\n","categories":["Quora"]},{"title":"SQL-刷题笔记","url":"/2023/02/07/SQL%20%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","content":"1. 查询列列查询、多列查询、限制返回数量、in、like、聚合函数。\n# 查询所有列SELECT * FROM user_profile# 查询多列SELECT device_id, gender, age, university FROM user_profile# 查询结果去重SELECT DISTINCT university FROM user_profile# 查询结果限制返回行数SELECT device_id FROM user_profile LIMIT 2# 查询后重命名SELECT device_id FROM user_profile AS user_infos_example LIMIT 2# 否定查询select device_id, gender, age, universityfrom user_profilewhere not university = &#x27;复旦大学&#x27;# 过滤空值select device_id, gender, age, universityfrom user_profilewhere not age is null# inselect device_id, gender, age, university, gpa from user_profilewhere university in (&#x27;北京大学&#x27;, &#x27;复旦大学&#x27;, &#x27;山东大学&#x27;)# likeselect device_id, age, universityfrom user_profilewhere university like &#x27;%北京%&#x27;# max() min()select max(gpa)from user_praofilewhere university = &#x27;复旦大学&#x27;# avg() count()select count(gender) as male_num, avg(gpa) as avg_gpafrom user_profilewhere gender = &#x27;male&#x27;# havingselect university, avg(question_cnt) avg_question_cnt, avg(answer_cnt) avg_answer_cntfrom user_profilegroup by universityhaving avg_question_cnt &lt; 5 or avg_answer_cnt &lt; 20# 生成新字段后不能用where要用having\n\n\n\n2. 涉及到多个表的情况select university, (count(q.question_id)/count(distinct(q.device_id))) avg_answer_cntfrom user_profile ujoin question_practice_detail qon u.device_id = q.device_idgroup by universityorder by university asc\n\nselect u.university, q.difficult_level, count(qp.question_id)/count(distinct qp.device_id) avg_answer_cntfrom user_profile u, question_practice_detail qp, question_detail qwhere u.university = &#x27;山东大学&#x27; and u.device_id = qp.device_id and q.question_id = qp.question_idgroup by u.university, difficult_levelorder by avg_answer_cnt\n\n\n\nSQL25 查找山东大学或者性别为男生的信息题目：现在运营想要分别查看学校为山东大学或者性别为男性的用户的device_id、gender、age和gpa数据，请取出相应结果，结果不去重。\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果（注意输出的顺序，先输出学校为山东大学再输出性别为男生的信息）：\n\n\n\ndevice_id\ngender\nage\ngpa\n\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n2138\nmale\n21\n3.4\n\n\n3214\nmale\nNone\n4\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n4321\nmale\n28\n3.6\n\n\nselect device_id, gender, age, gpafrom user_profilewhere university = &#x27;山东大学&#x27;union all select device_id, gender, age, gpafrom user_profilewhere gender = &#x27;male&#x27;\n\n\n\n3. ifSQL26 计算25岁以上和以下的用户数量题目：现在运营想要将用户划分为25岁以下和25岁及以上两个年龄段，分别查看这两个年龄段用户数量\n本题注意：age为null 也记为 25岁以下\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\nage_cut\nnumber\n\n\n\n25岁以下\n4\n\n\n25岁及以上\n3\n\n\nselect age_cut, count(device_id) numberfrom(select if(age &gt;= 25, &#x27;25岁及以上&#x27;, &#x27;25岁以下&#x27;) as age_cut, device_id from user_profile) t1group by age_cut\n\n\n\n4. caseSQL27 查看不同年龄段的用户明细题目：现在运营想要将用户划分为20岁以下，20-24岁，25岁及以上三个年龄段，分别查看不同年龄段用户的明细情况，请取出相应数据。（注：若年龄为空请返回其他。）\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ndevice_id\ngender\nage_cut\n\n\n\n2138\nmale\n20-24岁\n\n\n3214\nmale\n其他\n\n\n6543\nfemale\n20-24岁\n\n\n2315\nfemale\n20-24岁\n\n\n5432\nmale\n25岁及以上\n\n\n2131\nmale\n25岁及以上\n\n\n4321\nmale\n25岁及以上\n\n\nselect device_id, gender, case    when age &lt; 20 then &#x27;20岁以下&#x27;    when age &lt; 25 then &#x27;20-24岁&#x27;    when age &gt;= 25 then &#x27;25岁及以上&#x27;    else &#x27;其他&#x27;end age_cutfrom user_profile\n\n\n\n5. day()\nday()\n\nmonth()\n\nyear()\n\n\n6. 用户留存率计算SQL-计算用户次日留存率.md\n7. 字符串相关SQL-字符串相关.md\n","categories":["刷题笔记"],"tags":["SQL"]},{"title":"SQL-字符串相关","url":"/2023/02/07/SQL-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3/","content":"SQL30 统计每种性别的人数描述题目：现在运营举办了一场比赛，收到了一些参赛申请，表数据记录形式如下所示，现在运营想要统计每个性别的用户分别有多少参赛者，请取出相应结果\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;url&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;urlsydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ngender\nnumber\n\n\n\nmale\n2\n\n\nfemale\n3\n\n\n\nselect substring_index(profile, &#x27;,&#x27;, -1) as gender, count(device_id)from user_submitgroup by gender\n\n\n\nSQL31 提取博客URL中的用户名描述题目：对于申请参与比赛的用户，blog_url字段中url字符后的字符串为用户个人博客的用户名，现在运营想要把用户的个人博客用户字段提取出单独记录为一个新的字段，请取出所需数据。\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bisdgboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;dkittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;ur&#x2F;tigaer\n\n\n4321\n171 cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ndevice_id\nuser_name\n\n\n\n2138\nbisdgboy777\n\n\n3214\ndkittycc\n\n\n6543\ntigaer\n\n\n4321\nuhsksd\n\n\n2131\nsydney\n\n\n\n提取某个字符一般有四种做法：\ntrim()\ntrim() 是直接更改相同格式的一列，删除这一列内容中的统一部分，然后重命名：\nselect device_id, trim(&#x27;http:/url/&#x27; from blog_url) as user_namefrom user_submit\n\n​\t\n\nsubstring_index()\nsubstring_index() 是将字符串切割，1表示保留字符串的左边👈，-1表示保留字符串的右边👉：\nselect device_id, substring_index(blog_url, &#x27;/url/&#x27;, -1) as user_namefrom user_submit\n\n还有一个用法是计数：\nSUBSTRING_INDEX(str,delim,count) 返回从字符串str分隔符 delim 在计数发生前的子字符串。如果计数是正的，则返回一切到最终定界符(从左边算起)的左侧。如果count是负数，则返回一切最终定界符(从右边算起)的右侧。SUBSTRING_INDEX() 搜寻在delim时进行区分大小写的匹配。\nSELECT SUBSTRING_INDEX(&#x27;www.somewebsite.com&#x27;,&#x27;.&#x27;,2);\n\nOutput: &#39;www.somewebsite&#39;\n\nsubstr()\nsubstr() 是用具体位置（数字）来表示从哪开始截取的，参数里还包括截取的长度：\nselect device_id, substr(blog_url, 11, length(blog_url)-10) as user_namefrom user_submit\n\n\n\nreplace()\nreplace() 就是替换函数：\nsleect device_id, replace(blog_url, &#x27;http:/url/&#x27;, &#x27;&#x27;) as user_namefrom user_submit\n\nSQL32 截取出年龄描述题目：现在运营举办了一场比赛，收到了一些参赛申请，表数据记录形式如下所示，现在运营想要统计每个年龄的用户分别有多少参赛者，请取出相应结果\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\nage\nnumber\n\n\n\n27\n1\n\n\n26\n1\n\n\n25\n1\n\n\n23\n1\n\n\n22\n1\n\n\nselect substring_index(substring_index(profile, &#x27;,&#x27;, -2), &#x27;,&#x27;, 1) as age, count(device_id)from user_submitgroup by age\n\n","categories":["刷题笔记"],"tags":["SQL","字符串"]},{"title":"Kaggle|House Pricing|Comprehensive data exploration with python","url":"/2023/02/23/HousePricing/","content":"\nFrom kaggle: Comprehensive data exploration with python\n\nNotes偏度(skewness)和峰度(kurtosis）\nskewness 衡量数据分布的非对称程度\n正态分布 skewness &#x3D; 0\n右偏分布 skewness &gt; 0\n左偏分布 skewness &lt; 0\n\n\n\n\n\nkurtosis 表示概率密度曲线的峰值高低（峰的尖度）\n\n正态分布（&#x3D; 3）\n厚尾（&gt; 3）最低\n瘦尾（&lt; 3）最尖\n\n\n\n\nheaptmap怎么看相关性中间对角线永远是最相关的\n标准化from sklearn.preprocessing import StandardScaler\n🤖️ 当进行单个变量的标准化处理时，我们通常需要将其转换为一个二维数组。这是因为，对于单个变量来说，它只有一个维度，而进行标准化处理时需要在某个轴上进行运算，因此需要将其转换为一个具有多个维度的数组，以便进行运算。\n可以使用 print() 函数输出 area_2d 的值，例如:\nimport numpy as np# create a 1D numpy array with 5 valuesarea = np.array([1000, 1500, 1200, 1800, 2000])# reshape the 1D array to a 2D array with 5 rows and 1 columnarea_2d = area.reshape(-1, 1)print(area_2d)\n\n输出结果应该是：\n[[1000] [1500] [1200] [1800] [2000]]\n\n可以看到，这个二维数组包含5行1列，每一行代表一个样本，每一列代表一个特征（在这个例子中，只有一个特征）。\n如何解释probplot的结果红色线条表示正态分布，蓝色线条表示样本数据，蓝色越接近红色参考线，说明越符合预期分布（正态分布）。这个图通常与sns.distplot直方图与连续概率密度估计图一起使用。![image-20230224124152190](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230224124152190.png)\n禁止日志信息from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)\n\n这段代码的作用是禁止 matplotlib 输出一些不必要的日志信息，这些信息可能会干扰我们的结果。在这段代码中，我们将 matplotlib.axes._axes 模块的日志级别设置为 ERROR，这样 matplotlib 就不会输出该模块的日志信息了。具体来说，该代码会禁止 matplotlib 输出一些与对数轴有关的警告信息，这些信息通常在我们对数据进行可视化时出现。\n图像输出前空白from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)data.plot.scatter(x = var, y = &#x27;SalePrice&#x27;, ylim = (0,800000))\n\n👧🏻 为什么我运行完这一段代码之后在图像输出之前会有一大段空白，并且显示&lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;\tylabel=&#39;SalePrice&#39;&gt; No results.呢？\n🤖️ 这段代码的输出包括两个部分：第一部分是图像，第二部分是输出的对象类型和空结果。输出对象类型的信息是因为 data.plot.scatter() 函数返回了一个 AxesSubplot 对象，而空结果是因为在代码的最后没有加上 plt.show() 函数。\n因此，你可以将代码修改为如下形式来避免输出空结果：\nfrom matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)ax = data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0, 800000))plt.show()\n\n在这里，我们使用了 plt.show() 函数来显示图像，并且将 data.plot.scatter() 的输出赋值给了一个变量 ax，以便我们可以对图像进行更多的操作。\ncorrcoef() + 转置cm = np.corrcoef(df_train[cols].values.T)\n🤖️ 在Numpy中，corrcoef函数用于计算两个或多个变量之间的相关系数矩阵，该矩阵中的每个元素代表两个变量之间的相关性。如果有两个变量，那么corrcoef返回一个2x2的矩阵，其中第(i,j)个元素为第i个变量和第j个变量之间的相关系数。如果有多个变量，则返回一个n×n的矩阵，其中n为变量的个数。corrcoef函数的参数是需要计算相关系数的变量矩阵或数组，这些变量可以存储在Numpy数组或者Pandas的DataFrame或Series中。其中，参数rowvar默认为True，表示对每一行进行变量之间的相关性计算，参数bias默认为False，表示无偏估计。\n示例代码：\nimport numpy as npx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])corr_matrix = np.corrcoef(x)print(corr_matrix)\n\n输出结果为：\n[[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]\n\n上述代码中，x是一个3x3的矩阵，表示三个变量之间的相关系数矩阵。corr_matrix为corrcoef函数计算出的相关系数矩阵，由于每个变量与自身的相关性系数都为1，因此返回的结果为一个由1组成的矩阵。\n🤖️ df_train[cols]表示在DataFrame对象df_train中挑选出相关性最高的10个变量对应的列，其中cols是一个包含这些列名的列表。values将这些列转换为一个二维数组，其每一行代表一个样本，每一列代表一个变量。.T表示对这个二维数组进行转置，即将行与列交换，变成每一行代表一个变量，每一列代表一个样本的形式。这样的结果是一个10行n列的二维数组，其中n是训练数据集中的样本数。\n例如，如果有以下数据集：\n#scatterplotsns.set()cols = [&#x27;SalePrice&#x27;, &#x27;OverallQual&#x27;, &#x27;GrLivArea&#x27;, &#x27;GarageCars&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;FullBath&#x27;, &#x27;YearBuilt&#x27;]sns.pairplot(df_train[cols], size = 2.5)plt.show();\n\n如果选择相关性最高的3个变量’A’, ‘B’, ‘C’，则df_train[cols]将返回一个包含3个列的DataFrame对象，即：\n    A   B   C0   1   4   71   2   5   82   3   6   9\n\n通过.values转换成Numpy数组，即为：\narray([[1, 4, 7],       [2, 5, 8],       [3, 6, 9]])\n\n在此基础上进行.T操作，将得到一个3行的二维数组，即：\narray([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])\n\n这个二维数组包含了3个变量的取值，每一行代表一个变量，每一列代表一个样本。这个数组可用于计算相关系数矩阵。\n❗️ 🤖️不转置也可以生成相关系数矩阵和相关性热图，但是在数据集较大的情况下，由于相关系数矩阵是一个方阵，如果样本数很多，则相关系数矩阵的大小将非常大，计算量也会变得很大，因此运行速度会变慢。\n&#x3D;&#x3D;而转置操作可以将变量数作为矩阵的行数，这样就避免了矩阵非常大的问题，提高了计算效率，所以在处理大型数据集时，进行转置操作是一种有效的优化方式，可以加快计算速度。同时，转置后的热图可以更直观地展示变量之间的相关性，更便于观察和分析。&#x3D;&#x3D;\nheaatmap 绘制#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\n👧🏻 f和ax代表什么？\n🤖️ 在代码 f, ax = plt.subplots(figsize=(12, 9)) 中，f 和 ax 是两个变量名，分别表示图形画布和图形轴对象。\n具体来说，plt.subplots() 函数返回了一个包含画布和轴对象的元组 (fig, ax)，其中 fig 表示整个图形画布对象，而 ax 表示坐标轴对象。这个元组可以通过 Python 的解构赋值语法来拆分成单独的变量。\n在这段代码中，通过设置 figsize=(12, 9) 参数，指定画布大小为 12x9，然后将返回的元组分配给变量 f 和 ax，以便在后续代码中使用它们来绘制热力图。\n👧🏻 vmax &#x3D; .8是什么意思？\n🤖️ vmax = .8 是指在绘制热力图时，将数据映射到颜色图谱的最大值为 0.8。具体来说，这个参数用于设置热力图中颜色的亮度范围，最大值为 0.8 表示数据中最大值对应的颜色在颜色图谱中的亮度为 0.8，而其他值对应的颜色亮度将根据数据的分布情况自动调整。\n通过设置 vmax 参数，可以在热力图中强调数据中的极端值和重要特征。如果不设置这个参数，颜色图谱的最大值将自动匹配数据中的最大值，这可能会导致颜色映射范围太大或太小，影响热力图的可读性。\n删除原来dataframe中的某些行（2种方式）🤖️ 这种情况下直接drop，必须写inplace &#x3D; True才会修改原数据框：\ntop_2_index = df_train.sort_values(by=&#x27;GrLivArea&#x27;, ascending=False)[:2].indexdf_train.drop(top_2_index, inplace=True)\n\n🤖 也可以不使用 inplace=True 参数来修改 df_train，而是将 drop() 函数的返回值赋给一个新的 DataFrame：\ndf_train = df_train.drop(df_train[df_train[&#x27;Id&#x27;] == 1299].index)\n\n这样，将删除 Id 为 1299 的行之后的 DataFrame 赋值给了 df_train。需要注意的是，这种方法会返回一个修改后的副本，而不会直接修改原始 DataFrame。\n👧🏻 修改后的副本和原来的dataframe不是都叫df_train吗？这样跟直接修改原来的dataframe有什么区别呢？\n🤖️ 确实，修改后的副本和原始的 DataFrame 都叫做 df_train，它们的名称是一样的，但它们的内存地址不同，它们是两个不同的对象。当你执行 df_train = df_train.drop(df_train[df_train[&#39;Id&#39;] == 1299].index) 时，drop() 函数返回一个新的 DataFrame 对象，然后将这个新的对象赋值给原始的 df_train，因此这个语句会改变原始的 df_train。而如果你在 drop() 函数中使用 inplace=True 参数，就会直接修改原始的 DataFrame，不会返回新的对象。这两种方法的最终效果是一样的，但是修改副本和直接修改原始 DataFrame 的方式可能对代码的可读性和维护性有一定的影响。\n🤖 如果使用了赋值操作符 “&#x3D;”，将原始数据框赋值给副本，那么它们的变量名会相同，很容易让人混淆。为了避免混淆，可以在变量名上添加后缀或前缀以示区别，例如在副本的变量名中加入”_copy”或”_modified”等后缀。在实际应用中，需要根据具体情况决定如何命名变量以便更好地区分它们。\n四种assumptions最终得出结果要看因变量是否符合多变量分析中的假设。\n\nNormality 正态分布：用 histogram + normal probability plot\nRemember that &#x3D;&#x3D;univariate normality doesn’t ensure multivariate normality&#x3D;&#x3D;.\nIn big samples (&gt;200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that’s the main reason why we are doing this analysis.\n异质性就是说研究的样本的重要属性上存在差异，比如人和人之间的消费习惯可能大相径庭，这样你记录1000个人10年的月消费数据，即便他们收入流和资产完全相同，消费流也可能截然不同。在统计性质上，这种不同表现为异方差。所以在计量模型上，横截面数据和面板数据经常出现，也可以说总会存在异质性问题。\n\n\n\n\nHomoscedasticity 同方差性：用 graphically\nHomoscedasticity refers to the ‘assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)’ (Hair et al., 2013). \nHomoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n\nLinearity 线性\nThe most common way to assess linearity is to &#x3D;&#x3D;examine scatter plots and search for linear patterns&#x3D;&#x3D;. If patterns are not linear, it would be worthwhile to explore data transformations.\n\n\nAbsence of correlated errors 无相关错误\nCorrelated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there’s a relationship between these variables. This occurs often in time series, where some patterns are &#x3D;&#x3D;time related&#x3D;&#x3D;. We’ll also not get into this. \nHowever, &#x3D;&#x3D;if you detect something, try to add a variable that can explain the effect you’re getting&#x3D;&#x3D;. That’s the most common solution for correlated errors.\n\n\n\npyplot和matplotlib的关系🤖 pyplot 是 Matplotlib 库中的一个模块，用于创建图形和图表。在 Matplotlib 中，&#x3D;&#x3D;pyplot 模块被用作绘图工具的接口&#x3D;&#x3D;，提供了一些方便的函数和方法来创建和操作图形、轴、图例和注释等元素。\nMatplotlib 是一个数据可视化库，提供了各种各样的绘图函数和工具，可以用于创建折线图、散点图、柱状图、饼图等各种类型的图表。而 pyplot 模块则是 Matplotlib 中最常用的绘图工具之一，它提供了很多方便的函数和方法，使得绘制各种类型的图表变得更加简单易用。因此，pyplot 可以被认为是 Matplotlib 库中最主要的一个模块。\n同方差性(Homoskedasticity)和异方差性(Heteroskedasticity)👦 写一个通俗的解释：比如说 $income&#x3D;b*education+e$ 这个计量模型,描述了教育水平与收入的关系，e为残差，OLS等传统计量框架要求残差e不与任何变量相关,即: $E(e|x)&#x3D;0$。\n一种常见的残差e与x相关的方式是: 残差e的方差与变量x相关，这时候，残差e的方差会随着x变动而变动，因此方差是异质性的. 这被称为异方差问题。\n异方差问题会导致什么?\n拿上边那个教育-工资的例子来看，一般来说，受教育水平越高的人群收入变动越大，而教育水平较低的人群收入相差不会太大 (比如说，有最低工资法)，这时就出现异方差问题了, 因为e的方差会随着教育水平x的增大而增大。\n\n异方差存在的时候,大多数情况下,OLS估计出的方差会比实际的方差要小(当然, 小部分情况下估计值会比实际的大)，&#x3D;&#x3D;因此会过高地估计系数b的显著性&#x3D;&#x3D; (因为, 系数的t值&#x3D;系数&#x2F;标准差)，**&#x3D;&#x3D;而不会影响系数估计值的大小&#x3D;&#x3D;**(考虑异方差影响系数大小的问题是近几年的焦点和难点，略过不提)。\n👨 同方差性是我们对OLS回归残差的一个要求，即 &#x3D;&#x3D;$\\epsilon \\sim N(0, \\sigma^2)$&#x3D;&#x3D; 。简单的说就是残差必须是随机的，这个分布是我们用来描述随机分布的一种方法 – 均值为0，方差为sigma方。\n异方差则说明残差不满足这个正态分布。\n概括而言，同方差说明回归的残差项是随机的，异方差说明残差项不随机，你的回归设置或者变量的选取有问题，需要修正。\nget_dummies()#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\n🤖 这段代码使用了get_dummies()函数将数据框中的分类变量转换成了虚拟变量（dummy variable）。\n虚拟变量是指将一个分类变量拆分成多个二元变量的过程。例如，如果有一个名为color的分类变量，其中包括红色、蓝色和绿色三个类别，那么将color转换成虚拟变量的过程就是创建三个新的二元变量，分别代表红色、蓝色和绿色，如果样本属于红色类别，则对应的红色虚拟变量值为1，其余两个虚拟变量的值为0，以此类推。\nget_dummies()函数会自动识别出所有的分类变量，并将它们转换成虚拟变量。它还有一些其他的参数，例如drop_first可以指定是否要删除每个变量的第一个虚拟变量，以避免多重共线性问题。\nProcedure“理解问题 -&gt; 单变量分析 -&gt; 多变量分析 -&gt; 数据清洗 -&gt; 验证假设”\n1⃣️ Understand the problem. We’ll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\n观察 columns\n\n2⃣️ Univariable study. We’ll just focus on the dependent variable (‘SalePrice’) and try to know a little bit more about it.\n\n使用 describe\n使用直方图\n分析偏度和峰度\n使用散点图和箱图观察目标变量和其他变量之间的关系\n\n3⃣️ Multivariate study. We’ll try to understand how the dependent variable and independent variables relate.\n\n热图看所有变量之间的关系\n热图看跟目标变量最相关的变量之间的关系\nscatterplot 看目标变量及最相关的变量之间的关系\n\n4⃣️ Basic cleaning. We’ll clean the dataset and handle the missing data, outliers and categorical variables.\n\n缺失值\n\n看所有的变量中缺失值占比\n\n剔除缺失变量：对因变量不重要的可以直接剔除整个变量；有替代变量的可以直接剔除整个变量；不剔除整个变量的可以仅将包含缺失值的单个样本剔除\n\n检查无缺失值\n\n\n\n离群值\n\n[数据标准化][4]，设置一个阈值来判断离群值，输出过高&#x2F;过低的离群值\n使用散点图观察某个自变量和因变量的关系，有了上一步得到的离群值，我们可以在散点图中一眼看出离群值在哪\n保留遵循趋势的离群值，删除违背趋势的离群值\n\n\n\n5⃣️ Test assumptions. We’ll check if our data meets the assumptions required by most multivariate techniques.\n\n正态分布\n看histogram：偏度和峰度\n看常态机率图：数据分布应紧跟代表正态分布的对角线\n\n\n异方差性\n散点图可视化，一头紧凑一头分散，或中间紧凑两头分散，或中间分散两头紧凑\n\n\n虚拟化变量\n\nStep1: Understand our data#check the decorationdf_train.columns\n\n\n Look as each variable and try to understand their meaning and relvance to this problem. Although it’s time-comsuming, it gives us the flavour of our dataset.\n\n\nWe should focus 5 aspects. Create an Excel spreadsheet with following columns:\n\nVariable\nType -&gt; ‘numerical’ or ‘categorical’\nSegment -&gt; Define the segments of all variables then identify them. (eg. building&#x2F; space &#x2F;location)\nExpectation -&gt; the variable infulence on the dependent variable. (high&#x2F; medium&#x2F; low)\nConclusion -&gt; the importance of the variable \nComments -&gt; any general comments that occured to us\n\n\nExpectation will give us ‘sixth sense’. In order to fill it, we should read the description of all the variables one by one, ask ourselves:\n\nDo we care this variable when we are buying a house?\nHow important would this variable is?\nIs this information already decribed in any other variable?\n\n\nFocus the variables with ‘high’ expectation. Generate some scatter plots between those variables and the dependant variable(SalePrice), filling the conclusion column which is the correction of our expectations.\n\nYou may give up some of your expecting variables after vasualizing them.\n\n\n\nStep2: Univariable analysis: ‘SalePrice’\ndescribe()\nhistogram- normal distribution? (positive) skewness? peakedness?\n[skewness and kurtosis][1]\n\n#data#descriptive statistics summarydf_train[&#x27;SalePrice&#x27;].describe()#graph#histogramsns.distplot(df_train[&#x27;SalePrice&#x27;]);#data#skewness and kurtosisprint(&quot;Skewness: %f&quot; % df_train[&#x27;SalePrice&#x27;].skew())print(&quot;Kurtosis: %f&quot; % df_train[&#x27;SalePrice&#x27;].kurt())\n\nRelationship with numerical variables\nscatter plots - if there is a linear (exponential) reaction\n\n#graph#scatter plot grlivarea/salepricevar = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));#scatter plot totalbsmtsf/salepricevar = &#x27;TotalBsmtSF&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));\n\nRelationship with categorical fratures\nbox plots\n\n#box plot overallqual/salepricevar = &#x27;OverallQual&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(8, 6))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);var = &#x27;YearBuilt&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);plt.xticks(rotation=90);\n\n🤖️ 这段代码使用了Python中的Pandas和Seaborn库，用于绘制房价数据集中整体质量（OverallQual）与房价（SalePrice）之间的箱型图。具体解释如下：\n\nvar = &#39;OverallQual&#39;：将变量名OverallQual赋值给变量var，表示要绘制的x轴变量是整体质量。\ndata = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1)：将训练数据集df_train中的SalePrice和OverallQual列拼接在一起，并存储在名为data的变量中。这里使用了Pandas库中的concat函数。\nf, ax = plt.subplots(figsize=(8, 6))：创建一个8x6英寸大小的绘图对象，并将其存储在名为f的变量中，将其轴对象存储在名为ax的变量中。这里使用了Matplotlib库。\nfig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)：使用Seaborn库中的boxplot函数绘制箱型图，x轴为OverallQual，y轴为SalePrice，数据来源为data。将绘制结果存储在名为fig的变量中。\nfig.axis(ymin=0, ymax=800000)：将y轴的范围设定为0到800000，使得箱型图的纵坐标范围更加合适。这里使用了Matplotlib库中的axis函数。\n\nStep3: Multivariable analysis\n[heatmap][2] style - from a universla perspective\n\n#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\n🤖️ 这段代码用于绘制数据集中的相关性矩阵的热力图。以下是每一行的解释：\n\ncorrmat = df_train.corr(): 计算数据集 df_train 中所有数值型变量之间的相关性矩阵，其中 corr() 是 pandas 库中计算相关系数的函数。\nf, ax = plt.subplots(figsize=(12, 9)): 创建一个大小为 12x9 的图形画布，并将其保存在变量 f 和 ax 中，用于绘制相关性矩阵的热力图。这里用到了 matplotlib 库中的 subplots() 函数。\nsns.heatmap(corrmat, vmax=.8, square=True): 使用 seaborn 库中的 heatmap() 函数将相关性矩阵绘制成热力图，其中 corrmat 是相关性矩阵的数据，vmax 是热力图中颜色的最大值，square 参数用于指定热力图的形状是否为正方形。\n\n通过绘制相关性矩阵的热力图，可以更直观地了解数据集中各个变量之间的相关性，以便进行数据分析和建模。\n&#x3D;&#x3D;❗️Notice if some of the independent variables have a similar color distribution in the heat map! This proves that they play a similar role in the overall model, and perhaps it is possible to keep only the one variable with the least missing values in the cleaning.&#x3D;&#x3D;\n#saleprice correlation matrixk = 10 #number of variables for heatmapcols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].indexcm = np.corrcoef(df_train[cols].values.T)sns.set(font_scale=1.25)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#x27;.2f&#x27;, annot_kws=&#123;&#x27;size&#x27;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)plt.show()\n\n🤖️ 这段代码使用了Python中的Numpy、Pandas和Seaborn库，用于绘制房价数据集中各变量之间的相关系数热图。具体解释如下：\n\nk = 10：将热图中展示的相关系数最高的10个变量挑选出来。\ncols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index：从相关系数矩阵corrmat中选择SalePrice列与其他列的相关系数最高的10个变量，将其索引存储在cols变量中。\ncm = np.corrcoef(df_train[cols].values.T)：从训练数据集df_train中挑选出相关系数最高的10个变量，计算其相关系数矩阵，并将其存储在cm变量中。这里使用了Numpy库中的[corrcoef函数][3]。\nsns.set(font_scale=1.25)：设置Seaborn库中的字体大小。\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.2f&#39;, annot_kws=&#123;&#39;size&#39;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)：绘制热图。其中，参数cm是一个相关系数矩阵，参数cbar表示是否显示颜色条，参数annot表示是否显示每个格子中的数值，参数square表示是否将每个格子设置为正方形，参数fmt表示显示数值时保留两位小数，参数annot_kws表示注释的参数，这里设置字体大小为10，yticklabels和xticklabels表示行和列的标签名，这里都为cols.values。将绘制结果存储在名为hm的变量中。\nplt.show()：显示绘制出的热图。这里使用了Matplotlib库中的show函数。\n\nsns.set()cols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].index.tolist()sns.pairplot(df_train[cols], height = 2.5)plt.show()\n\n\n\nStep4: Missing dataMissing data#missing datatotal = df_train.isnull().sum().sort_values(ascending=False)percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)missing_data = pd.concat([total, percent], axis=1, keys=[&#x27;Total&#x27;, &#x27;Percent&#x27;])missing_data.head(20)\n\n🤖️\n\ndf_train.isnull()：判断数据集 df_train 中每个元素是否为缺失值，返回一个与 df_train 大小相同的布尔矩阵，内容是True&#x2F;False。\ndf_train.isnull().sum()：将布尔矩阵中每列（变量）中的缺失值数量相加，得到每列缺失值的总数。\ndf_train.isnull().count()：将布尔矩阵中每列（变量）中的元素（无论是否缺失）数量相加，得到每列元素的总数。\n\n#dealing with missing datadf_train = df_train.drop((missing_data[missing_data[&#x27;Total&#x27;] &gt; 1]).index, axis = 1)df_train = df_train.drop(df_train.loc[df_train[&#x27;Electrical&#x27;].isnull()].index)df_train.isnull().sum().max() #just checking that there&#x27;s no missing data missing...\n\nOutliers#standardizing datafrom sklearn.preprocessing import StandardScalersaleprice_scaled = StandardScaler().fit_transform(np.array(df_train[&#x27;SalePrice&#x27;])[:, np.newaxis]);low_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][:10]high_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][-10:]print(&#x27;outer range (low) of the distrubution:&#x27;)print(low_range)print(&#x27;\\n outer range (high) of the distribution:&#x27;)print(high_range)\n\n\n\nStep5: Getting hard coreNormality: histogram + normal probability plot#histogram and normal probability plotfrom scipy.stats import normsns.distplot(df_train[&#x27;SalePrice&#x27;], kde = True,fit = norm);fig = plt.figure()res = stats.probplot(df_train[&#x27;SalePrice&#x27;], plot = plt)\n\n\n 🤖️ histplot不支持直接显示拟合的正态分布曲线。如果要同时显示正态分布曲线，可以使用displot。\n\n&#x3D;&#x3D;In case of positive skewness, log transformations usually works well.&#x3D;&#x3D;\ndef dist_probplot(x):    sns.distplot(df_train[x], fit = norm)    fig = plt.figure()    res = stats.probplot(df_train[x], plot = plt)\n\n\n#applying log transformationdf_train[&#x27;SalePrice&#x27;] = np.log(df_train[&#x27;SalePrice&#x27;])dist_probplot(&#x27;SalePrice&#x27;)\n\n\ndist_probplot(&#x27;TotalBsmtSF&#x27;)\n\n\n\nOk, now we are dealing with the big boss. What do we have here?\n\nSomething that, in general, presents skewness.\nA significant number of observations with value zero (houses without basement).\nA big problem because the value zero doesn’t allow us to do log transformations.\n\n\n处理方法：将非零值做对数转换，零值还是零，但是若执行：\n![image-20230228093921098](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230228093921098.png)\n会出现以上报错。\n🤖️ 出现RuntimeWarning: divide by zero encountered in log的原因是在对数变换时遇到了 0，因为 log(0) 是没有定义的，所以会产生警告。可以将值为 0 的观测在变换前用一个极小的数代替，比如可以用 1e-6 代替 0。\n👧🏻 但是这段代码不是说只有不等于0时才会进行对数变换吗？为什么还会遇到log(0)的情况呢？\n🤖️ 这是由于浮点数的精度问题导致的。可能有一些值在处理之后非常接近0，但实际上并不等于0，因此在进行np.log()函数计算时，会返回一个非常大的负无穷值，比如-log(1e-15)&#x3D;-34.538776394910684。这也就是报错中提到的“divide by zero encountered in log”错误，其实并非是真正意义上的除零操作，而是一个数学上的约定。\n🤖️ 为了解决这个问题，一种常见的方法是在对数变换前，先对数据中的所有非零值加上一个极小的常数，比如1e-8，避免出现log(0)的情况，同时由于这个常数值非常小，对整体数据的统计特征影响也非常小。因此可以将代码修改为：\nepsilon = 1e-8  # 定义一个极小值df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] + epsilon)\n\n👧🏻 所以其实应该是这样：\n#transform datadf_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;])\n\n#histogram and normal probability plotsns.distplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], fit = norm)fig = plt.figure()res = stats.probplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], plot = plt)\n\n👧🏻 plot &#x3D; plt是什么意思？\n🤖 plot=plt是将&#x3D;&#x3D;Matplotlib库的当前pyplot对象&#x3D;&#x3D;传递给probplot函数中的参数plot。这样做是为了让probplot函数能够在当前的Matplotlib图形对象中绘制概率图。通常，我们在创建一个新的Matplotlib图形对象时会使用plt.figure()函数，并为其指定一个新的名称或编号。但是，如果我们已经有了一个现有的图形对象，我们可以将其传递给plot参数，使得probplot函数在该对象中绘制图形。\n👧🏻 我应该怎么获取Matplotlib库的当前pyplot对象？\n🤖 你可以使用matplotlib.pyplot.gcf()函数来获取当前图形对象。gcf()代表“get current figure”，它将返回最近创建的Figure对象（如果没有现有Figure，则会创建一个新的）。\nHomoscedasticity: graphically\nDepartures from an equal dispersion are shown by such shapes as &#x3D;&#x3D;cones (small dispersion at one side of the graph, large dispersion at the opposite side)&#x3D;&#x3D; or &#x3D;&#x3D;diamonds (a large number of points at the center of the distribution)&#x3D;&#x3D;.\n\n#scatter plotplt.scatter(df_train[&#x27;GrLivArea&#x27;], df_train[&#x27;SalePrice&#x27;]);\n\n#scatter plotplt.scatter(df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;TotalBsmtSF&#x27;], df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;SalePrice&#x27;]);\n\n\n\nDummy variables#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\n","categories":["刷题笔记"],"tags":["Python","kaggle","data exploration"]},{"title":"Hello World","url":"/2023/02/07/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"SQL-计算用户留存率","url":"/2023/02/07/SQL-%E8%AE%A1%E7%AE%97%E7%94%A8%E6%88%B7%E6%AC%A1%E6%97%A5%E7%95%99%E5%AD%98%E7%8E%87/","content":"计算用户次日留存率如果只是计算用户的次日留存率，那么使用date_sub()函数就够了：\nselect avg(if(b.device_id is not null, 1, 0)) as avg_retfrom(    select distinct device_id, date    from question_practice_detail) aleft join(    select distinct device_id, date_sub(date, interval 1 day) as date    from question_practice_detail) bon a.device_id = b.device_id and a.date = b.date\n\n\ndate_sub() 函数：\n\nSELECT DATE_SUB(‘2010-08-12’, INTERVAL 3 DAY) AS NewDate \n结果： 2010-08-09\n\nSELECT DATE_SUB(‘2010-08-12’, INTERVAL ‘3-2’ YEAR_MONTH) AS NewDate \n结果： 2007-06-12\n\nSELECT DATE_SUB(‘2011-09-14 2:44:36’, INTERVAL ‘2:26’ HOUR_MINUTE) AS NewDate \n结果： 2011-09-14 00:18:36\n\n\n\n\n基于SQL的留存率计算\n转自知乎：基于SQL的留存率计算\n\n一、什么是留存率互联网行业里，留存率是用于反映网站、互联网应用或网络游戏的运营情况的统计指标，其具体含义为在统计周期（周&#x2F;月）内，每日活跃用户数在第N日仍启动该App的用户数占比的平均值。其中N通常取2、3、7、14、30，分别对应次日留存率、三日留存率、周留存率、半月留存率和月留存率。\n留存率常用于反映用户粘性，当N取值越大、留存率越高时，用户粘性越高。\n二、留存率的计算\n留存率 &#x3D; 登陆用户数&#x2F;新增用户数 * 100%\n\n新增用户数：在当前时间段新注册（或新访问）的用户数；\n\n登录用户数：在统计的时间段至少登录过一次的用户数；\n\n次日留存率：在次日至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️3日留存率：在往后3天内至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️7日留存率：在往后7天内至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️15日留存数：当天新增的用户数，在往后7天内至少登录过一次的用户，在往后第8天到第14天内至少再登陆过一次的用户数\n➡️ 3日和7日，至少登陆过一次；15日，7天为一段，在每段内至少登录一次！\n\n\nSQL中计算用户的留存率\n新增用户数\n\n由于数据过大，这截取时间2017.11.26~2017.12.03为例。\n首先计算分母，这里有的算法是用新增用户数，有的算法是用活跃用户数。\n⚠️注意：新增用户数与活跃用户数并不相等，活跃用户数包含新增用户数。活跃用户数，当天的访问人数，也就是UV。\n-- 每位用户的最早登录日期SELECT 用户ID, MIN(日期) AS 最早登录日期FROM userbehaviorWHERE 日期 &gt; &#x27;2017-11-25&#x27;AND 日期 &lt; &#x27;2017-12-04&#x27;GROUP BY 用户ID\n\n\n再从上表中计算出每天的新增人数，❗️算新增人数用的日期是最早登录日期！\nSELECT 最早登录日期 AS 日期, COUNT(DISTINCT 用户ID) AS 新增人数FROM(SELECT 用户ID, MIN(日期) AS 最早登录日期     FROM userbehavior     WHERE 日期 &gt; &#x27;2017-11-25&#x27; AND 日期 &lt; &#x27;2017-12-04&#x27;     GROUP BY 用户ID) AS fGROUP BY 最早登录日期\n\n\n以下是活跃用户数的算法，二者确实数值上并不相等。❗️算每日活跃用户数用的日期就是当天的日期！\n-- 每天活跃用户数=UV 访客数SELECT 日期, COUNT(DISTINCT 用户ID) AS 活跃用户数FROM userbehaviorwhere 日期 &gt;&#x27;2017-11-25&#x27; AND 日期 &lt; &#x27;2017-12-04&#x27;GROUP BY 日期\n\n\n抽取7天的活跃用户数。\n\n次日留存的用户数\n\n\n次日的时间间隔为1，涉及到时间间隔采用自联结。\n\nSELECT a.用户ID, a.`日期` AS atime, b.`日期` AS btimeFROM userbehavior AS a LEFT JOIN userbehavior AS bON a.`用户ID` = b.`用户ID`WHERE a.`日期` &gt; &#x27;2017-11-25&#x27; AND a.`日期` &lt; &#x27;2017-12-04&#x27;\n\n\n将上表存为视图C，\nCREATE VIEW C(用户ID, atime, btime)ASSELECT a.用户ID, a.`日期`, b.`日期` from userbehavior AS a LEFT JOIN userbehavior AS bON a.`用户ID`=b.`用户ID`WHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;;\n\n\n计算时间间隔用timestampdiff函数\n\nSELECT `用户ID`, TIMESTAMPDIFF(DAY, atime, btime)AS 时间间隔FROM (  SELECT a.用户ID, a.日期 as atime, b.日期 as btime  FROM userbehavior as a LEFT JOIN userbehavior as b  ON a.用户ID=b.`用户ID`  WHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;) AS c-- 这里直接设置为表c\n\n\n得到了用户的时间间隔\n3.用case语句筛选出时间间隔为1的数据，并且进行计数\nSELECT *, COUNT(DISTINCT case when 时间间隔 = 1 then `用户ID`\t\t\tELSE NULL\t\t\tEND)AS 次日留存数FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n次日留存率\n\n次日留存率&#x3D;次日留存用户数&#x2F;当日活跃用户数\nSELECT *,COUNT(DISTINCT case when 时间间隔 = 1 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 次日留存率FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n\n三、三日以及N日留存的计算只需要修改时间间隔&#x3D;N即可。\nSELECT 日期,COUNT(DISTINCT 用户ID AS 活跃用户数,COUNT(DISTINCT case when 时间间隔=1 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 次日留存率COUNT(DISTINCT case when 时间间隔=3 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 三日留存率COUNT(DISTINCT case when 时间间隔=7 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 七日留存率FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN 每日新增用户数表 as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d","categories":["刷题笔记"],"tags":["SQL"]}]