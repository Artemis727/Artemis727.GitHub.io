[{"title":"Data Science R Basic","url":"/2023/02/09/Data-Science-R-Basic/","content":"Words &amp; Expressions\nin a nested way 嵌套\n\nconcatenate 连接 c() -&gt; all items in ‘()’ to a vector\n\n\nWhy use ‘[[’ instead of ‘[’ to access variables?We extract the population like this:\np &lt;- murders$population\n\nIf you instead try to access a column with just one bracket,\nmurders[&quot;population&quot;]\n\nR returns a subset of the original data frame containing just this column. This new object will be of class data.frame rather than a vector. To access the column itself you need to use either the $ accessor or the double square brackets [[:\nmurders[[&quot;population&quot;]]\n\n\n\nR- sorting functions\nsort()\n# sort()排序，排序结果不可逆转# 默认是升序# decreasing为TRUE，表示降序# decreasing为FALSE，表示升序#排序后并不会修改原对象的值#示例如下：&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; #sort()默认为从小到大（升序）排序，等同于decreasing=FALSE&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; sort(a,decreasing = F) [1]  3  4  5  6  7  9 10 13 16 22&gt; #decreasing=TRUE,为从大到小（降序）排序&gt; sort(a,decreasing = T) [1] 22 16 13 10  9  7  6  5  4  3#排序并不会修改原对象的值，a仍为原来未排序的a&gt; a [1]  3  9 16  6  7  4 22  5 10 13\n\n\n\nrank()\n用法：rank(a)函数说明：指出当前向量中各元素大小的排名，默认升序函数还有其他的参数：rank(x &#x3D; data, na.last &#x3D; TRUE)x 表示待排序的向量na.last 表示是否排序时是否将NA放在最后面，默认忽略NA\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; order(a) [1]  1  6  8  4  5  2  9 10  3  7#说明：在向量a中，3是第一小的数，位置下标为1；4是第二小的数，位置下标为6；最大的数是22，位置下标为7#a[order(a)] 等同于sort(a)&gt; a[order(a)]  [1]  3  4  5  6  7  9 10 13 16 22\n\n\n\norder()\n说明：返回的值表示位置，默认是升序，依次对应的是向量的最小值、次小值、第三小值…最大值\n用法：order(a), a为要排序的向量order(… &#x3D; data, na.last &#x3D; TRUE,decreasing &#x3D; TRUE)… 表示待排序向量na.last 表示时候将NA值放在最后面（默认排序忽略NA）decreasing 表示是否按照降序排序，默认升序。\n&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) [1]  3  4  5  6  7  9 10 13 16 22&gt; rank(a) [1]  1  6  9  4  5  2 10  3  7  8 #说明：向量a中的第一个数为3，是最小的，故排名为1；第二个数是9，是第六小的数，排名为6\n\n\n\n比较\n&gt; a &lt;- c(3,9,16,6,7,4,22,5,10,13)&gt; a [1]  3  9 16  6  7  4 22  5 10 13&gt; sort(a) #将a**从小到大排序并列出** [1]  3  4  5  6  7  9 10 13 16 22&gt; order(a) #返回从小到大的数的**位置下标**，a[order(a)]=sort(a) [1]  1  6  8  4  5  2  9 10  3  7&gt; rank(a) #返回a中**每个数的排名（从小到大）** [1]  1  6  9  4  5  2 10  3  7  8&gt; a [1]  3  9 16  6  7  4 22  5 10 13#注意：经过sort()、order()、rank()排序后，a不改变\n\norder和rank的区别在于一个返回的是index，一个是rank，index不一定是数字。\n\n\n","categories":["刷题笔记"],"tags":["R"]},{"title":"Kaggle|House Pricing|Comprehensive data exploration with python","url":"/2023/02/23/HousePricing/","content":"\nFrom kaggle: Comprehensive data exploration with python\n\nNotes偏度(skewness)和峰度(kurtosis）\nskewness 衡量数据分布的非对称程度\n正态分布 skewness &#x3D; 0\n右偏分布 skewness &gt; 0\n左偏分布 skewness &lt; 0\n\n\n\n\n\nkurtosis 表示概率密度曲线的峰值高低（峰的尖度）\n\n正态分布（&#x3D; 3）\n厚尾（&gt; 3）最低\n瘦尾（&lt; 3）最尖\n\n\n\n\nheaptmap怎么看相关性中间对角线永远是最相关的\n标准化from sklearn.preprocessing import StandardScaler\n🤖️ 当进行单个变量的标准化处理时，我们通常需要将其转换为一个二维数组。这是因为，对于单个变量来说，它只有一个维度，而进行标准化处理时需要在某个轴上进行运算，因此需要将其转换为一个具有多个维度的数组，以便进行运算。\n可以使用 print() 函数输出 area_2d 的值，例如:\nimport numpy as np# create a 1D numpy array with 5 valuesarea = np.array([1000, 1500, 1200, 1800, 2000])# reshape the 1D array to a 2D array with 5 rows and 1 columnarea_2d = area.reshape(-1, 1)print(area_2d)\n\n输出结果应该是：\n[[1000] [1500] [1200] [1800] [2000]]\n\n可以看到，这个二维数组包含5行1列，每一行代表一个样本，每一列代表一个特征（在这个例子中，只有一个特征）。\n如何解释probplot的结果红色线条表示正态分布，蓝色线条表示样本数据，蓝色越接近红色参考线，说明越符合预期分布（正态分布）。这个图通常与sns.distplot直方图与连续概率密度估计图一起使用。![image-20230224124152190](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230224124152190.png)\n禁止日志信息from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)\n\n这段代码的作用是禁止 matplotlib 输出一些不必要的日志信息，这些信息可能会干扰我们的结果。在这段代码中，我们将 matplotlib.axes._axes 模块的日志级别设置为 ERROR，这样 matplotlib 就不会输出该模块的日志信息了。具体来说，该代码会禁止 matplotlib 输出一些与对数轴有关的警告信息，这些信息通常在我们对数据进行可视化时出现。\n图像输出前空白from matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)data.plot.scatter(x = var, y = &#x27;SalePrice&#x27;, ylim = (0,800000))\n\n👧🏻 为什么我运行完这一段代码之后在图像输出之前会有一大段空白，并且显示&lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;\tylabel=&#39;SalePrice&#39;&gt; No results.呢？\n🤖️ 这段代码的输出包括两个部分：第一部分是图像，第二部分是输出的对象类型和空结果。输出对象类型的信息是因为 data.plot.scatter() 函数返回了一个 AxesSubplot 对象，而空结果是因为在代码的最后没有加上 plt.show() 函数。\n因此，你可以将代码修改为如下形式来避免输出空结果：\nfrom matplotlib.axes._axes import _log as matplotlib_axes_loggermatplotlib_axes_logger.setLevel(&#x27;ERROR&#x27;)var = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis = 1)ax = data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0, 800000))plt.show()\n\n在这里，我们使用了 plt.show() 函数来显示图像，并且将 data.plot.scatter() 的输出赋值给了一个变量 ax，以便我们可以对图像进行更多的操作。\ncorrcoef() + 转置cm = np.corrcoef(df_train[cols].values.T)\n🤖️ 在Numpy中，corrcoef函数用于计算两个或多个变量之间的相关系数矩阵，该矩阵中的每个元素代表两个变量之间的相关性。如果有两个变量，那么corrcoef返回一个2x2的矩阵，其中第(i,j)个元素为第i个变量和第j个变量之间的相关系数。如果有多个变量，则返回一个n×n的矩阵，其中n为变量的个数。corrcoef函数的参数是需要计算相关系数的变量矩阵或数组，这些变量可以存储在Numpy数组或者Pandas的DataFrame或Series中。其中，参数rowvar默认为True，表示对每一行进行变量之间的相关性计算，参数bias默认为False，表示无偏估计。\n示例代码：\nimport numpy as npx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])corr_matrix = np.corrcoef(x)print(corr_matrix)\n\n输出结果为：\n[[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]\n\n上述代码中，x是一个3x3的矩阵，表示三个变量之间的相关系数矩阵。corr_matrix为corrcoef函数计算出的相关系数矩阵，由于每个变量与自身的相关性系数都为1，因此返回的结果为一个由1组成的矩阵。\n🤖️ df_train[cols]表示在DataFrame对象df_train中挑选出相关性最高的10个变量对应的列，其中cols是一个包含这些列名的列表。values将这些列转换为一个二维数组，其每一行代表一个样本，每一列代表一个变量。.T表示对这个二维数组进行转置，即将行与列交换，变成每一行代表一个变量，每一列代表一个样本的形式。这样的结果是一个10行n列的二维数组，其中n是训练数据集中的样本数。\n例如，如果有以下数据集：\n#scatterplotsns.set()cols = [&#x27;SalePrice&#x27;, &#x27;OverallQual&#x27;, &#x27;GrLivArea&#x27;, &#x27;GarageCars&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;FullBath&#x27;, &#x27;YearBuilt&#x27;]sns.pairplot(df_train[cols], size = 2.5)plt.show();\n\n如果选择相关性最高的3个变量’A’, ‘B’, ‘C’，则df_train[cols]将返回一个包含3个列的DataFrame对象，即：\n    A   B   C0   1   4   71   2   5   82   3   6   9\n\n通过.values转换成Numpy数组，即为：\narray([[1, 4, 7],       [2, 5, 8],       [3, 6, 9]])\n\n在此基础上进行.T操作，将得到一个3行的二维数组，即：\narray([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])\n\n这个二维数组包含了3个变量的取值，每一行代表一个变量，每一列代表一个样本。这个数组可用于计算相关系数矩阵。\n❗️ 🤖️不转置也可以生成相关系数矩阵和相关性热图，但是在数据集较大的情况下，由于相关系数矩阵是一个方阵，如果样本数很多，则相关系数矩阵的大小将非常大，计算量也会变得很大，因此运行速度会变慢。\n&#x3D;&#x3D;而转置操作可以将变量数作为矩阵的行数，这样就避免了矩阵非常大的问题，提高了计算效率，所以在处理大型数据集时，进行转置操作是一种有效的优化方式，可以加快计算速度。同时，转置后的热图可以更直观地展示变量之间的相关性，更便于观察和分析。&#x3D;&#x3D;\nheaatmap 绘制#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\n👧🏻 f和ax代表什么？\n🤖️ 在代码 f, ax = plt.subplots(figsize=(12, 9)) 中，f 和 ax 是两个变量名，分别表示图形画布和图形轴对象。\n具体来说，plt.subplots() 函数返回了一个包含画布和轴对象的元组 (fig, ax)，其中 fig 表示整个图形画布对象，而 ax 表示坐标轴对象。这个元组可以通过 Python 的解构赋值语法来拆分成单独的变量。\n在这段代码中，通过设置 figsize=(12, 9) 参数，指定画布大小为 12x9，然后将返回的元组分配给变量 f 和 ax，以便在后续代码中使用它们来绘制热力图。\n👧🏻 vmax &#x3D; .8是什么意思？\n🤖️ vmax = .8 是指在绘制热力图时，将数据映射到颜色图谱的最大值为 0.8。具体来说，这个参数用于设置热力图中颜色的亮度范围，最大值为 0.8 表示数据中最大值对应的颜色在颜色图谱中的亮度为 0.8，而其他值对应的颜色亮度将根据数据的分布情况自动调整。\n通过设置 vmax 参数，可以在热力图中强调数据中的极端值和重要特征。如果不设置这个参数，颜色图谱的最大值将自动匹配数据中的最大值，这可能会导致颜色映射范围太大或太小，影响热力图的可读性。\n删除原来dataframe中的某些行（2种方式）🤖️ 这种情况下直接drop，必须写inplace &#x3D; True才会修改原数据框：\ntop_2_index = df_train.sort_values(by=&#x27;GrLivArea&#x27;, ascending=False)[:2].indexdf_train.drop(top_2_index, inplace=True)\n\n🤖 也可以不使用 inplace=True 参数来修改 df_train，而是将 drop() 函数的返回值赋给一个新的 DataFrame：\ndf_train = df_train.drop(df_train[df_train[&#x27;Id&#x27;] == 1299].index)\n\n这样，将删除 Id 为 1299 的行之后的 DataFrame 赋值给了 df_train。需要注意的是，这种方法会返回一个修改后的副本，而不会直接修改原始 DataFrame。\n👧🏻 修改后的副本和原来的dataframe不是都叫df_train吗？这样跟直接修改原来的dataframe有什么区别呢？\n🤖️ 确实，修改后的副本和原始的 DataFrame 都叫做 df_train，它们的名称是一样的，但它们的内存地址不同，它们是两个不同的对象。当你执行 df_train = df_train.drop(df_train[df_train[&#39;Id&#39;] == 1299].index) 时，drop() 函数返回一个新的 DataFrame 对象，然后将这个新的对象赋值给原始的 df_train，因此这个语句会改变原始的 df_train。而如果你在 drop() 函数中使用 inplace=True 参数，就会直接修改原始的 DataFrame，不会返回新的对象。这两种方法的最终效果是一样的，但是修改副本和直接修改原始 DataFrame 的方式可能对代码的可读性和维护性有一定的影响。\n🤖 如果使用了赋值操作符 “&#x3D;”，将原始数据框赋值给副本，那么它们的变量名会相同，很容易让人混淆。为了避免混淆，可以在变量名上添加后缀或前缀以示区别，例如在副本的变量名中加入”_copy”或”_modified”等后缀。在实际应用中，需要根据具体情况决定如何命名变量以便更好地区分它们。\n四种assumptions最终得出结果要看因变量是否符合多变量分析中的假设。\n\nNormality 正态分布：用 histogram + normal probability plot\nRemember that &#x3D;&#x3D;univariate normality doesn’t ensure multivariate normality&#x3D;&#x3D;.\nIn big samples (&gt;200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that’s the main reason why we are doing this analysis.\n异质性就是说研究的样本的重要属性上存在差异，比如人和人之间的消费习惯可能大相径庭，这样你记录1000个人10年的月消费数据，即便他们收入流和资产完全相同，消费流也可能截然不同。在统计性质上，这种不同表现为异方差。所以在计量模型上，横截面数据和面板数据经常出现，也可以说总会存在异质性问题。\n\n\n\n\nHomoscedasticity 同方差性：用 graphically\nHomoscedasticity refers to the ‘assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)’ (Hair et al., 2013). \nHomoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n\nLinearity 线性\nThe most common way to assess linearity is to &#x3D;&#x3D;examine scatter plots and search for linear patterns&#x3D;&#x3D;. If patterns are not linear, it would be worthwhile to explore data transformations.\n\n\nAbsence of correlated errors 无相关错误\nCorrelated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there’s a relationship between these variables. This occurs often in time series, where some patterns are &#x3D;&#x3D;time related&#x3D;&#x3D;. We’ll also not get into this. \nHowever, &#x3D;&#x3D;if you detect something, try to add a variable that can explain the effect you’re getting&#x3D;&#x3D;. That’s the most common solution for correlated errors.\n\n\n\npyplot和matplotlib的关系🤖 pyplot 是 Matplotlib 库中的一个模块，用于创建图形和图表。在 Matplotlib 中，&#x3D;&#x3D;pyplot 模块被用作绘图工具的接口&#x3D;&#x3D;，提供了一些方便的函数和方法来创建和操作图形、轴、图例和注释等元素。\nMatplotlib 是一个数据可视化库，提供了各种各样的绘图函数和工具，可以用于创建折线图、散点图、柱状图、饼图等各种类型的图表。而 pyplot 模块则是 Matplotlib 中最常用的绘图工具之一，它提供了很多方便的函数和方法，使得绘制各种类型的图表变得更加简单易用。因此，pyplot 可以被认为是 Matplotlib 库中最主要的一个模块。\n同方差性(Homoskedasticity)和异方差性(Heteroskedasticity)👦 写一个通俗的解释：比如说 $income&#x3D;b*education+e$ 这个计量模型,描述了教育水平与收入的关系，e为残差，OLS等传统计量框架要求残差e不与任何变量相关,即: $E(e|x)&#x3D;0$。\n一种常见的残差e与x相关的方式是: 残差e的方差与变量x相关，这时候，残差e的方差会随着x变动而变动，因此方差是异质性的. 这被称为异方差问题。\n异方差问题会导致什么?\n拿上边那个教育-工资的例子来看，一般来说，受教育水平越高的人群收入变动越大，而教育水平较低的人群收入相差不会太大 (比如说，有最低工资法)，这时就出现异方差问题了, 因为e的方差会随着教育水平x的增大而增大。\n\n异方差存在的时候,大多数情况下,OLS估计出的方差会比实际的方差要小(当然, 小部分情况下估计值会比实际的大)，&#x3D;&#x3D;因此会过高地估计系数b的显著性&#x3D;&#x3D; (因为, 系数的t值&#x3D;系数&#x2F;标准差)，**&#x3D;&#x3D;而不会影响系数估计值的大小&#x3D;&#x3D;**(考虑异方差影响系数大小的问题是近几年的焦点和难点，略过不提)。\n👨 同方差性是我们对OLS回归残差的一个要求，即 &#x3D;&#x3D;$\\epsilon \\sim N(0, \\sigma^2)$&#x3D;&#x3D; 。简单的说就是残差必须是随机的，这个分布是我们用来描述随机分布的一种方法 – 均值为0，方差为sigma方。\n异方差则说明残差不满足这个正态分布。\n概括而言，同方差说明回归的残差项是随机的，异方差说明残差项不随机，你的回归设置或者变量的选取有问题，需要修正。\nget_dummies()#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\n🤖 这段代码使用了get_dummies()函数将数据框中的分类变量转换成了虚拟变量（dummy variable）。\n虚拟变量是指将一个分类变量拆分成多个二元变量的过程。例如，如果有一个名为color的分类变量，其中包括红色、蓝色和绿色三个类别，那么将color转换成虚拟变量的过程就是创建三个新的二元变量，分别代表红色、蓝色和绿色，如果样本属于红色类别，则对应的红色虚拟变量值为1，其余两个虚拟变量的值为0，以此类推。\nget_dummies()函数会自动识别出所有的分类变量，并将它们转换成虚拟变量。它还有一些其他的参数，例如drop_first可以指定是否要删除每个变量的第一个虚拟变量，以避免多重共线性问题。\nProcedure“理解问题 -&gt; 单变量分析 -&gt; 多变量分析 -&gt; 数据清洗 -&gt; 验证假设”\n1⃣️ Understand the problem. We’ll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\n观察 columns\n\n2⃣️ Univariable study. We’ll just focus on the dependent variable (‘SalePrice’) and try to know a little bit more about it.\n\n使用 describe\n使用直方图\n分析偏度和峰度\n使用散点图和箱图观察目标变量和其他变量之间的关系\n\n3⃣️ Multivariate study. We’ll try to understand how the dependent variable and independent variables relate.\n\n热图看所有变量之间的关系\n热图看跟目标变量最相关的变量之间的关系\nscatterplot 看目标变量及最相关的变量之间的关系\n\n4⃣️ Basic cleaning. We’ll clean the dataset and handle the missing data, outliers and categorical variables.\n\n缺失值\n\n看所有的变量中缺失值占比\n\n剔除缺失变量：对因变量不重要的可以直接剔除整个变量；有替代变量的可以直接剔除整个变量；不剔除整个变量的可以仅将包含缺失值的单个样本剔除\n\n检查无缺失值\n\n\n\n离群值\n\n[数据标准化][4]，设置一个阈值来判断离群值，输出过高&#x2F;过低的离群值\n使用散点图观察某个自变量和因变量的关系，有了上一步得到的离群值，我们可以在散点图中一眼看出离群值在哪\n保留遵循趋势的离群值，删除违背趋势的离群值\n\n\n\n5⃣️ Test assumptions. We’ll check if our data meets the assumptions required by most multivariate techniques.\n\n正态分布\n看histogram：偏度和峰度\n看常态机率图：数据分布应紧跟代表正态分布的对角线\n\n\n异方差性\n散点图可视化，一头紧凑一头分散，或中间紧凑两头分散，或中间分散两头紧凑\n\n\n虚拟化变量\n\nStep1: Understand our data#check the decorationdf_train.columns\n\n\n Look as each variable and try to understand their meaning and relvance to this problem. Although it’s time-comsuming, it gives us the flavour of our dataset.\n\n\nWe should focus 5 aspects. Create an Excel spreadsheet with following columns:\n\nVariable\nType -&gt; ‘numerical’ or ‘categorical’\nSegment -&gt; Define the segments of all variables then identify them. (eg. building&#x2F; space &#x2F;location)\nExpectation -&gt; the variable infulence on the dependent variable. (high&#x2F; medium&#x2F; low)\nConclusion -&gt; the importance of the variable \nComments -&gt; any general comments that occured to us\n\n\nExpectation will give us ‘sixth sense’. In order to fill it, we should read the description of all the variables one by one, ask ourselves:\n\nDo we care this variable when we are buying a house?\nHow important would this variable is?\nIs this information already decribed in any other variable?\n\n\nFocus the variables with ‘high’ expectation. Generate some scatter plots between those variables and the dependant variable(SalePrice), filling the conclusion column which is the correction of our expectations.\n\nYou may give up some of your expecting variables after vasualizing them.\n\n\n\nStep2: Univariable analysis: ‘SalePrice’\ndescribe()\nhistogram- normal distribution? (positive) skewness? peakedness?\n[skewness and kurtosis][1]\n\n#data#descriptive statistics summarydf_train[&#x27;SalePrice&#x27;].describe()#graph#histogramsns.distplot(df_train[&#x27;SalePrice&#x27;]);#data#skewness and kurtosisprint(&quot;Skewness: %f&quot; % df_train[&#x27;SalePrice&#x27;].skew())print(&quot;Kurtosis: %f&quot; % df_train[&#x27;SalePrice&#x27;].kurt())\n\nRelationship with numerical variables\nscatter plots - if there is a linear (exponential) reaction\n\n#graph#scatter plot grlivarea/salepricevar = &#x27;GrLivArea&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));#scatter plot totalbsmtsf/salepricevar = &#x27;TotalBsmtSF&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)data.plot.scatter(x=var, y=&#x27;SalePrice&#x27;, ylim=(0,800000));\n\nRelationship with categorical fratures\nbox plots\n\n#box plot overallqual/salepricevar = &#x27;OverallQual&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(8, 6))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);var = &#x27;YearBuilt&#x27;data = pd.concat([df_train[&#x27;SalePrice&#x27;], df_train[var]], axis=1)f, ax = plt.subplots(figsize=(16, 8))fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)fig.axis(ymin=0, ymax=800000);plt.xticks(rotation=90);\n\n🤖️ 这段代码使用了Python中的Pandas和Seaborn库，用于绘制房价数据集中整体质量（OverallQual）与房价（SalePrice）之间的箱型图。具体解释如下：\n\nvar = &#39;OverallQual&#39;：将变量名OverallQual赋值给变量var，表示要绘制的x轴变量是整体质量。\ndata = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1)：将训练数据集df_train中的SalePrice和OverallQual列拼接在一起，并存储在名为data的变量中。这里使用了Pandas库中的concat函数。\nf, ax = plt.subplots(figsize=(8, 6))：创建一个8x6英寸大小的绘图对象，并将其存储在名为f的变量中，将其轴对象存储在名为ax的变量中。这里使用了Matplotlib库。\nfig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data)：使用Seaborn库中的boxplot函数绘制箱型图，x轴为OverallQual，y轴为SalePrice，数据来源为data。将绘制结果存储在名为fig的变量中。\nfig.axis(ymin=0, ymax=800000)：将y轴的范围设定为0到800000，使得箱型图的纵坐标范围更加合适。这里使用了Matplotlib库中的axis函数。\n\nStep3: Multivariable analysis\n[heatmap][2] style - from a universla perspective\n\n#correlation matrixcorrmat = df_train.corr()f, ax = plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True);\n\n🤖️ 这段代码用于绘制数据集中的相关性矩阵的热力图。以下是每一行的解释：\n\ncorrmat = df_train.corr(): 计算数据集 df_train 中所有数值型变量之间的相关性矩阵，其中 corr() 是 pandas 库中计算相关系数的函数。\nf, ax = plt.subplots(figsize=(12, 9)): 创建一个大小为 12x9 的图形画布，并将其保存在变量 f 和 ax 中，用于绘制相关性矩阵的热力图。这里用到了 matplotlib 库中的 subplots() 函数。\nsns.heatmap(corrmat, vmax=.8, square=True): 使用 seaborn 库中的 heatmap() 函数将相关性矩阵绘制成热力图，其中 corrmat 是相关性矩阵的数据，vmax 是热力图中颜色的最大值，square 参数用于指定热力图的形状是否为正方形。\n\n通过绘制相关性矩阵的热力图，可以更直观地了解数据集中各个变量之间的相关性，以便进行数据分析和建模。\n&#x3D;&#x3D;❗️Notice if some of the independent variables have a similar color distribution in the heat map! This proves that they play a similar role in the overall model, and perhaps it is possible to keep only the one variable with the least missing values in the cleaning.&#x3D;&#x3D;\n#saleprice correlation matrixk = 10 #number of variables for heatmapcols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].indexcm = np.corrcoef(df_train[cols].values.T)sns.set(font_scale=1.25)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#x27;.2f&#x27;, annot_kws=&#123;&#x27;size&#x27;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)plt.show()\n\n🤖️ 这段代码使用了Python中的Numpy、Pandas和Seaborn库，用于绘制房价数据集中各变量之间的相关系数热图。具体解释如下：\n\nk = 10：将热图中展示的相关系数最高的10个变量挑选出来。\ncols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index：从相关系数矩阵corrmat中选择SalePrice列与其他列的相关系数最高的10个变量，将其索引存储在cols变量中。\ncm = np.corrcoef(df_train[cols].values.T)：从训练数据集df_train中挑选出相关系数最高的10个变量，计算其相关系数矩阵，并将其存储在cm变量中。这里使用了Numpy库中的[corrcoef函数][3]。\nsns.set(font_scale=1.25)：设置Seaborn库中的字体大小。\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.2f&#39;, annot_kws=&#123;&#39;size&#39;: 10&#125;, yticklabels=cols.values, xticklabels=cols.values)：绘制热图。其中，参数cm是一个相关系数矩阵，参数cbar表示是否显示颜色条，参数annot表示是否显示每个格子中的数值，参数square表示是否将每个格子设置为正方形，参数fmt表示显示数值时保留两位小数，参数annot_kws表示注释的参数，这里设置字体大小为10，yticklabels和xticklabels表示行和列的标签名，这里都为cols.values。将绘制结果存储在名为hm的变量中。\nplt.show()：显示绘制出的热图。这里使用了Matplotlib库中的show函数。\n\nsns.set()cols = corrmat.nlargest(k, &#x27;SalePrice&#x27;)[&#x27;SalePrice&#x27;].index.tolist()sns.pairplot(df_train[cols], height = 2.5)plt.show()\n\n\n\nStep4: Missing dataMissing data#missing datatotal = df_train.isnull().sum().sort_values(ascending=False)percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)missing_data = pd.concat([total, percent], axis=1, keys=[&#x27;Total&#x27;, &#x27;Percent&#x27;])missing_data.head(20)\n\n🤖️\n\ndf_train.isnull()：判断数据集 df_train 中每个元素是否为缺失值，返回一个与 df_train 大小相同的布尔矩阵，内容是True&#x2F;False。\ndf_train.isnull().sum()：将布尔矩阵中每列（变量）中的缺失值数量相加，得到每列缺失值的总数。\ndf_train.isnull().count()：将布尔矩阵中每列（变量）中的元素（无论是否缺失）数量相加，得到每列元素的总数。\n\n#dealing with missing datadf_train = df_train.drop((missing_data[missing_data[&#x27;Total&#x27;] &gt; 1]).index, axis = 1)df_train = df_train.drop(df_train.loc[df_train[&#x27;Electrical&#x27;].isnull()].index)df_train.isnull().sum().max() #just checking that there&#x27;s no missing data missing...\n\nOutliers#standardizing datafrom sklearn.preprocessing import StandardScalersaleprice_scaled = StandardScaler().fit_transform(np.array(df_train[&#x27;SalePrice&#x27;])[:, np.newaxis]);low_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][:10]high_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][-10:]print(&#x27;outer range (low) of the distrubution:&#x27;)print(low_range)print(&#x27;\\n outer range (high) of the distribution:&#x27;)print(high_range)\n\n\n\nStep5: Getting hard coreNormality: histogram + normal probability plot#histogram and normal probability plotfrom scipy.stats import normsns.distplot(df_train[&#x27;SalePrice&#x27;], kde = True,fit = norm);fig = plt.figure()res = stats.probplot(df_train[&#x27;SalePrice&#x27;], plot = plt)\n\n\n 🤖️ histplot不支持直接显示拟合的正态分布曲线。如果要同时显示正态分布曲线，可以使用displot。\n\n&#x3D;&#x3D;In case of positive skewness, log transformations usually works well.&#x3D;&#x3D;\ndef dist_probplot(x):    sns.distplot(df_train[x], fit = norm)    fig = plt.figure()    res = stats.probplot(df_train[x], plot = plt)\n\n\n#applying log transformationdf_train[&#x27;SalePrice&#x27;] = np.log(df_train[&#x27;SalePrice&#x27;])dist_probplot(&#x27;SalePrice&#x27;)\n\n\ndist_probplot(&#x27;TotalBsmtSF&#x27;)\n\n\n\nOk, now we are dealing with the big boss. What do we have here?\n\nSomething that, in general, presents skewness.\nA significant number of observations with value zero (houses without basement).\nA big problem because the value zero doesn’t allow us to do log transformations.\n\n\n处理方法：将非零值做对数转换，零值还是零，但是若执行：\n![image-20230228093921098](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230228093921098.png)\n会出现以上报错。\n🤖️ 出现RuntimeWarning: divide by zero encountered in log的原因是在对数变换时遇到了 0，因为 log(0) 是没有定义的，所以会产生警告。可以将值为 0 的观测在变换前用一个极小的数代替，比如可以用 1e-6 代替 0。\n👧🏻 但是这段代码不是说只有不等于0时才会进行对数变换吗？为什么还会遇到log(0)的情况呢？\n🤖️ 这是由于浮点数的精度问题导致的。可能有一些值在处理之后非常接近0，但实际上并不等于0，因此在进行np.log()函数计算时，会返回一个非常大的负无穷值，比如-log(1e-15)&#x3D;-34.538776394910684。这也就是报错中提到的“divide by zero encountered in log”错误，其实并非是真正意义上的除零操作，而是一个数学上的约定。\n🤖️ 为了解决这个问题，一种常见的方法是在对数变换前，先对数据中的所有非零值加上一个极小的常数，比如1e-8，避免出现log(0)的情况，同时由于这个常数值非常小，对整体数据的统计特征影响也非常小。因此可以将代码修改为：\nepsilon = 1e-8  # 定义一个极小值df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] + epsilon)\n\n👧🏻 所以其实应该是这样：\n#transform datadf_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;] = np.log(df_train.loc[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0, &#x27;TotalBsmtSF&#x27;])\n\n#histogram and normal probability plotsns.distplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], fit = norm)fig = plt.figure()res = stats.probplot(df_train[df_train[&#x27;TotalBsmtSF&#x27;] &gt; 0][&#x27;TotalBsmtSF&#x27;], plot = plt)\n\n👧🏻 plot &#x3D; plt是什么意思？\n🤖 plot=plt是将&#x3D;&#x3D;Matplotlib库的当前pyplot对象&#x3D;&#x3D;传递给probplot函数中的参数plot。这样做是为了让probplot函数能够在当前的Matplotlib图形对象中绘制概率图。通常，我们在创建一个新的Matplotlib图形对象时会使用plt.figure()函数，并为其指定一个新的名称或编号。但是，如果我们已经有了一个现有的图形对象，我们可以将其传递给plot参数，使得probplot函数在该对象中绘制图形。\n👧🏻 我应该怎么获取Matplotlib库的当前pyplot对象？\n🤖 你可以使用matplotlib.pyplot.gcf()函数来获取当前图形对象。gcf()代表“get current figure”，它将返回最近创建的Figure对象（如果没有现有Figure，则会创建一个新的）。\nHomoscedasticity: graphically\nDepartures from an equal dispersion are shown by such shapes as &#x3D;&#x3D;cones (small dispersion at one side of the graph, large dispersion at the opposite side)&#x3D;&#x3D; or &#x3D;&#x3D;diamonds (a large number of points at the center of the distribution)&#x3D;&#x3D;.\n\n#scatter plotplt.scatter(df_train[&#x27;GrLivArea&#x27;], df_train[&#x27;SalePrice&#x27;]);\n\n#scatter plotplt.scatter(df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;TotalBsmtSF&#x27;], df_train[df_train[&#x27;TotalBsmtSF&#x27;]&gt;0][&#x27;SalePrice&#x27;]);\n\n\n\nDummy variables#convert categorical variable into dummydf_train = pd.get_dummies(df_train)\n\n","categories":["刷题笔记"],"tags":["Python","kaggle","data exploration"]},{"title":"Kaggle|房价预测","url":"/2023/02/28/HousePricing2/","content":"NotesProfileReport [1]import ydata_profiling as ppfimport pandas as pddf_train = pd.read_csv(&#x27;train.csv&#x27;)ppf.ProfileReport(df_train)\n\n👧🏻 ydata_profiling 和 pandas_profiling?\n👧🏻 ProfileReport() 运行卡顿？\n🤖 这可能与不同浏览器对于 Jupyter Notebook 的支持和性能优化有关。Jupyter Notebook 是一个基于 Web 的应用程序，它的性能和稳定性可能会受到不同浏览器的影响。\nSafari 浏览器在运行 Jupyter Notebook 时可能会更加流畅，这可能是因为 Safari 浏览器具有更好的 Web 技术支持和性能优化。同时，Safari 浏览器在 Mac OS X 系统中是默认的浏览器，可能与操作系统之间的兼容性更好。\n另一方面，Chrome 浏览器在运行 Jupyter Notebook 时可能会出现卡顿问题，这可能是因为 Chrome 浏览器具有更高的系统资源占用率，以及更多的插件和扩展功能，导致其在运行 Jupyter Notebook 时更容易出现性能问题。\n如果你在使用 Chrome 浏览器时遇到了性能问题，你可以尝试优化浏览器的设置或减少浏览器插件和扩展的使用，或者使用其他浏览器来运行 Jupyter Notebook，例如 Safari、Firefox 等等。同时，你也可以考虑在本地使用 Jupyter Notebook，而不是在浏览器中运行 Jupyter Notebook，这可以提供更好的性能和稳定性。\nEncoding\n数据预处理阶段，object类的特征会根据类别使用不同的编码方式处理（label encoding和One-hot encoding）\n\nLabel encodingOne-hot encodingReference[1] kaggle案例-Python实现房价预测-完整分析流程\n[2] [Kaggle竞赛丨房价预测（House Prices）][https://zhuanlan.zhihu.com/p/137076292]\n[3] \n","categories":["刷题笔记"],"tags":["Python","kaggle","data prediction"]},{"title":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?","url":"/2023/02/08/How-is-Hadley-Wickham-able-to-contribute-so-much-to-R-particularly-in-the-form-of-packages/","content":"How is Hadley Wickham able to contribute so much to R, particularly in the form of packages?\nDavid Robinson:\nFrom following Hadley’s work, it seems to me that along with being an exceptional programmer and data scientist, and having the advantage of developing R packages as part of his job, Hadley follows a few strategies that serve as useful wisdom for all developers:\n\nHe writes packages that make himself more productive. Three of Hadley’s popular packages, devtools, Roxygen2, and testthat, make it very easy to (respectively) develop, document and test R packages. He recognized that the time spent to create and maintain those was small compared to the time it would save him (and others!) in developing future packages. This extends beyond those package development tools: packages like stringr and lubridate are designed to make working with strings and dates easier. This also extends beyond his own packages: he takes advantage of packages like Rcpp (http://www.rcpp.org/) that make writing R C++ extensions fast and intuitive.\nHe takes full advantage of social coding. He’s a prolific GitHub user (hadley (Hadley Wickham)), which makes it efficient to receive and respond to bug reports and feature requests, and to collaborate with others (for instance, with Romain Francois on dplyr).\nHe works to simplify his packages rather than complicate them. In his announcement of the tidyr package (Introducing tidyr) he notes that “Just as reshape2 did less than reshape, tidyr does less than reshape2.” When packages are simpler (doing a few things well instead of hundreds of things poorly), they’re easier to develop and maintain.\n\n**Hadley Wickham: **\nI like David’s answer, but here are a few more thoughts from a personal perspective ;)\n\nWriting. I have worked really hard to build a solid writing habit - &#x3D;&#x3D;I try and write for 60-90 minutes every morning. It’s the first thing I do after I get out of bed.&#x3D;&#x3D; I think writing is really helpful to me for a few reasons. First, &#x3D;&#x3D;I often use my writing as a reference&#x3D;&#x3D; - I don’t program in C++ every day, so I’m constantly referring to @Rcpp every time I do. Writing also makes me aware of gaps in my knowledge and my tools, and filling in those gaps tends to make me more efficient at tackling new problems.\nReading. I read a lot. I follow about 300 blogs, and keep a pretty close eye on the R tags on Twitter and Stack Overflow. I don’t read most things deeply - &#x3D;&#x3D;the majority of content I only briefly skim. But this wide exposure helps me keep up with changes in technology, interesting new programming languages, and what others are doing with data.&#x3D;&#x3D; It’s also helpful that if when you’re tackling a new problem you can recognise the basic name - then googling for it will suggest possible solutions. If you don’t know the name of a problem, it’s very hard to research it.\nChunking. Context-switching is expensive, so if I worked on many packages at the same time, I’d never get anything done. Instead, at any point in time, most of my packages are lying fallow, steadily accumulating issues and ideas for new feature. Once a critical mass has accumulated, I’ll spend a couple of days on the package.\n\nFinally, it’s hard to over-emphasise the impact that working full-time on R makes. Since I’ve left Rice, I now spend well over 90% of my work time thinking about and programming in R. This has a compounding effect because as I built better tools (cognitive and computational) it becomes even easier to build new tools. I can create a new package in seconds, and I have many techniques on-hand (in-brain) for solving new problems.\n","categories":["Quora"]},{"title":"SQL-刷题笔记","url":"/2023/02/07/SQL%20%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","content":"1. 查询列列查询、多列查询、限制返回数量、in、like、聚合函数。\n# 查询所有列SELECT * FROM user_profile# 查询多列SELECT device_id, gender, age, university FROM user_profile# 查询结果去重SELECT DISTINCT university FROM user_profile# 查询结果限制返回行数SELECT device_id FROM user_profile LIMIT 2# 查询后重命名SELECT device_id FROM user_profile AS user_infos_example LIMIT 2# 否定查询select device_id, gender, age, universityfrom user_profilewhere not university = &#x27;复旦大学&#x27;# 过滤空值select device_id, gender, age, universityfrom user_profilewhere not age is null# inselect device_id, gender, age, university, gpa from user_profilewhere university in (&#x27;北京大学&#x27;, &#x27;复旦大学&#x27;, &#x27;山东大学&#x27;)# likeselect device_id, age, universityfrom user_profilewhere university like &#x27;%北京%&#x27;# max() min()select max(gpa)from user_profilewhere university = &#x27;复旦大学&#x27;# avg() count()select count(gender) as male_num, avg(gpa) as avg_gpafrom user_profilewhere gender = &#x27;male&#x27;# havingselect university, avg(question_cnt) avg_question_cnt, avg(answer_cnt) avg_answer_cntfrom user_profilegroup by universityhaving avg_question_cnt &lt; 5 or avg_answer_cnt &lt; 20# 生成新字段后不能用where要用having\n\n\n\n2. 涉及到多个表的情况select university, (count(q.question_id)/count(distinct(q.device_id))) avg_answer_cntfrom user_profile ujoin question_practice_detail qon u.device_id = q.device_idgroup by universityorder by university asc\n\nselect u.university, q.difficult_level, count(qp.question_id)/count(distinct qp.device_id) avg_answer_cntfrom user_profile u, question_practice_detail qp, question_detail qwhere u.university = &#x27;山东大学&#x27; and u.device_id = qp.device_id and q.question_id = qp.question_idgroup by u.university, difficult_levelorder by avg_answer_cnt\n\n\n\nSQL25 查找山东大学或者性别为男生的信息题目：现在运营想要分别查看学校为山东大学或者性别为男性的用户的device_id、gender、age和gpa数据，请取出相应结果，结果不去重。\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果（注意输出的顺序，先输出学校为山东大学再输出性别为男生的信息）：\n\n\n\ndevice_id\ngender\nage\ngpa\n\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n2138\nmale\n21\n3.4\n\n\n3214\nmale\nNone\n4\n\n\n5432\nmale\n25\n3.8\n\n\n2131\nmale\n28\n3.3\n\n\n4321\nmale\n28\n3.6\n\n\nselect device_id, gender, age, gpafrom user_profilewhere university = &#x27;山东大学&#x27;union all select device_id, gender, age, gpafrom user_profilewhere gender = &#x27;male&#x27;\n\nunion可以，但是结果是去重的，所以要用union all。\nwhere gender = ‘male’ or university = ‘山东大学’也不行，结果也是去重的。\n3. ifSQL26 计算25岁以上和以下的用户数量题目：现在运营想要将用户划分为25岁以下和25岁及以上两个年龄段，分别查看这两个年龄段用户数量\n本题注意：age为null 也记为 25岁以下\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\nage_cut\nnumber\n\n\n\n25岁以下\n4\n\n\n25岁及以上\n3\n\n\nselect age_cut, count(device_id) numberfrom(select if(age &gt;= 25, &#x27;25岁及以上&#x27;, &#x27;25岁以下&#x27;) as age_cut, device_id from user_profile) t1group by age_cut\n\n\n\n4. caseSQL27 查看不同年龄段的用户明细题目：现在运营想要将用户划分为20岁以下，20-24岁，25岁及以上三个年龄段，分别查看不同年龄段用户的明细情况，请取出相应数据。（注：若年龄为空请返回其他。）\n示例：user_profile\n\n\n\nid\ndevice_id\ngender\nage\nuniversity\ngpa\nactive_days_within_30\nquestion_cnt\nanswer_cnt\n\n\n\n1\n2138\nmale\n21\n北京大学\n3.4\n7\n2\n12\n\n\n2\n3214\nmale\n\n复旦大学\n4\n15\n5\n25\n\n\n3\n6543\nfemale\n20\n北京大学\n3.2\n12\n3\n30\n\n\n4\n2315\nfemale\n23\n浙江大学\n3.6\n5\n1\n2\n\n\n5\n5432\nmale\n25\n山东大学\n3.8\n20\n15\n70\n\n\n6\n2131\nmale\n28\n山东大学\n3.3\n15\n7\n13\n\n\n7\n4321\nmale\n26\n复旦大学\n3.6\n9\n6\n52\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ndevice_id\ngender\nage_cut\n\n\n\n2138\nmale\n20-24岁\n\n\n3214\nmale\n其他\n\n\n6543\nfemale\n20-24岁\n\n\n2315\nfemale\n20-24岁\n\n\n5432\nmale\n25岁及以上\n\n\n2131\nmale\n25岁及以上\n\n\n4321\nmale\n25岁及以上\n\n\nselect device_id, gender, case    when age &lt; 20 then &#x27;20岁以下&#x27;    when age &lt; 25 then &#x27;20-24岁&#x27;    when age &gt;= 25 then &#x27;25岁及以上&#x27;    else &#x27;其他&#x27;end age_cutfrom user_profile\n\n\n\n5. day()\nday()\n\nmonth()\n\nyear()\n\n\n6. 用户留存率计算SQL-计算用户次日留存率.md\n7. 字符串相关SQL-字符串相关.md\n","categories":["刷题笔记"],"tags":["SQL"]},{"title":"SQL 复习1","url":"/2023/03/06/SQL-%E5%A4%8D%E4%B9%A01/","content":"对于 join，如果不写 left, inner, right ，默认是什么联接呢？是inner join。\nselect * from a join b on a.id = b.idselect * from a inner join b on a.id = b.idselect * from a, b where a.id = b.id# 上面的三个是相等的.# 剩下的join类型有:left joinright joincross joinfull join# 2005新加cross applyouter apply\n\n 各种连接方式的区别？\ninner_join：  内连接，根据两个表共有的列来匹配其中的行，强调只有两个表共有的列值对应的行才能匹配出来。\nleft join&#x2F;right join&#x2F;all join： （左，右，全）外连接，以left join 为例，如果指定了需要匹配的列名，无论右表对应行是否包含满足连接条件的数据，左表的数据都会提取出来，则结果会将右表的这些值以空值的形式匹配进来。\ncross join：  交叉连接，结果是笛卡尔积，就是第一个表符合查询条件的行数乘以第二个表符合查询条件的行数。\nSQL窗口函数窗口函数原则上只能写在select语句中，对where或group by子句处理后的结果进行操作，可以分为以下两种函数：\n1） 专用窗口函数，包括后面要讲到的rank, dense_rank, row_number等专用窗口函数。\n2） 聚合函数，如sum. avg, count, max, min等\n窗口函数的基本语法如下：\n&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt;                order by &lt;用于排序的列名&gt;)\n\n窗口函数使用\n转自：https://zhuanlan.zhihu.com/p/92654574\n\nrankselect *, rank() over(\tpartition by class\torder by grade desc) as rankingfrom classtable\n\n1）每个班级内：按班级分组\npartition by用来对表分组。在这个例子中，所以我们指定了按“班级”分组（partition by 班级）2）按成绩排名\norder by子句的功能是对分组后的结果进行排序，默认是按照升序（asc）排列。在本例中（order by 成绩 desc）是按成绩这一列排序，加了desc关键词表示降序排列。\n窗口函数具备了我们之前学过的group by子句分组的功能和order by子句排序的功能。那么，为什么还要用窗口函数呢？\n这是因为，group by分组汇总后改变了表的行数，一行只有一个类别。而partiition by和rank函数不会减少原表中的行数。例如下面统计每个班级的人数。\n\n现在我们说回来，为什么叫“窗口”函数呢？这是因为partition by分组后的结果称为“窗口”，这里的窗口不是我们家里的门窗，而是表示“范围”的意思。\n简单来说，窗口函数有以下功能：\n1）同时具有分组和排序的功能\n2）不减少原表的行数\n其他窗口函数专用窗口函数rank, dense_rank, row_number有什么区别呢？\nselect *,   rank() over (order by 成绩 desc) as ranking,   dense_rank() over (order by 成绩 desc) as dese_rank,   row_number() over (order by 成绩 desc) as row_numfrom 班级表\n\n\n\nrank函数：这个例子中是5位，5位，5位，8位，也就是如果有并列名次的行，会占用下一名次的位置。比如正常排名是1，2，3，4，但是现在前3名是并列的名次，结果是：1，1，1，4。\ndense_rank函数：这个例子中是5位，5位，5位，6位，也就是如果有并列名次的行，不占用下一名次的位置。比如正常排名是1，2，3，4，但是现在前3名是并列的名次，结果是：1，1，1，2。\nrow_number函数：这个例子中是5位，6位，7位，8位，也就是不考虑并列名次的情况。比如前3名是并列的名次，排名是正常的1，2，3，4。\n在上述的这三个专用窗口函数中，函数后面的括号不需要任何参数，保持()空着就可以。\n聚合函数作为窗口函数聚和窗口函数和上面提到的专用窗口函数用法完全相同，只需要把聚合函数写在窗口函数的位置即可，但是函数后面括号里面不能为空，需要指定聚合的列名。\n我们来看一下窗口函数是聚合函数时，会出来什么结果：\nselect *,   sum(成绩) over (order by 学号) as current_sum,   avg(成绩) over (order by 学号) as current_avg,   count(成绩) over (order by 学号) as current_count,   max(成绩) over (order by 学号) as current_max,   min(成绩) over (order by 学号) as current_minfrom 班级表\n\n\n\n有发现什么吗？我单独用sum举个例子：\n如上图，聚合函数sum在窗口函数中，是对自身记录、及位于自身记录以上的数据进行求和的结果。比如0004号，在使用sum窗口函数后的结果，是对0001，0002，0003，0004号的成绩求和，若是0005号，则结果是0001号~0005号成绩的求和，以此类推。\n不仅是sum求和，平均、计数、最大最小值，也是同理，都是针对自身记录、以及自身记录之上的所有数据进行计算，现在再结合刚才得到的结果（下图），是不是理解起来容易多了？\n如果想要知道所有人成绩的总和、平均等聚合结果，看最后一行即可。\n这样使用窗口函数有什么用呢？\n聚合函数作为窗口函数，可以在每一行的数据里直观的看到，截止到本行数据，统计数据是多少（最大值、最小值等）。同时可以看出每一行数据，对整体统计数据的影响。\n注意事项partition子句可省略，省略就是不指定分组，结果如下，只是按成绩由高到低进行了排序：\nselect *,   rank() over (order by 成绩 desc) as rankingfrom 班级表\n\n得到结果：\n\n\n但是，这就失去了窗口函数的功能，所以一般不要这么使用。\n总结1.窗口函数语法\n&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt;                order by &lt;用于排序的列名&gt;)\n\n&lt;窗口函数&gt;的位置，可以放以下两种函数：\n1） 专用窗口函数，比如rank, dense_rank, row_number等\n2） 聚合函数，如sum. avg, count, max, min等\n2.窗口函数有以下功能：\n1）同时具有分组（partition by）和排序（order by）的功能\n2）不减少原表的行数，所以经常用来在每组内排名\n3.注意事项\n窗口函数原则上只能写在select子句中\n4.窗口函数使用场景\n1）业务需求“在每组内排名”，比如：\n\n排名问题：每个部门按业绩来排名topN问题：找出每个部门排名前N的员工进行奖励\n\non和where的区别\n参考：https://blog.csdn.net/tayngh/article/details/99684035\n\n前提：数据库在连接多张表返回记录时，都会生成一个中间临时表。\n在多表查询时，ON和where都表示筛选条件，on先执行，where后执行。区别：外连接时，on条件是在生成临时表时使用的条件，它不管on中的条件是否为真，都会返回左边表中的记录。而where条件是在临时表生成好后，再对临时表进行过滤的条件。\nSELECT * FROM emp e LEFT JOIN dept d ON e.deptno=d.`deptno` AND e.`deptno`=40;\n\n\nSELECT * FROM emp e LEFT JOIN dept d ON e.deptno=d.`deptno` WHERE e.`deptno`=40;\n\n\n来我们分析一下为什么会造成以上两种不同的结果。\n\non是生成临时表时使用的条件，上面我们采用的是左外连接，左外连接是以左表为基础的，左表的记录将会全部表示出来，而右表只会显示符合搜索条件的记录。也就是说emp是左表，dept是右表，条件是emp的deptno与dept中的deptno相等且为40时才连接，但emp表中不存在deptno为40的记录，也就是右表没有符合条件的记录，而记录不足的地方均用NULL来补充。\n而where是在临时表生成好后，再对临时表进行过滤。也就是说emp表与dept的连接条件只是emp的deptno与dept中的deptno相等，然后在对生成的临时表进行筛选，由于emp表中不存在deptno为40的记录，所以未找到符合条件的记录。\n\n由于内连接是从结果表中删除与其他被连接表中没有匹配行的所有行，所以在内连接时on和where的结果是相同的。而左外、右外与全连接由于它的特殊性，on和where造成的差别大小取决于表达式和表中的数据。\n\n参考：https://www.runoob.com/w3cnote/sql-different-on-and-where.html\n\n数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。\n在使用 left join 时，on 和 where 条件的区别如下：\n1、on 条件是在生成临时表时使用的条件，它不管 on 中的条件是否为真，都会返回左边表中的记录。\n2、where 条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有 left join 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。\n假设有两张表：\n表1：tab1\nid size1  102  203  30\n\n表2：tab2\nsize name10   AAA20   BBB20   CCC\n\n两条SQL:\n1、select * from tab1 left join tab2 on tab1.size = tab2.size where tab2.name=&#x27;AAA&#x27;2、select * from tab1 left join tab2 on tab1.size = tab2.size and tab2.name=&#x27;AAA&#x27;\n\n第一条SQL的过程：\n1、中间表\non 条件:\ntab1.size = tab2.sizetab1.id tab1.size tab2.size tab2.name1 10 10 AAA2 20 20 BBB2 20 20 CCC3 30 (null) (null)\n\n2、再对中间表过滤\nwhere 条件：\ntab2.name&#x3D;’AAA’\ntab1.id tab1.size tab2.size tab2.name1 10 10 AAA\n\n第二条SQL的过程：\n1、中间表\non 条件:\ntab1.size = tab2.size and tab2.name=&#x27;AAA&#x27;(条件不为真也会返回左表中的记录) tab1.id tab1.size tab2.size tab2.name1 10 10 AAA2 20 (null) (null)3 30 (null) (null)\n\n其实以上结果的关键原因就是 left join,right join,full join 的特殊性。\n不管 on 上的条件是否为真都会返回 left 或 right 表中的记录，full 则具有 left 和 right 的特性的并集。\n而 inner jion 没这个特殊性，则条件放在 on 中和 where 中，返回的结果集是相同的。\n总结：前提：数据库在连接多张表返回记录时，都会生成一个中间临时表。\n在内连接中，使用on或者where没有区别。\n在外连接里，例如使用left join时：\non是在生成临时表时使用的条件，不管on的条件是否为真，都会返回左边表中的全部记录。\nwhere条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有left join的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。\nSQL行列转换\n参考：https://www.jianshu.com/p/1c6fb0df9f58\n\n行转列假如我们有下表：\n\nSELECT *FROM studentPIVOT (    SUM(score) FOR subject IN (语文, 数学, 英语))\n\n通过上面 SQL 语句即可得到下面的结果\n\nPIVOT 后跟一个聚合函数来拿到结果，FOR 后面跟的科目是我们要转换的列，这样的话科目中的语文、数学、英语就就被转换为列。IN 后面跟的就是具体的科目值。\npivot将原来表中 课程字段中的 数据行 语文，数学，英语 转换为列，并用sum取对应列的值。\n当然我们也可以用 CASE WHEN 得到同样的结果，就是写起来麻烦一点。\nSELECT name,  MAX(  CASE    WHEN subject=&#x27;语文&#x27;    THEN score    ELSE 0  END) AS &quot;语文&quot;,  MAX(  CASE    WHEN subject=&#x27;数学&#x27;    THEN score    ELSE 0  END) AS &quot;数学&quot;,  MAX(  CASE    WHEN subject=&#x27;英语&#x27;    THEN score    ELSE 0  END) AS &quot;英语&quot;FROM studentGROUP BY name\n\n使用 CASE WHEN 可以得到和 PIVOT 同样的结果，没有 PIVOT 简单直观。\n列转行假设我们有下表 student1\n\nSELECT *FROM student1UNPIVOT (    score FOR subject IN (&quot;语文&quot;,&quot;数学&quot;,&quot;英语&quot;))\n\n通过 UNPIVOT 即可得到如下结果：\n\n我们也可以使用下面方法得到同样结果\nSELECT    NAME,    &#x27;语文&#x27; AS subject ,    MAX(&quot;语文&quot;) AS scoreFROM student1 GROUP BY NAMEUNIONSELECT    NAME,    &#x27;数学&#x27; AS subject ,    MAX(&quot;数学&quot;) AS scoreFROM student1 GROUP BY NAMEUNIONSELECT    NAME,    &#x27;英语&#x27; AS subject ,    MAX(&quot;英语&quot;) AS scoreFROM student1 GROUP BY NAME\n\n索引的作用索引是为了提高数据库查询数据的速度而增加的标志符号（通过创建唯一性索引，可以保证表中每一行数据的唯一性）。索引主要建立在①经常搜索的列；②主键所在列；③外键所在列\n索引包括聚集索引与非聚集索引，它们的区别在于索引记录的顺序与表记录的顺序是否一致。\n聚集索引:  可以理解为索引记录的顺序与表记录的顺序一致，SQL默认在依次递增的主键上建立聚集索引，例如，id为1的数据在第一条，id为2的数据在第二条。聚集索引会按照主键的顺序来排序。（例如，用字典找字，对于认识的字可以通过拼音排序对应正文找到页码）\n非聚集索引:  可以理解数据存储在一个地方，索引指向数据存储的位置，索引的顺序与表中数据记录的顺序不一定一致。例如说，建立数据表登记学生考试成绩，字段包括姓名，学号与分数。假定该表按照成绩排序、学号信息错乱，可以考虑构建非聚集索引，第一名对应1，第二名对应2……，想要提取第10个学生的学号，查找索引10指向的数据即可。（用字典找字，不认识的字可以采用部首结合笔画等信息在检字表中搜索，找到页码。比如查”张”字，检字表中”张”的页码是60页，检字表中”张”的上面是”驰”字，但页码却是100页，”张”的下面是”弩”字，页面是200页。很显然，在正文里面这些字并不是真正的分别位于”张”字的上下方，而检字表中连续的”驰、张、弩”三字实际上就是他们在非聚集索引中的排序）\n（关于聚集索引和非聚集索引的区别可以百度学习下，我记得好几家面试都直接问了……）\n排名函数与排序函数排序函数：order by （默认asc升序，指定desc降序），例如将表格数据按照考试成绩从低到高排序。\n排名函数：rank, dense rank, row number ，得到的成绩的排序后，根据成绩的高低对学生排名，100分对应第一名，99分第二名。它们的区别在于：\nrank函数：如果有并列名次的行，会占用下一名次的位置。1，1，1，4。\ndense_rank函数：如果有并列名次的行，不占用下一名次的位置。1，1，1，2。\nrow_number函数：不考虑并列名次的情况。1，2，3，4。\n连接多个select？**union:**连接select，不允许重复值，而且select的对象要有相同的列以及数据类型；(例如A表中某字段的数据是1,1,2,3,4，B表中对应的数据是（0,0,0,0,0），则提取的数据是（1,0；2,0；3,0；4,0），也就是说（1,0）这个组合只会出现一次。)\n**union all:**作用同union,但是允许重复值（也就是说，与上面一样的操作里，(1,0)这个组合会出现2次）\n一般来说如果select 字段大于1个，用union all比用union速度快，因为union 会将多个结果中重复的数据合并，union all则是直接合并\nIntersect: 和 union指令类似，intersect也是合并两个select语句结果的函数。不同的地方是， union的处理结果类似于全集 (如果这个值存在于第一个select或者第二个select，它就会被选出)，而intersect则比较像取出交集 ( 值要同时存在于第一个select和第二个select)。\n**minus:**先找出第一个 select 语句所产生的结果，然后看这些结果有没有在第二个 select语句的结果中。最后会输出第一个select中没有，但是第二个select中有的数据\n主键和外建？主键是一张表中能够确定一条记录的唯一标志(数据库中的一条记录中有若干个属性，若其中某一个属性组(注意是组)能唯一标识一条记录，该属性组就可以成为一个主键 )，比如身份证号。\n外键用于和另一张表进行关联。例如，A字段是A表的主键，那么出现在B表中的A字段能够作为B表的外键，实现A,B表的连接查询。\n向表中插入数据insert into tablename values:  普通插入数据模式\ninsert or ignore into: 如果没有则插入数据，如果有则忽略\ninsert or replace into: 如果不存在就插入，存在就更新\n删除表中数据delete  : 删除表中数据，可以指定具体数据（where）\ndrop column&#x2F; drop table :  删除列数据，与delete 不同，drop函数会将数据以及表的结构全部删除。\n**truncate:**仅删除数据，且默认删除所有数据。和delete不同，truncate不能用where进行筛选，但删除速度比delete快\n字符串常见操作函数concat():  将多个字符串连接成一个字符串，连接符用“”包起来\n**concat_ws()**； 将多个字符串连接成一个字符串，在最开始的位置指定连接符（指定一次即可）\ngroup concat():  将group by产生的同一个分组中的值连接起来，返回一个字符串。\nlike():   需要与通配符一起使用（’%’代表任意字符出现任意次数；’_’仅能匹配单个字符）\nsubstr():   用于从字段中提取相应位置的字符。\nregexp() :  正则表达式匹配函数\nin&#x2F;exist 的关系和区别子查询过程中，In和exist函数效率比较：\n当进行连接的两个表大小相似，效率差不多；\n如果子查询的内表更大，则exist的效率更高（exist先查询外表，然后根据外表中的每一个记录，分别执行exist语句判断子查询的内表是否满足条件，满足条件就返回ture）。\n如果子查询的内表小，则in的效率高（in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积 (表中的每一行数据都能够任意组合A表有a行，B表有b行，最后会输出a*b行)，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快）。\nExist的原理: 使用exist时，若子查询能够找到匹配的记录，则返回true，外表能够提取查询数据；使用 not exist 时，若子查询找不到匹配记录，则返回true，外表能够提取查询数据。\n","tags":["SQL"]},{"title":"SQL-字符串相关","url":"/2023/02/07/SQL-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E5%85%B3/","content":"SQL30 统计每种性别的人数描述题目：现在运营举办了一场比赛，收到了一些参赛申请，表数据记录形式如下所示，现在运营想要统计每个性别的用户分别有多少参赛者，请取出相应结果\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;url&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;urlsydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ngender\nnumber\n\n\n\nmale\n2\n\n\nfemale\n3\n\n\n\nselect substring_index(profile, &#x27;,&#x27;, -1) as gender, count(device_id)from user_submitgroup by gender\n\n\n\nSQL31 提取博客URL中的用户名描述题目：对于申请参与比赛的用户，blog_url字段中url字符后的字符串为用户个人博客的用户名，现在运营想要把用户的个人博客用户字段提取出单独记录为一个新的字段，请取出所需数据。\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bisdgboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;dkittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;ur&#x2F;tigaer\n\n\n4321\n171 cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\ndevice_id\nuser_name\n\n\n\n2138\nbisdgboy777\n\n\n3214\ndkittycc\n\n\n6543\ntigaer\n\n\n4321\nuhsksd\n\n\n2131\nsydney\n\n\n\n提取某个字符一般有四种做法：\ntrim()\ntrim() 是直接更改相同格式的一列，删除这一列内容中的统一部分，然后重命名：\nselect device_id, trim(&#x27;http:/url/&#x27; from blog_url) as user_namefrom user_submit\n\n​\t\n\nsubstring_index()\nsubstring_index() 是将字符串切割，1表示保留字符串的左边👈，-1表示保留字符串的右边👉：\nselect device_id, substring_index(blog_url, &#x27;/url/&#x27;, -1) as user_namefrom user_submit\n\n还有一个用法是计数：\nSUBSTRING_INDEX(str,delim,count) 返回从字符串str分隔符 delim 在计数发生前的子字符串。如果计数是正的，则返回一切到最终定界符(从左边算起)的左侧。如果count是负数，则返回一切最终定界符(从右边算起)的右侧。SUBSTRING_INDEX() 搜寻在delim时进行区分大小写的匹配。\nSELECT SUBSTRING_INDEX(&#x27;www.somewebsite.com&#x27;,&#x27;.&#x27;,2);\n\nOutput: &#39;www.somewebsite&#39;\n\nsubstr()\nsubstr() 是用具体位置（数字）来表示从哪开始截取的，参数里还包括截取的长度：\nselect device_id, substr(blog_url, 11, length(blog_url)-10) as user_namefrom user_submit\n\n\n\nreplace()\nreplace() 就是替换函数：\nsleect device_id, replace(blog_url, &#x27;http:/url/&#x27;, &#x27;&#x27;) as user_namefrom user_submit\n\nSQL32 截取出年龄描述题目：现在运营举办了一场比赛，收到了一些参赛申请，表数据记录形式如下所示，现在运营想要统计每个年龄的用户分别有多少参赛者，请取出相应结果\n示例：user_submit\n\n\n\ndevice_id\nprofile\nblog_url\n\n\n\n2138\n180cm,75kg,27,male\nhttp:&#x2F;ur&#x2F;bigboy777\n\n\n3214\n165cm,45kg,26,female\nhttp:&#x2F;url&#x2F;kittycc\n\n\n6543\n178cm,65kg,25,male\nhttp:&#x2F;url&#x2F;tiger\n\n\n4321\n171cm,55kg,23,female\nhttp:&#x2F;url&#x2F;uhksd\n\n\n2131\n168cm,45kg,22,female\nhttp:&#x2F;url&#x2F;sydney\n\n\n根据示例，你的查询应返回以下结果：\n\n\n\nage\nnumber\n\n\n\n27\n1\n\n\n26\n1\n\n\n25\n1\n\n\n23\n1\n\n\n22\n1\n\n\nselect substring_index(substring_index(profile, &#x27;,&#x27;, -2), &#x27;,&#x27;, 1) as age, count(device_id)from user_submitgroup by age\n\n","categories":["刷题笔记"],"tags":["SQL","字符串"]},{"title":"SQL-计算用户留存率","url":"/2023/02/07/SQL-%E8%AE%A1%E7%AE%97%E7%94%A8%E6%88%B7%E6%AC%A1%E6%97%A5%E7%95%99%E5%AD%98%E7%8E%87/","content":"计算用户次日留存率如果只是计算用户的次日留存率，那么使用date_sub()函数就够了：\nselect avg(if(b.device_id is not null, 1, 0)) as avg_retfrom(    select distinct device_id, date    from question_practice_detail) aleft join(    select distinct device_id, date_sub(date, interval 1 day) as date    from question_practice_detail) bon a.device_id = b.device_id and a.date = b.date\n\n\ndate_sub() 函数：\n\nSELECT DATE_SUB(‘2010-08-12’, INTERVAL 3 DAY) AS NewDate \n结果： 2010-08-09\n\nSELECT DATE_SUB(‘2010-08-12’, INTERVAL ‘3-2’ YEAR_MONTH) AS NewDate \n结果： 2007-06-12\n\nSELECT DATE_SUB(‘2011-09-14 2:44:36’, INTERVAL ‘2:26’ HOUR_MINUTE) AS NewDate \n结果： 2011-09-14 00:18:36\n\n\n\n\n基于SQL的留存率计算\n转自知乎：基于SQL的留存率计算\n\n一、什么是留存率互联网行业里，留存率是用于反映网站、互联网应用或网络游戏的运营情况的统计指标，其具体含义为在统计周期（周&#x2F;月）内，每日活跃用户数在第N日仍启动该App的用户数占比的平均值。其中N通常取2、3、7、14、30，分别对应次日留存率、三日留存率、周留存率、半月留存率和月留存率。\n留存率常用于反映用户粘性，当N取值越大、留存率越高时，用户粘性越高。\n二、留存率的计算\n留存率 &#x3D; 登陆用户数&#x2F;新增用户数 * 100%\n\n新增用户数：在当前时间段新注册（或新访问）的用户数；\n\n登录用户数：在统计的时间段至少登录过一次的用户数；\n\n次日留存率：在次日至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️3日留存率：在往后3天内至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️7日留存率：在往后7天内至少登录过一次的用户数&#x2F;当天新增的用户数；\n\n❗️15日留存数：当天新增的用户数，在往后7天内至少登录过一次的用户，在往后第8天到第14天内至少再登陆过一次的用户数\n➡️ 3日和7日，至少登陆过一次；15日，7天为一段，在每段内至少登录一次！\n\n\nSQL中计算用户的留存率\n新增用户数\n\n由于数据过大，这截取时间2017.11.26~2017.12.03为例。\n首先计算分母，这里有的算法是用新增用户数，有的算法是用活跃用户数。\n⚠️注意：新增用户数与活跃用户数并不相等，活跃用户数包含新增用户数。活跃用户数，当天的访问人数，也就是UV。\n-- 每位用户的最早登录日期SELECT 用户ID, MIN(日期) AS 最早登录日期FROM userbehaviorWHERE 日期 &gt; &#x27;2017-11-25&#x27;AND 日期 &lt; &#x27;2017-12-04&#x27;GROUP BY 用户ID\n\n\n再从上表中计算出每天的新增人数，❗️算新增人数用的日期是最早登录日期！\nSELECT 最早登录日期 AS 日期, COUNT(DISTINCT 用户ID) AS 新增人数FROM(SELECT 用户ID, MIN(日期) AS 最早登录日期     FROM userbehavior     WHERE 日期 &gt; &#x27;2017-11-25&#x27; AND 日期 &lt; &#x27;2017-12-04&#x27;     GROUP BY 用户ID) AS fGROUP BY 最早登录日期\n\n\n以下是活跃用户数的算法，二者确实数值上并不相等。❗️算每日活跃用户数用的日期就是当天的日期！\n-- 每天活跃用户数=UV 访客数SELECT 日期, COUNT(DISTINCT 用户ID) AS 活跃用户数FROM userbehaviorwhere 日期 &gt;&#x27;2017-11-25&#x27; AND 日期 &lt; &#x27;2017-12-04&#x27;GROUP BY 日期\n\n\n抽取7天的活跃用户数。\n\n次日留存的用户数\n\n\n次日的时间间隔为1，涉及到时间间隔采用自联结。\n\nSELECT a.用户ID, a.`日期` AS atime, b.`日期` AS btimeFROM userbehavior AS a LEFT JOIN userbehavior AS bON a.`用户ID` = b.`用户ID`WHERE a.`日期` &gt; &#x27;2017-11-25&#x27; AND a.`日期` &lt; &#x27;2017-12-04&#x27;\n\n\n将上表存为视图C，\nCREATE VIEW C(用户ID, atime, btime)ASSELECT a.用户ID, a.`日期`, b.`日期` from userbehavior AS a LEFT JOIN userbehavior AS bON a.`用户ID`=b.`用户ID`WHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;;\n\n\n计算时间间隔用timestampdiff函数\n\nSELECT `用户ID`, TIMESTAMPDIFF(DAY, atime, btime)AS 时间间隔FROM (  SELECT a.用户ID, a.日期 as atime, b.日期 as btime  FROM userbehavior as a LEFT JOIN userbehavior as b  ON a.用户ID=b.`用户ID`  WHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;) AS c-- 这里直接设置为表c\n\n\n得到了用户的时间间隔\n3.用case语句筛选出时间间隔为1的数据，并且进行计数\nSELECT *, COUNT(DISTINCT case when 时间间隔 = 1 then `用户ID`\t\t\tELSE NULL\t\t\tEND)AS 次日留存数FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n次日留存率\n\n次日留存率&#x3D;次日留存用户数&#x2F;当日活跃用户数\nSELECT *,COUNT(DISTINCT case when 时间间隔 = 1 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 次日留存率FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN userbehavior as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n\n三、三日以及N日留存的计算只需要修改时间间隔&#x3D;N即可。\nSELECT 日期,COUNT(DISTINCT 用户ID AS 活跃用户数,COUNT(DISTINCT case when 时间间隔=1 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 次日留存率COUNT(DISTINCT case when 时间间隔=3 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 三日留存率COUNT(DISTINCT case when 时间间隔=7 then `用户ID`\t\t\tELSE NULL\t\t\tEND) AS 次日留存数/COUNT(DISTINCT 用户ID) AS 七日留存率FROM( SELECT `用户ID`,TIMESTAMPDIFF(DAY,atime,btime)AS 时间间隔\t\t\t FROM (\t\t\t\tSELECT a.用户ID,a.日期 as atime,b.日期 as btime\t\t\t\tFROM userbehavior as a LEFT JOIN 每日新增用户数表 as b\t\t\t\tON a.用户ID=b.`用户ID`\t\t\t\tWHERE a.`日期`&gt;&#x27;2017-11-25&#x27;AND a.`日期` &lt; &#x27;2017-12-04&#x27;\t\t\t\t) AS c\t\t\t)AS d\n\n\n\n四、连续登录三天用户数量","categories":["刷题笔记"],"tags":["SQL"]},{"title":"数据分析｜AB实验","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9CAB%E5%AE%9E%E9%AA%8C/","content":"\n面试官:“你有没有想过用AB实验来优化项目？”\n​    瑟瑟发抖的我：“不好意思，我在做项目的时候想过用AB实验，但由于XXXX的原因无法落实。不过我自己构思了比较完整的实验思路……（此处省略若干字），如果有机会让我实施AB实验，我相信能够让项目表现更好。”\n​    其实这是一个很加分的回答。在面试官看来，眼前的候选人虽然没有参与AB实验，但自己琢磨思考了项目优化方案，应该有不错的自我学习和自我驱动能力 （给自己鼓掌撒花ing）。\n作者：阿狸和小兔链接：https://www.jianshu.com/p/0b54dc8b0880来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n什么是A&#x2F;B测试简单来讲，A&#x2F;B测试是一种比较手段：通过分析同一总体下，由于某些不同的策略导致样本数据表现出的差异，来推断某些策略的效果。\n\n几个核心概念：同一总体，不同策略，不同样本，差异及效果\n本质上来说，A&#x2F;B测试就是假设检验理论的一个实际应用而已，所以想了解A&#x2F;B测试的理论，只需要了解假设检验的理论即可。\nA&#x2F;B测试流程一个完整的A&#x2F;B test主要包括如下几部分：\n1、分析现状，建立假设：分析业务，确定最高优先级的改进点，作出假设，提出优化建议。\n2、设定指标：设置主要指标来衡量版本的优劣；设置辅助指标来评估其他影响。\n3、设计与开发：设计优化版本的原型并完成开发。\n4、确定测试时长：确定测试进行的时长。\n5、确定分流方案：确定每个测试版本的分流比例及其他分流细节。\n6、采集并分析数据：收集实验数据，进行有效性和效果判断。\n7、给出结论：①确定发布新版本；②调整分流比例继续测试；③优化迭代方案重新开发，回到步骤1。\n注意点：1. 测试时长：测试的时长不宜过短，否则参与试验的用户几乎都是产品的高频用户。\n2. 分流（或者说抽样）：应该保证同时性、同质性、唯一性、均匀性。\n①同时性：分流应该是同时的，测试的进行也应该是同时的。\n②同质性：也可以说是相似性，是要求分出的用户群，在各维度的特征都相似。可以基于用户的设备特征（例如手机机型、操作系统版本号、手机语言等）和用户的其他标签（例如性别、年龄、新老用户、会员等级等）进行分群，每一个A&#x2F;B测试试验都可以选定特定的用户群进行试验。\n思考：如何判断是不是真的同质？可以采用AAB测试。抽出两份流量进行A版本的测试，进行AA测试，并分别与B版本进行AB测试。通过考察A1和A2组是否存在显著性差异，就可以确定试验的分流是否同质了。\n③唯一性：即要求用户不被重复计入测试。\n④均匀性：要求各组流量是均匀的。Hash算法。现在一般由专用的A&#x2F;B测试工具负责。也有看到一篇文章写了python实现，大体的思路是对用户id添加Salt值，对其散列，并据此算出一个0-1之间的浮点数，同设定好的阈值比大小，从而分组。有兴趣的可以看看，该作者的思路很清晰： 随机分配里的Why and How。（统计学原理上，我没有找到均匀性这一要求的依据，其实双样本的假设检验并不要求两个样本的数量相等或相近。当然从直观上是可以理解，希望分出的用户组越相近越好，包括人数的相近。）\n3. A&#x2F;B测试只能有两个版本么？\nA&#x2F;B test不是只能A方案和B方案，实际上一个测试可以包含A&#x2F;B&#x2F;C&#x2F;D&#x2F;E&#x2F;……多个版本，但是要保证单变量，比如按钮的颜色赤&#x2F;橙&#x2F;黄&#x2F;绿&#x2F;青&#x2F;蓝&#x2F;紫，那么这七个方案是可以做A&#x2F;B测试的；但如果某方案在旁边新增了另一个按钮，即便实验结果产生了显著差异，我们也无法判断这种差异的成因究竟是谁。\n4. 同一段时间内可以做不同的A&#x2F;B测试么？\n比如一个test抽取总体20%的流量做按钮颜色的实验，另一个test也抽取总体20%的流量做布局样式的实验。是否可行？\n我认为是可行的。但要求多个方案并行测试，同层互斥。如果从总体里，先后两次随机抽取20%流量，则很有可能会有重叠的用户，既无法满足控制单变量，又影响了用户的使用体验。\n\n同层指的是在同一流量层中创建实验，在此层中创建的实验共享此层中的100%流量。\n互斥指的是在此层中，一个设备有且只能分配到此层多个实验中的某一个实验。\n\nA&#x2F;B测试原理：假设检验\n参考：https://zhuanlan.zhihu.com/p/68019926\n\n\n\n参考：https://www.jianshu.com/p/0b54dc8b0880\n\nAB实验是数据分析、产品运营、算法开发在工作中都时常接触到的工作。按钮颜色、广告算法、标签排序，这些互联网产品里常见的功能与展示都是在一次次AB实验中得到优化。\n   在实习中，并不是每一个实习生都能接触到AB实验，这也让很多没有相关经历的人误认为AB实验是一项高大上的工作任务。但其实——\n   许多公司搭建了可(sha)视(gua)化的AB实验平台，业务、运营以及产品都一眼看出AB实验的结果……不过，由于企业搭建的AB实验平台在权限、监控上有诸多限制，加上很多Leader出于实习生实习期短、留用概率小的顾虑，大部分实习生都不会参与完整的AB项目。\n​    可是——\n​    尽管在实习中没有接触过AB实验，在简历中也没提到丝毫，这不代表在面试中就能逃过一劫……\n\n\n​    我在实习中就没有接触过AB实验，但面试官仍然对我抛出来的项目分析过程饶有兴趣，也会问我有没有使用AB实验作出优化。得益于面试前全方位的准备，我的面试回（tao）答（lu）往往是:\n​    面试官:“你有没有想过用AB实验来优化项目？”\n​    瑟瑟发抖的我：“不好意思，我在做项目的时候想过用AB实验，但由于XXXX的原因无法落实。不过我自己构思了比较完整的实验思路……（此处省略若干字），如果有机会让我实施AB实验，我相信能够让项目表现更好。”\n​    其实这是一个很加分的回答。在面试官看来，眼前的候选人虽然没有参与AB实验，但自己琢磨思考了项目优化方案，应该有不错的自我学习和自我驱动能力 （给自己鼓掌撒花ing）。\n   引申一步，当你对自己的项目足够熟悉时，哪怕没有机会开展AB实验，仍然可以和我一样，在回答项目相关问题时，代入自己对AB实验的思考。\n   好了，言归正传，进入今天的正题。\n01 我们先介绍统计检验\n​    在统计学中，想要证明一个命题是正确的，只能通过证明其否命题是错误的来达到目的。假设检验是用统计数据来判断命题真伪的方式。我们常常会假设两个命题：\n​    H0：备受质疑的命题\n​    H1：有待验证的问题\n   那如何来证明H0和H1孰对孰错呢？这时候我们需要用到P值。\n   P值是什么？P值就是在H0假设成立的情况下，得到样本观察结果或更极端的观察结果出现的概率。\n   这句话好绕口，我们可以简单的理解成P代表了对H0命题的支持程度。所以P值越小，H0命题正确的概率就越小，H1命题正确的概率越大。我们有常常会指定显著性水平α&#x3D;0.05，当P&lt;α时，H0命题成立的概率&lt;0.05，这是一个受到统计学支持的假命题。\n   在学习统计学时，我们接触了一大堆显著性水平，显著区间的概念；在考完统计学后，统统还给了大学老师……如果是面试数据分析相关的岗位，强烈建议复习一下，再总结成方便自己记忆的文字，存入面试文档（插播一句，后台好多人私信我面试文档里准备了哪些内容……按当前进度，一只伪装加班狗表示写到这一块预计应该要4月了吧emmmm）。\n   若是有面试官问我：“置信区间和置信度你了解吗？”\n   根据准备在面试文档中的内容，我能够脱口而出：“在假设检验的过程中，我们往往采用样本数据特征来估计整体的数据特征。在中心极限定理里，我们知道从总体中进行N次样本抽取，N次样本的均值会围绕总体均值上下波动。因此，置信区间就是为总体的均值提供了一个可波动的范围，置信区间与置信度是相对应的。例如，在95%的置信度下，置信区间为【a,b】，也就是说，抽取100次样本，其中有95次样本的均值能够落在【a,b】范围内。”\n   可惜我做了充分的准备，并没有面试官问我这个问题（哭）\n   也有人问，面试中面试官会以什么形式来问假设检验的知识点呢？举一个栗子 ：\n   面试VIVO的时候面试官随口提问：“你会怎么证明中医的有效性？”\n   我当时的回答是：\n   “我会用假设检验来做证明。\n   第一步，提出两个命题。H0命题是中医无效；H1命题是中医有效。在这里，H0命题是我希望被推翻的命题，而H1命题是我希望被证实的命题。\n   第二步，随机选择两组生理特征、疾病状况一致的人。一组人不给予治疗；另一组人给予中药治疗，持续观测两拨人生理状况。\n   第三步，对两组人的生理数据进行独立样本t检验，观测统计结果P值。\n   第四步：设定显著性水平α&#x3D;0.05，如果统计结果P≤0.05，则推翻了H0假设，证明在该显著性水平下，中医是有效的。”\n  所以我们简化一下，回答假设检验相关题目的时候，遵循“提出命题-选择实验对象-检验-输出结论”的流程即可。只是有一点，建议多使用“第一第二第三”、“首先其次最后”这些次序词，用以展示相对流畅的思考逻辑。\n   不知道我讲清楚了没有……如果没有，建议结合《商务经济统计》再继续理解几遍，自我感觉以上内容应该可以应付绝大多数面试过程中的假设检验问题。\n   好了……不管了，我要强行进入第二个话题了……\n02 工作中的AB实验是如何开展的？\n   再举一个简单的栗子。过去在对首页产品进行排序时，往往是运营人员结合自己工作经验进行人工排序，现在算法小哥更新了产品排序的逻辑，希望通过AB实验证明自己的算法优于运营人工推荐，提高转化效果。\n   算法小哥和开发沟通好上线AB实验，用转化率(CR)来评估不同排序方式的效果，并设定显著性水平α&#x3D;0.05。在这场AB实验中，用到的两个假设分别是：\n   H0：运营的人工排序效果好（备受质疑、希望被推翻的命题）\n   H1：算法的推荐算法效果好（有待验证，希望被证实的命题）\n   在AB平台观察今天的实验效果，数据结果显示P&#x3D;0.003，可以理解成数据统计结果对H0的支持力度只有0.003，小于显著性水平0.05，这时候H1命题成立，相比运营的手工排序，算法工程师的推荐算法取得了更好的效果。\n   那在这个实验里，有什么要注意的点吗？\n  ——嗯，AB实验有很多需要注意的问题。\n （1）AB组是否真的只有一个变量\n   这场AB实验的变量是产品排序的逻辑，但大家都想开展AB实验，在首页上同期开展的AB实验有几十个，UI想测试筛选框的颜色、产品想测试标签的文案……。这种背景下，我们AB实验里被划分为A群体和B群体的用户往往同时参与了N多实验，不能绝对保证变量的唯一性。一般而言认为流量足够大，其AB实验和我们观测的AB实验没有直接交集，可以忽略其影响。\n（2）新策略是否真的上线了\n   因为研发每天都有很多AB实验，而且AB实验平台出错也是常有的事情。所以我们要在别人告诉我们AB实验上线了以后，自己验证实验策略是否真的上线了。\n（3）在实验前确定评估指标\n   我们的实验评估指标是CR转化率。这一点在实验上线前就要沟通好。\n（4）多观察几天数据\n   很多AB实验上线后前几天数据表现是不稳定的，最好持续观察半个月再给出结论。\n（5）存档AB测试的结果\n   对于数据分析师来说，每一个项目、每一个分析都需要做复盘和存档。比如AB实验项目，可以用一个标准化的模板来记录测试内容，为什么测试，测试对接人，测试效果等等，在年终的时候可以更好的汇报和复盘。\n   在大半年的工作中，终于有机会接触到诸多的AB实验，慢慢感知到各种AB实验原来殊途同归。但不可置否，它仍然是互联网产品迭代的利器，仍然是分析师证明自身价值的手段之一。想来这也是为什么诸多的面试官喜欢在面试中询问AB实验、假设检验的原因。\n作者：阿狸和小兔链接：https://www.jianshu.com/p/0b54dc8b0880来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n","tags":["数据分析"]},{"title":"六级翻译","url":"/2023/03/04/%E5%85%AD%E7%BA%A7%E7%BF%BB%E8%AF%91/","content":"2023&#x2F;03华北平原是中国第二大平原，位于黄河下游。华北平原西邻太行山，东至海岸，北起燕山，南接淮河，面积31万平方公里。这里地势低平，多在海拔50米以下。它由黄河、淮河和海河的冲积物形成，也被称为黄河淮海平原。平原地势平坦，河湖众多，交通便利，经济发达，自古即为中国政治、经济、文化中心。\nThe North China Plain is the second largest plain in China, located at the lower reaches of the Yellow River. Bordering on the Taihang Mountains in the west, the coast in the east, Yanshan Mountain in the north and Huaihe River in the south, it covers 310,000 square km. It has a smooth terrain with an elevation of lower than 50 meters. Formed of alluvial deposits from the Yellow, Huaihe and Haihe rivers, it is also known as the Yellow River-Huaihe-Haihe Plain. With its flat terrain, many rivers and lakes, convenient transportation and developed economy, the plain has been the political, economic and cultural center of China since ancient times.\n\n华北平原（The North China Plain）是中国最大的冲积（alluvial）平原。华北平原是世界上人口最密集的区域之一。它的面积约为40.95万平方公里，延绵覆盖着河南、河北和山东省的大部分地区。它是中国重要的农作物产区之一。首都北京坐落在华北平原的东北边缘位置。此外，天津，这所重要的工业城市和商业港口，也临近其东北海岸。传统意义上，华北平原的南部被称之为“中原”，“中原”是中华文明的摇篮。\nThe North China Plain, the largest alluvial plain in China, is one of the regions with the densest population in the world. It covers an area of about 409.5 thousand square kilometers, and it extends over much of Henan, Hebei and Shandong provinces. It is one of China’s most important agricultural regions. The plain is one of the most densely populated regions in the world. Beijing, the national capital, is located on the northeast edge of the plain. In addition, Tianjin, as an important industrial city and commercial port, is near its northeast coast. Its southern part is traditionally referred to as the Central Plain, which formed the cradle of Chinese civilization.\n\n\nCurrently, the whole society is gaining a better understanding of the issue of population. It is agreed that the growth of population is beneficial to a coordinated and sustained development between population and economy, society, resources and environment. To solve the population problem, emphasis should be laid on the improvement of the population quality, health level and quality of life so as to realize the all-round development of all humanity. The issue of population is essentially a problem of development and could only be solved through economic, social and cultural development.\nÒ【翻译热词】\n四六级考场阅卷老师全喜欢的高分短语!\n1.issue of population人口问题\n2.gain a better understanding of更好地理解\n3.the growth of population人口增长\n4.a coordinated and sustained development协调和可持续发展\n5.population quality 人口素质\n6.all-round development全面发展\n7.health level健康水平\n8.cultural development文化发展\n【高频词汇】\n\ncoordinate[kau’5:dinert] v.协同动作;协调(7次)\nsustain[sa’stein] v.维持，保持（14次)\nessentially[I’senJoli] adv.本质上;根本上(11次)\n\n\n翻译题目方言\n方言(dialect)是指只在局部地区使用的语言。形成方言的原因很多，如人口的迁移、山川地理的阻隔、不同语言的相互接触和影响等。方言之间的差异表现在语音(phonetics)、词汇、语法各个方面，语音方面尤为突出。据说中国十大最难懂方言中，温州话排名第一，广东话紧随其后。普通话作为人与人之间交流沟通的工具，普及固然重要，而方言作为地方文化的一种，是民族文化不可缺少的组成部分，也应被保护和传承。\n参考译文\nA dialect is a language spoken only in a local area. There are many reasons for the formation of dialects, such as the migration of population, the barrier of mountains and rivers and the mutual contact and influence of different languages. The differences between dialects are manifested in phonetics, vocabulary and grammar, especially phonetic aspects. It is said that Wenzhou dialect is the most difficult one to understand in China, followed by Cantonese. Mandarin, as a tool of interpersonal communication, is of course important to be popularized, while dialects, as a kind of local culture, are an indispensable part of national culture and should also be protected and inherited.\n\n2023年3月四六级翻译预测| 二十四节气：\n24节气(24 solarterms)是统称，包括 12节气（12 majorsolar terms)和 12中气 (12 minorsolar terms),它们彼此之间相互关联。24节气反映了天气变化，指导农业耕作，也影响着人们的生活。春秋战国时期，人们开始使用节气作为补充历法 (calendar)。公元前104年，24节气最终确立。众所周知，中国是个有着悠久农业发展史的国家。农业生产受自然规律影响极大。在古代，农民根据太阳的运动安排农业生产活动。24节气考虑到了太阳的位置，这就是我们重视它的原因。\n参考译文：\nThe 24 solar terms are the whole name of the system that consists of 12 major solar terms and 12 minor solar terms linked with each other. It reflects climate change, guides agriculture arrangements, and also affects people’s life. In the Spring and Autumn Period and the Warring States Period, people began to use solar terms as the supplementary calendar. It was in 104 B, C. that the 24 solar terms were finally set down. As we all know, China is a country with a long history of agriculture. Agricultural production is largely influenced by the laws of nature. In ancient times, farmers arranged their agricultural activities according to the move of the sun. It is the fact that the 24 solar terms take into account the position of the sun that makes us attach importance to it.\n\n四六级翻译|中国朝代\n中国有4000多年的历史，是世界最古老的文明之一。从公元前21世纪的夏朝开始至清朝结束，中国历史上经历过几十个朝代的变更。每个朝代在政治、经济、文化、科技领域等都有独特的成就。汉朝是当时世界上最先进的帝国。”汉族“（the Han Nationality）这一名称就得名于汉朝。唐朝因统一时间长、国力强盛而被国人铭记，因此在海外的中国人自称为“唐人”（Tang people）。宋朝和明朝是经济、文化、教育与科学高度繁荣的时代。但朝代的更替一般会导致连年战争，给人民大众带来了难以言表的痛苦。\n参考译文：\nWith a history of more than 4,000 years,China is one of the oldest ancient civilizations of the world. From Xia Dynasty in the 21st century BC to Qing Dynasty, China experienced dozens of dynasties in history. Each dynasty achieved unique accomplishments in the fields of politics, economy, culture, science and technology, etc. Han Dynasty was the most advanced empire at that time, which contributes to the formation of the name “the Han Nationality . Tang Dynasty impressed Chinese for its long time unification and powerful national strength, because of which overseas Chinese call themselves “Tang people” abroad. Song Dynasty and Ming Dynasty were periods when economy, culture, education and science were highly prosperous. But the change from one dynasty to another usually led to long-lasting wars, which brought unspeakable suffering for the masses.\n\n2023年3月四六级翻译预测| 清明节\n清明节是中国重要的传统节日，通常在阳历4月5日左右。它始于周朝，有2500多年的历史。清明节是中国人祭祀（offer sacrifices to）祖先的日子。在清明节祭祀逝去的先祖亲人实际上体现了生者对逝者的思念与敬爱，由于清明节与中国另一个传统节日“寒食节”（Cold Food Day）临近，所以吃寒食也成为清明节的习俗之一。除了扫墓，清明节还有荡秋千等风俗活动。如今，清明节已经成为中国重要的非物质文化遗产（intangible cultural heritage）。\n参考译文：\nThe Qing Ming Festival is an important traditional Chinese festival which usually falls around April 5 in the solar calendar. It started from Zhou Dynasty, with a history of over 2,500 years. The Qing Ming Festival is an occasion for Chinese people to offer sacrifices to ancestors. Offering sacrifices to the departed on the Qing Ming festival actually shows that the living miss, respect and love the dead. The Qing Ming Festival is close to another Chinese traditional festival “Cold Food Day”; therefore, eating cold food has turned into one of the customs of the day. Besides tomb-sweeping，there are also other customs and activities like swinging on that day. At present, the Qing Ming Festival has become an important intangible cultural heritage in China.\n2022&#x2F;09贴春 联 (Spring Festival couplets)是中国人欢度春节的一个重要习俗。春联由一对诗句 和四字横 批 (horizontal scroll)组成，诗句和横批用金色或黑色写在红纸上，红色代表幸运， 金色代表财富。春联贴在大门左右两侧和门框上方。春联的诗句体现中国传统诗词的特点， 两句诗的字数相同、内容相关。横批凸显春联的主题，更是锦上添花。春联以简洁的文字 描绘生动的形象，抒发美好的愿望。当家家户户贴春联时，人们就会意识到春节己经正式拉 开序幕。\nPasting the Spring Festival couplets is an important custom for Chinese people to celebrate the Spring Festival. The Spring Festival couplets consist of a pair of poems and a four-character horizontal scroll written in gold or black on red paper, with red representing luck and gold representing wealth. The Spring Festival couplets are pasted on the left and right sides of the gate and above the door frame. The lines of the Spring Festival couplets embody the characteristics of traditional Chinese poetry, with two lines having the same number of characters and related content. The horizontal scroll highlighting the theme of the Spring Festival couplets is even the icing on the cake. People depict vivid images in simple words in the Spring Festival couplets, expressing a beautiful vision. When every household puts up the Spring Festival couplets, people will realize that the Spring Festival has officially kicked off.\n2022&#x2F;09自古以来， 印章在中国就是身份的凭证和权力的象征。 印章不仅具有实用性， 而且也 是一种艺术形式， 是一门集书法与雕刻于一体的古老艺术， 经常被看作与书画并列的独立 艺术品。 印章从材料的选择、制作的工艺到字体的设计， 都具有极其丰富的美学表现。 其 他国家的艺术家通常在其绘画作品上签名， 而中国艺术家则往往在其书画作品上盖上印章 代替签名。 这样， 印章也就成为作品的组成部分， 是体现作品独特性的一种方式。\nFrom ancient times, the seal has been a certification of identity and a symbol of power. The seal is not only practical, bus also a kind of artistic form. It is a kind of time-honored art that mixes caligraph and sculpture, being regarded as an independant artwork that is comparable with painting and caligraphy. The seal has an extremely rich aesthetic performance from the choice of materials, the technique of producing and the design of fonts. Artists from other countries usually sign on their paintings, while Chinese artists always print their seals on their works instead of signing by hand. Therefore, the seal is going to become an intergral part of the works and a way to present the uniqueness of works. \n Since ancient times, the seal has been proof of identity and a symbol of power in China. The seal is not only practical, but also an art form and an ancient art that combines caligraphy and sculpture. It is often seen as an independent work of art alongside caligraphy and painting. The seal is extremely rich in aesthetic expression, from the selection of materials, the craftsmanship to the design of the typeface. Arstists in other countries usually sign their paintings, while Chinese artists often use seals instead of signitures on their aintings and caligraphy. In this way, the seal also becomes an integral part of the work and is a way to reflect its uniqueness. \n\nproof前面没有冠词；symbol前面有冠词\n\nancient 古代的；古老的\n\n书法是calligraphy不是caligraph，雕刻是sculpture\n\ntypeface &amp; font: \n\nA typeface is the underlying visual design that can exist in many different typesetting technologies, and a font is one of these implementations. In other words, a typeface is what you see and a font is what you use.\n\n\nselection &amp; choice: \n\nWhen used as nouns, choice means an option, whereas selection means the process or act of selecting.\n“Do I have a choice of what color to paint it?”\n“The large number of good candiadtes made selection difficult.”\n\n\ncertification &amp; certificate &amp; proof: \n\ncertification强调的是出具证明的动作 act&#x2F;process，为不可数名词；而certificate强调的是一份证明 document&#x2F;paper，为可数名词。\n\nthe medical certification of the cause of death\n为死因出具医学鉴定\n\ncertification of competence\n颁发技能证书\n\na birth certificate\n出生证明\n\na degree certificate\n学位证书\n\na certificate course 认证课程; the certification exam 证书考试\n\n\n\nA certificate is a official document stating that information is true for example a birth or marriage certificate.\n\nProof is evidence&#x2F;a piece of information that shows that something exists or is true.\n\n\n\n\n2022&#x2F;09中央电视台总部大楼位于北京市朝阳区，总建筑面积约55万平方米。主楼由两座塔楼组成，因其独特的造型，成为这座城市的一个热门景点，每天都吸引众多游客前来参观。大楼的创新结构是中外建筑师长期合作的成果，不仅体现了环保意识，而且大大节约了建筑材料。中央电视台总部设有一条穿过大楼的专用通道，向公众展示各个工作室以及中央电视台的历史。在那里，参观者还可以看到故宫和北京其他地方的壮观景色。\nThe CCTV headquarters building is located in Chaoyang district, Beijing and has an area of 550 thousand square metres. The major building consists of two towers that becomes a hot spot in Beijing because of its distinct character, attracting a largr number of tourists to visit everyday. The innovative structure of the building results from the long-term cooperation of  Chinese and foreign architects. It not only refelcts environmental awareness, but slao saves a lot of constructing materials. The CCTV headquarter has a specific access that is used to present history of all kinds of studios and CCTV to the public. There, the visitors can also witness the spectacular sceneries of the Imperial Palace and other places in Beijing. \nChina Central Television (CCTV) headquarters Building is located in Chaoyang District, Beijing, with a toal floor area of about 550,000 square meters. The main building consisting of two towers has become a popular attraction of the city because of its unique shape, attracting many tourists every day. The innovative structure of the building is attributed to the long-term collaboration between Chinese and foreign architects, which not only reflects environmental awareness, but also greatly saves building materials. CCTV headquarters set up a dedicated passage through the building to show the public the various studios anf the history of CCTV. From there, visitors can also enjoy the spectacular views of the Forbidden City and other places in Beijing.\n\ncooperation &amp; collaboration: \ncooperate 艰苦地干活、努力，很多人一起合作共同创造或实现一个目标，不分彼此的合作\ncollaborate（operate）操作、运作，长期的一种关系，单纯的合作&#x2F;配合对方\n\n\n\n2022&#x2F;06卢沟桥位于天安门广场西南 15 公里处，横跨永定河， 是北京现存最古老的多拱石桥。 卢 沟 桥 最 初 建 成 于 1192 年 ， 1698 年重建， 由 281 根柱子支撑。 每根柱子上都有一头石狮。 这些石狮的头、背、腹部或爪子上都藏着更多的狮子。 这些石狮生动逼真、千姿百态， 是 卢沟桥石刻艺术的精品。桥上的石狮不计其数，因而北京地区流传着“卢沟桥上的石狮子——数不清”的说法。\n卢沟桥不仅以其美学特征闻名于世，还被公认为石桥建筑史上的一座丰碑。\nLugou Bridge is 15 kilometers southeastern far from the Tiananmen Square, across the Yongding River and is the present oldest multi-arch stone bridge. Lugou bridge is originally built in 1192 and is reconstructed in 1682, supported by 281 pillars that each has a stone lion on them. More lions are hided on their head, back, belly and claw. These stone lions are very vivid and have a variety of gestures, are miracles of Lugou Bridge’s stone-sculpture art. The stone lions on the bridge are hard to count, hence there is a folk statement that “the lions on the Lugou Bridge - countless”.\nLugou Bridge is not only famous for its aesthetic characteristic, but also regarded as a milestone in the stone bridge history.\nLocated 15 km southwest of Tian’anmen Square, Lugou Bridge stretches over the Yongding River and is the oldest existing multi-arched stone bridge in Beijing. The original construction of the bridge was completed in 1192 and then in 1698 the bridge was reconstructed. The bridge is supported by 281 pillars, and on each pillar stands a stone lion. More lions hide themselves on the head and back, under the belly or on the paws of each lion. The lions are vivd with various postures and differrent expressions, and they are known as the fine work of the stone carving art. As there are numorous stone lions on the bridge, there is a saying in Beijing echoing, “The lions are too numerous to count.”\nLugou Bridge is not only well-known in the world for its aesthetic features, but also well recognized as a monument in the architectural history of stone bridge. \n2022&#x2F;06南京长江大桥是长江上首座由中国设计、采用国产材料建造的铁路、公路两用桥，上层 的 4 车道公路桥长 4589 米， 下层的双轨道铁路桥长 6772 米。铁路桥连接原来的天津一浦口 和上海一南京两条铁路线，使火车过江从过去一个半小时缩短为现在的 2 分钟。 大桥是南北 交通的重要枢纽， 也是南京的著名景点之一。\n南京长江大桥的建成标志着中国桥梁建设的一个飞跃， 大大方便了长江两岸的物资交流 和人员来往， 对促进经济发展和改善人民生活起到了巨大作用。\nNanjing Long River Bridge is the first double-used bridge by both rail road and highway that  is designed by Chinese and constructed with China-made materials. The upper-level four-car road is 4589 meters in length and the lower-level two-rails road is 6772 meters in length. The rail-road beidge connects two rail lines, the previous Tianjin-Pukou and Shanghai-Nanjing, which shortens the time that trains need to pass the river from half an hour to 2 minutes. The bridge is not only an important hinge of transportation between north and south, but also a famous spot in Nanjing.\nThe completion of the construction of Nanjing Long River Road symbolizes a huge leap of Chinese bridge constructing techniques, making the items’ exchange and people’s interaction far more convenient than before, making a huge effect on promoting the development of the economy and people’s quality of life.\n\n\n\n\n2022&#x2F;06赵州桥建于隋朝，公元 605 年左右，长 50.82 米，宽 9.6 米，跨度 37.37 米。天才建筑 师李春设计并监督了桥的建设。赵州桥结构新颖、造型优美。桥有一个大拱，在大拱的两 端有两个小拱，帮助排泄洪水、减轻桥梁重量并节省石材。建成以来，该桥经受了多次洪 水和地震，但其主体结构仍然完好无损，至今仍在使用。\n赵州桥是世界桥梁建筑史上的一次创举，是中国古代文明史上的一项杰出成就。类似 设计的桥梁直到14 世纪才在欧洲出现，比赵州桥晚了 700 多年。\nZhaozhou Bridge was built in Sui Dynasty, about BC 605. It is 50.82 meters long, 9.6 meters wide, crossing over 37.37 meters. The genius architect, Li Chun, designed and supervised the construction of the bridge. Zhaozhou Bridge has an innovative structure and beautiful shape. The bridge has a big arch, with two small arches in both sides of it that can help with discharging flood, reducing the weight og the bridge and saving stone materials. Since the bridge was built, it experienced flood and earthquake for several times, but its main structure is still in good condition and under used.\nZhaozhou Bridge is a great work in the world’s bridge construction history and an outstanding achievement in Chinese history of civilization. Bridges with similar design did not appear until 14 centuries in Europe and were later than Zhaozhou Bridge for mroe than 700 years.\nThe Zhaozhou Bridge, which was built in the Sui Dynasty around 605 AD, is 50.82 metres long and 9.6 metres wide with a span of 37.37 meters. Li Chun,a genius architect, designed and supervised its construction. The bridge boasts a novel structure and a graceful appearance, with a major arch in the middle and two minor ones on its ends which help discharge floods, reduce the weight of the bridge and save stones. Since the completion, the bridge has withstood floods and earthquakes, but remains intact in its main structure and stil available in use. The Zhaozhou Bridge is a pioneering undertaking in the world history of bridge construction and a masterpiece of the Chinese ancient civilization for the simple reason that its similar bridge did not appear in Europe until the 14th century,700 years later than the Zhaozhou Bridge.\n\n公元前 BC（Before Christ 基督前） 公元后 AD（Anno Domini 主后）\n\na pioneering undertaking \n\nremain in use&#x2F; be in use\n\nthe history of the CHinese ancient civilization 文明史\n\n\n2021&#x2F;122021.12英语六级翻译真题(一)\n延安位于陕西省北部，地处黄河中游，是中国革命的圣地。毛泽东等老一辈革命家曾在这里生活战斗了十三个春秋，领导了抗日战争和解放战争，培育了延安精神，为中国革命做出了巨大贡献。延安的革命旧址全国数量最 大、分布最 广、级别最 高。延安是全国爱国主义、革命传统和延安精神教育基地。延安有9个革命纪念馆，珍藏着中共中央和老一辈革命家在延安时期留存下来的大量重要物品，因此享有“中国革命博物馆城”的美誉。\nLocated in the northern part of Shanxi Province and in the middle reaches of the Yellow River，Yan’an is the sacred land of the Chinese Revolution. The old generation of revolutionaries such as Mao Zedong lived and fought here for thirteen years, leading the War of Resistance against Japanese Aggression and the War of Liberation, cultivating the spirit of Yan’an, and making great contribution to the Chinese revolution. With the largest number of revolutionary sites that are distributed widest and have the highest level in the country，Yan’an is a national base for patriotism, revolutionary tradition and spiritual education. There are nine revolutionary memorial halls, which hold a large number of important items left by the Central Committee of the Communist Party of China and the old generation of revolutionaries during the Yan’an period, so it enjoys the reputation of the museum city of Chinese revolution.\n2021.12英语六级翻译真题(二)\n井冈山地处湖南江西两省交界处，因其辉煌的革命历史被誉为“中国革命红色摇篮”。1927年10月，毛泽 东、朱德等老一辈革命家率领中国工农红军来到这里，开展了艰苦卓绝的斗争，创建了第 一个农村革命根据地，点燃了中国革命的星星之火，开辟了“农村包围(besiege)城市，武装夺取政权”这一具有中国特色的革命道路，中国革命从这里迈向胜利。井冈山现有100多处革命日址，成为一个“没有围墙的革命历史博物馆”，是爱国主义和革命传统教育的重要基地。\nJinggangshan is located at the boundary of Hunan Province and Jiangxi Province, which is honored as “the red cradle of Chinese revolution” for its glorious revolutionary history. In October 1927, the old generation like Mao Zedong and Zhu De led the Chinese Workers’ and Peasants’ Army here, where they created the first rural revolutionary base with much extremely hard and bitter struggle and light the sparks of Chinese revolution, breaking a revolutionary path with Chinese characteristic to besiege the city from the countryside and to seize power by armed force. It is here that the revolution moves toward success. There are over 100 revolutionary sites in Jinggangshan, a vital base for patriotism and education of revolutionary traditions, which has become a revolutionary history museum without any walls.\n2021.12英语六级翻译真题(三)\n中国共产党第 一次全国代表大会会址位于上海兴业路76号，是一栋典型的上海式住宅，建于1920年秋。1921年7月23日，中国共产党第 一次全国代表大会在此召开，大会通过了中国共产党的第 一个纲领和第 一个决议，选举产生了中央领导机构，宣告了中国共产党的诞生。1952年9月，中共一大会址修复，立纪念馆并对外开放。纪念馆除了介绍参加一大的代表之外，还介绍党的历史发展进程，现已成为了解党史、缅怀革命先烈的爱国主义教育基地。\nSituated at No.76 Xingye Road, Shanghai, the Site of the First National Congress of the Communist Party of China is a typical Shanghai-style residence built in the fall of 1920. On July 23, 1921, the First National Congress of the CPC was held here, where the first programme and resolution of the CPC were passed, and the collective central leadership was elected, thus announcing the birth of the Communist Party of China. In September 1952, the site was renovated, and a Memorial Hall was established and opened to the public. In addition to introducing the representatives who participated in the First Congress, the Memorial Hall also introduced the historical development of the party, which has become a patriotic education base to know party’s history and commemorate the memory of revolutionary martyrs.\n","categories":["六级"],"tags":["英语"]},{"title":"数据分析｜BI搭建","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9CBI%E6%90%AD%E5%BB%BA/","content":"BI搭建流程\n此阶段不可小觑，做好项目规划才能站在项目的高处掌控项目全局。在此阶段需要组建项目团队、确定项目目标以及项目里程碑。项目目标是项目的魂，后续工作都需要围绕目标展开，为工作指明方向，防止在项目执行过程中迷失。此阶段需要进行的工作内容是项目资料的交接与收集、组建项目团队以及组织项目启动会。\n1.方案阶段：方案设计\nBI项目都是由企业需求驱动的，项目方案也只有和企业的需求契合才能产生价值。所以在方案输出之前，要摸清楚需求、背景、客观条件、可投入资源等，最好要具体到业务、数据、技术等层面的需求，这关乎项目的落地和项目交付验收，同时也支撑BI项目工具选型。\n从启动阶段确认好项目何时开始后，便可以进入项目方案阶段，此阶段的目标为：调研相关需求内容及期望；收敛需求范围，统一交付目标；确认项目实施&amp;研发计划；确认项目解决方案。方案阶段主要是确认“做多少”以及“怎么做”，所以这一阶段主要的工作为：业务需求调研，项目需求说明书，原型设计、方案输出及确认。\n2.实施阶段：系统建设\n搭建一个合理的BI系统是BI项目成功的关键。在项目实施阶段，主要工作内容为监督项目成员按BI项目计划及解决方案内容，准时保质保量开发完成各项功能，并可以交付使用。BI项目实施可用三步曲进行概括。\n第一步：环境搭建。根据企业的实际使用人数、并发人数等指标，来确定正式环境服务器配置、带宽配置、是否需要集群部署以及数据库、中间件类型等，然后制定环境搭建方案。\n第二步：数仓建设。数仓建设是数据分析及应用的基础，我们可以采取从上至下的搭建方法，根据已经确定的BI分析主题构建合适的数仓模型，从而逐步整合企业的业务数据。此种搭建方式针对性强，目标明确，聚焦于所需的源数据整理，缩小数据整理的范围，有效地避免了资源浪费。\n第三步：BI开发。此阶段主要完成前端可视化的工作，可采取代码开发，也可以借助成熟的可视化工具进行开发。一般包含常规报表、可视化图表、分析报告、大屏展示、移动应用这几种展现形式。\n3.上线阶段：上线与验收\n到了上线阶段，可先在小范围试运行系统，从业务满足性方面检验BI系统试运行效果，重点是业务流程满足度和业务场景满足度。同时要做好用户操作培训和运维培训，保障BI系统后续独立使用。\n此阶段所要做的事归结起来就是一个词——“查漏补缺”，主要的工作内容有：核查并完成未完成事项、 用户操作培训、系统运维培训、交付文档整理、项目总结与验收。\n4.优化阶段：升级迭代\n一个好的BI项目要注重持续建设，不断完善与扩展。在实际使用中，通过用户的反馈不断的打磨和完善系统，让BI系统更加贴合实际使用场景。\n二、法 · 如何快速落地BI建设\n现在有很多企业不断尝试落地BI系统，那么如何高效地并有价值地建设BI系统呢？\n首先要选择一个合适的建设路径，有两种方法：一个是按KPI驱动，另外一个是按角色和场景搭建体系模型。\n1、从KPI管控出发，监控问题的角度，提炼KPI库\n每个企业的侧重KPI不一样，我们要结合各个主题，提炼适合客户的运营KPI。对它进行提炼、分析、预警，以及后续采取动作，并根据一些核心指标进行驱动和预警。通过KPI来驱动企业人员，调用企业所有人员一起共同建设BI平台，助力BI平台快速落地。\n2、以不同角色的视角、场景化搭建\n不同的角色关注的分析指标会不同，看指标的维度也会不同，比如高层领导对所负责的整体业务、运营情况要有宏观的掌控，需要满足其日常管理、经营分析、专项业务分析的需要。部门经理除了关注部门的核心绩效指标之外，还需要深入探究现象发生的原因，沿着数据的脉络去找寻问题，解决问题。所以我们需要结合不同角色视角，按照日常管理的场景合理化搭建BI平台，这样BI平台才能快速的落地。\n再者，我们还可以借鉴行业应用模板，快速导入复用。现在各行业更新换代特别快，产品周期短，可以借鉴一些成型的模板快速导入。\n除此之外，工具的灵活应用，也能很好加快平台建设，大大提高搭建效率，减少人力成本的投入。接下来小编来告诉大家如何挑选BI工具。\n三、器 · BI工具哪家强\n工欲善其事，必先利其器。BI工具是BI项目的核心，选对工具，BI项目就成功了一半。面对市场上鱼龙混杂的BI工具，不少企业眼花缭乱，无从下手。如果将需求和产品功能进行匹配和梳理，总结下来，无非四大方面：\n\nBI不断地迭代，亿信华辰就有这样一款功能全面而又可靠的工具。深耕大数据领域15年，连续多年荣登商务智能应用榜首，在成千上万个项目中不断打磨产品和服务，形成从数据填报、ETL数据处理、数据建模到数据分析、数据可视化、移动应用等一整套企业级商业智能产品方案——亿信ABI，实现多维度的数据分析应用，让数据发挥价值，驱动业务运营。\n\n\n你可以把它当作数据处理工具，通过拖拽式的流程设计，实现数据的抽取、清洗、转换、装载与调度，面向业务分析构建数据仓库，实现数据融合，提升数据分析效率。\n你可以把它视作报表工具，因为它能接入各种ERP、OA、CRM等系统数据，不写代码不写SQL就能批量化做报表。\n你也可以把它当作可视化工具，因为它自带上百种可视化图表以及动态效果，拖拽即可生成，制作领导驾驶舱、大屏可视化不在话下，更是结合全景3D建模和数据分析引擎实现酷炫3D效果。\n你还可以把它看作数据分析工具，因为从数据采集、数据处理、数据应用全流程完整覆盖，一个工具搞定数据生命周期的各个环节，实现数据填报、处理、分析一体化。\n\n结语：BI项目的建设是一个不断的实践、循环迭代的过程。多年项目经验总结了八个字：明道、优法、利器、践行。当然，选择合适且强大的“利器”，可让你在工作中变得更加的从容，亿信ABI提供一站式数据分析服务，助你达成所愿。\n","tags":["数据分析"]},{"title":"数据分析|DAU下降分析","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9CDAU%E4%B8%8B%E9%99%8D%E5%88%86%E6%9E%90/","content":"产品日活(DAU)下降，该如何分析\n案例：例如网易新闻APP日活突然下降5%，需要尽快排查一下数据下跌的原因。\n\n分析框架和逻辑思维![image-20230308082432891](&#x2F;Users&#x2F;xiaoyu&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20230308082432891.png)\n可以分为五个步骤。\n第一，结合以往经验及各种信息，对数据的异常作出假设。比如假期效应、热点事件、活动影响、政策影响和统计口径变化等。\n第二，确认数据的真实性。可以将时间轴拉长至3个月，做同比和环比，看是近期异常还是历史异常；还需要看与该指标关联的其他指标是否异常，并找关键人员比如产品和研发确认数据的真实性。\n第三，对数据从不同的维度进行拆分来验证假设。比如从新用户&#x2F;老用户、新版本&#x2F;老版本、登录渠道和入口等维度。\n第四，分析外部原因和内部原因。外部原因考虑政治、经济和社会因素，比如政策、竞品APP数据和社会事件影响；内部影响从产品、研发和运营入手，并注意统计口径是发生了变化，比如业务逻辑的改变和指标计算方式的变化。\n第五，需要将当前的结论文档化，并持续跟踪后期的数据是否再次异常。确认无误之后给相关人员发邮件输出影响范围和分析的结论等等。\n1. 对数据异常原因做出假设，利用数据验证影响DAU因素较多，对所有维度直接拆解耗时耗力。所以需要结合以往经验及各种信息，对数据异常的原因做出假设，然后对数据从不同维度拆分来验证假设。可能随着之前的假设的验证不断进行新的假设，直到定位原因。\n\n2. 确认数据真实性\n将时间轴拉长(3个月)，做同比和环比，看近期异常还是历史异常；\n查看与该指标关联的其他指标是否异常；\n找数据流相关产品和研发确实数据真实性。\n\n\n3. 常见拆分维度\n根据以上维度拆分之后，每项数据都需要和历史数据做对比，计算影响系数。\n影响系数&#x3D;(今日数据−昨日数据)&#x2F;(今日总量−昨日总量)\n影响系数越大，说明此处为主要原因所在。\n通过上述维度进行初步拆分，可以大致定位数据异常范围。\n4. 外部原因分析外部：外部原因分析可以根据PEST（政治、经济、社会、技术）模型进行分析。\n\n5. 内部原因分析通过初步分析定位范围之后，需要进行进一步的排查，一般从三个维度来分析：产品、技术、运营；可以和这几个人一起拉一个会讨论一下。同时应注意数据统计口径是否发生变化。\n\n6. 总结所以我们整个的分析流程大致为：\n先结合以往数据异常进行假设 —— 在一个假设得到验证之后 —— 从不同维度进行拆解，确定异常范围 —— 从产品、运营、技术侧逐一排查，最终找到原因。\n以上分析框架不仅局限于DAU波动，对于数据异常类问题即可根据以上模型进行分析。\n","tags":["数据分析"]},{"title":"数据分析｜DAU预测","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9CDAU%E9%A2%84%E6%B5%8B/","content":"日活会受到很多因素的影响，产品迭代，运营活动，推广的变化等等都会影响到日活。当然这些因素中，有的影响较小，有的暂时无法预估。因此在预测的过程中，我们可以将一些影响不大的因素，剔除出去，从而简化得到一个可计算的状态。（这个简化到可计算的过程中，其实就叫数学建模。）\n因此为了计算，我们首先构建日活的一个简单数学模型。\n建立日活的数学模型影响日活的因素中，最本质的其实是两个，一个是每日新增用户数，一个是新增用户的留存率。\n某一天的日活，我们可以看作是，当天的新增，加上前一天的新增的次日留存用户，再加上大前天的新增的二日留存用户……\n以此类推，我们可以认为日活是“当天的新增用户和此前每一天新增用户在当天的留存用户之和”，基于此，我们可以用一个很简单的公式表达日活。\n$DAU(n)&#x3D;A(n)+A(n-1)R(1)+A(n-2)R(2)+… …+A(1)R(n-1)$\n其中，DAU(n)为第n天的日活，A(n)为第n天的新增，R(n-1)为新增用户在第n-1天后的留存率。如果我们假设，每日用户的新增是一个固定的数值A，则公式可简写为：\n$DAU(n)&#x3D;A(1+R(1)+R(2)+… …+R(n-1))$\n上述公式可以看成是日活的一个简单的数学模型。从这个模型中，我们可以看出，新增A是一个较为确定的数值，另一部分：\n$1+R(1)+R(2)+… …+R(n-1)$\n留存之和的确定稍微有些麻烦。可以用下述的方法，预估留存。\n如何预估留存留存率是一个产品最为核心的指标了，下图是一个产品的留存率衰减曲线：\n\n（1-30日留存率衰减曲线）\n由图中，我们可以看出：留存率的衰减曲线，非常类似幂函数的曲线，其实，在业内绝大部分产品的留存衰减曲线，基本都是符合幂函数曲线。\n基于此，我们可以通过幂函数来近似拟合留存率的衰减曲线，也就可以顺利的预估出日活模型中需要的留存之和。\n一般在预估一个产品的留存之前，我们会有一些先验的数据基础，如果你的产品已经上线来一段时间，可以使用历史数据作为基础。如果产品还未上线，没有历史的数据，因为不同类型产品的留存和衰减速度都不太一样，因此可以用业内同类型的产品的大概留存数据作为拟合预测的参考。\n因此，留存曲线拟合基本会遇到两种情况：\n\n已经知道了若干天的留存，预估后续的留存？\n不知道具体每天的留存，只知道次留，周留，月留存等数据，预估每一天的留存。\n\n这两个情况本质上属于同一个问题，这里以第二种情况为例，简单说下如何操作。曲线拟合的方法有很多，这里我介绍一个最为简单的方式，就是利用excel来做一个简单的拟合计算，具体步骤如下：\nstep1假设我们知道了一个产品次日留存，7日留存，30留存如下：\n\n（某产品若干日留存）\nstep2在excel中按照对应留存天数，写出留存率，并画出散点图：\n\n（留存散点图）\nstep3在excel图表对上述散点添加趋势线，并在趋势线选项中，选择幂函数，并选择显示幂函数公式\n\n（基于散点拟合曲线）\n得到的幂函数为：\n$y&#x3D;0.4861*x^(-0.435)$，其中x为对应的天数，y为对应天数的留存率。\nstep4基于得到的幂函数公式，可以求的所有对应天数的留存率。\n计算得到预估的日活基于得到的幂函数，算出对应的留存率之后，就可以简单求和得到，带入日活公式中：\n$DAU(n)&#x3D;A(1+R(1)+R(2)+… …+R(n-1))$\n这样就可以通过预估的每日新增，得到在未来第n天日活所处的水平。\n总结&amp;后记以上通过简化日活模型，仅考虑新增和留存对于日活的影响（其它影响，也都是通过新增和留存间接对日活产生影响的），可以粗略的估算出未来一段时间的产品日活规模。\n进而通过日活的规模，再去估算一些潜在收益，以及运营成本等等数据。上述计算一定存在误差，并且不能满足所有的场景，但整体的思路可以作为参考，应该能搞解决大部分相关问题了。\n","tags":["数据分析"]},{"title":"数据分析｜指标体系搭建","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB%E6%90%AD%E5%BB%BA/","content":"什么是指标体系当我们把多个不同的指标有规律，有体系的组织在一起共同去量化人口现状时，它们就成了一套指标体系。\n指标体系是指标与体系的结合体，是一套从多个维度拆解业务现状并有系统有规律的组合起来的多个指标。也就是说单个指标只能叫指标，多个有一定规律，内部有一定关联的指标的组合才能叫做指标体系。\n指标体系的作用指标体系的功能大致可以分为三点，监控-效率-应急\n第一，指标体系是一套标准化的衡量指标，可以监控业务的发展情况；\n第二，指标体系可以形成标准化的体系模版，并且可以固化下来以减少重复的工作；\n第三，如果业务出现问题，我们可以通过指标拆解，迅速定位业务问题，给出业务优化方向。\n\n构建一套指标体系需要注意的问题（1）数据提前埋点\n对于互联网公司而言，数据产生于用户行为，用户行为都是通过埋点促发而记录，所以要获得相应的用户数据就得先埋好点。\n（2）统一指标口径\n统一指标计算口径也是很重要的步骤，统一的计算口径可以使得业务具有横向和纵向的可比性，所以需要在统一整套指标体系的最小计算单位，不同的计算口径计算出的数据也会略有差异。\n（3）指标穷尽且相互独立，遵循MCEC原则\n对于某一块业务我们需要下钻和拆解，下钻维度和拆解维度需要相互穷尽且完全独立，也就是麦肯锡提出的MCEC原则，样才能更好的暴露业务存在问题。MCEC原则也会在指标体系构建方法中详细介绍。\n如何构建一套指标体系数据指标体系建设的方法可以总结为三个步骤，即明确业务目标，梳理产品生命周期及用户行为路径以及指标分层治理，在这三个步骤当中又涉及到 OSM(Object,Strategy,Measure), AARRR(Acquisition,Activation,Retention,Revenue,Referral), UJM(User, Journey, Map), MECE (Mutually Exclusive, Collectively Exhaustive) 四个模型，这四个模型是指导我们构建完整而清晰的指标体系的方法论。\n明确业务目标：OSM模型OSM模型是 Object, Strategy, Measure的缩写。\n1. 在建立数据指标体系之前，一定要清晰的了解业务目标，也就是模型中的O,Object。换句话说，业务的目标也就是业务的核心KPI，了解业务的核心KPI能够帮助我们快速理清指标体系的方向。\n2. 了解业务目标方向之后，就需要制定相应的行动策略，也就是模型中的S,Strategy。行动策略的制定可以根据产品生命周期或者用户行为路径进行拆解，也就是把业务的核心KPI拆解到产品生命周期(AARRR)或者用户行为路径(UJM)当中，在整条链路当中分析可以提升核心KPI的点。\n3. 最后，就需要制定较细的评估指标，也就是模型中的M,Measure。评估指标的制定是将产品链路或者行为路径中的各个核心KPI进行下钻细分，这里用到的方法就是麦肯锡著名的MECE模型，需保证每个细分指标是完全独立且相互穷尽的。\n总结一下OSM模型的内容及其与AARRR,UJM,MECE模型之间的关系，OSM模型是指标体系建设的指导思想，理解业务KPI是OSM模型的核心；制定行动策略是实现业务KPI的手段，而AARRR和UJM模型是实现策略制定的方法论；制定细分指标是评估业务策略优劣的方法，而MECE模型制定细分指标的方法论。\n\n清理产品生命周期及用户行为路径：AARRR&#x2F;UJMAARRR和UJM模型都是路径模型，二者原理相似，只是它们出发的角度不一样。AARRR模型是从产品角度出发，揭示产品的整个生命周期；而UJM模型是从用户出发，揭示用户的行为路径。\nAARRR模型是基于产品角度，简单地来说就是拉新，促活，留存，付费，推广。对于一款产品来说，我们首先要从各个渠道获取用户；其次需要激活这些用户并让他们留存下来；对于留存下来的用户引导他们付费以及推广产品。\nUJM模型则是从用户角度出发，描述了用户进入产品的整个路径流程，即注册，登陆，加购，购买，复购链路流程。\n无论是产品角度还是用户角度进行链路流程，核心KPI都可以下钻到相应的节点，这样我们就在整条链路流程当中拆解了业务的核心KPI。这样的好处是，我们可以从更多的角度和维度监控和分析业务问题。\n\n指标体系分层治理：MECE前面两个步骤，首先我们明确了业务核心目标；其次，我们将业务核心的KPI下钻到产品生命周期或者用户路径行为中；接下来我们需要对这些核心KPI向下进行三到五层的拆解，这个过程我们称为指标体系分级治理，用到的模型是MECE模型。\nMECE模型的指导思想是完全独立，相互穷尽，根据这个原则拆分可以暴露业务最本质的问题，帮助数据分析师们快速地定位业务问题。例如，客户总成交额GMV进行以及拆解可以是付费用户数与平均客单价的乘积。\n\n以GMV为例，用三个步骤，四个模型教会你搭建指标体系的方法\n如果你的老板给出你一个很大的业务问题，他说，“我们现在做一套GMV相关的指标体系，你出一个方案吧！”面对这么大的一个命题，我们就需要对命题进行分解，将其分解成若干个子问题并找到各个子问题之间的联系，做成一套业务监控指标体系，帮助数据分析师快速定义业务问题。在这里，我们就通过上面提到的三个步骤，四个模型去搭建GMV相关的指标体系。\n\n\n第一步，根据OSM模型构建整体框架，明确业务目标。\n为什么业务会关注GMV？当然这是业务的核心KPI，关系到自己的饭碗，GMV当然越高越年终奖越高。所以，作为数据分析师我们提炼出业务目标——提升用户总成交量GMV。\n第二步，根据AARRR或UJM模型拆解用户达成GMV的路径，将业务目标转换为提升用户路径转化率。\n用户达成GMV需要通过六个步骤，即注册-登录-曝光-点击-加购-成交。到目前为止，我们已经将提升GMV这个目标转换为提升用户付费路径的转化率，只要我们提升用户每一步的基数，使得每一步的转化率变高就可以达成提高GMV的目标。\n将提升GMV转化为提高用户达成GMV路径转化率还有另外一个好处，即通过路径拆解能够暴露业务更多的问题，同时，分析师可以根据暴露的业务问题提出相应的建议方案，这也是数据分析师的价值所在。\n第三步，根据MECE模型对GMV达成路径的每一个指标进行拆解，实现指标分级治理。\n有了GMV达成路径之后，我们就可以将这个路径的核心步骤抽象成GMV的分级指标并进行回溯下钻。同时，找出影响每一个步骤的关键因素作为二级指标，每一个关键因素之间需要完全独立，相互穷尽。\n我们先根据公式1：\n$GMV&#x3D;成交用户数*平均客单价$\n\n\n这里将核心KPI用户总成交量GMV进行了一级拆解。\n又有公式2：\n$成交用户数&#x3D;点击UV*访购率$\n将公式2带入公式1得到：\n$GMV&#x3D;点击UV访购率平均客单价$\n\n\n又有公式3：\n$点击UV&#x3D;曝光UV*转化率$\n将公式3带入公式1得到：\n$GMV&#x3D;曝光UV转化率访购率*平均客单价$\n\n\n到这里呢，我们已经将核心KPI用户总成交量GMV进行三级回溯拆解，形成了分级治理的指标体系。到这里并没有结束，像曝光UV等着指标还可以继续向下拆解，例如，谷歌渠道曝光UV，华为渠道曝光UV等等，可以根据具体的工作场景进行适当的调整和向下拆解。\n讲到这里你可能会有几个问题。\n问题1：指标分级治理拆这么细有什么用？\n正向作用：分解核心KPI，明确每一个步骤的行动计算和每个行动考核指标。\n例如，老板让你估算明年GMV，就可以根据历史数据运用这套指标体系对明年的GMV进行估算。\n再例如，老板让你下个月做到1个亿的GMV，让你出个方案。这是就可以再对曝光UV进行细分，把量拆解到每一个渠道上去。\n反向作用：当业务出现问题，可以通过指标体系反向排查业务问题。\n例如，这个月的GMV下降了10%，老板让你排查下问题在哪里。这时候就可以根据这套指标体系逐一排查问题，定位到是哪个步骤，哪个环节出现问题，并提出相应的解决策略。\n问题2：在运用MECE模型进行指标体系分级治理时，是不是拆的越细越好，越全越好？\n当然不是，在进行MECE拆解时，需要找到与核心指标有重要关联关系的子集进行拆解分类，这样才能保证指套指标体系能够指导业务进行决策分析，帮助数分定位业务问题！\n","tags":["数据分析"]},{"title":"数据分析｜数据埋点设计","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E6%95%B0%E6%8D%AE%E5%9F%8B%E7%82%B9%E8%AE%BE%E8%AE%A1/","content":"六个步骤实现数据埋点设计1.确认事件与变量\n这里的事件指产品中的功能或者用户的操作，而变量是指描述事件的属性或者关键指标。确认事件与变量可以通过AARRR模型或者UJM模型进行逐步拆解，理清用户生命周期和行为路径，抽象出每一个步骤的关键指标。\nTips：AARRR模型和UJM模型会在之前的文章中有讲过，点击阅读原文即可跳转。\n\nAARRR 模型\n\nUJM 模型\n\n\n2.明确事件的触发时机\n不同的触发时机代表不同的计算口径，因此触发时机是影响数据准确的重要因素。以用户付款为例，是以用户点击付款界面作为触发条件，还是以付款成功作为触发条件进行埋点呢？二者口径不同，数据肯定会有一定差异，因此明确事件触发条件非常重要。\n而在用户付款这个例子中，我们建议使用两个字段记录用户付款行为，一个字段记录点击付款界面这个行为，另一个字段记录是否付款成功。\n3.明确事件的上报机制\n不同的上报机制也是数据准确性的重要影响因素之一，客户端上报数据可能会由于网络原因出现丢包的情况，前面章节已经详细介绍过，这里就不在赘述上报机制之间的异同。而作为数据分析师，在完成埋点工作的时候也需要确定数据是实时上报还是异步上报，以确定埋点是否合理，并及时调整数据埋点方案。\n4.设计数据表结构\n统一的数据表结构，方便团队内部进行数据的管理和复用，建议团队内部形成一套统一的数据结构规范。例如，将表分为不同的层级，第一层记录用户的基础信息，包括id,地区,昵称等；第二层记录玩家行为信息。\n5.统一字段命名规范\n有了统一的数据表结构档案还是不够的，统一数据命名规范数据埋点工作的重要一环。确保同一变量在所有的数据表当中都用统一的字段，比如消费金额这个字段，我们希望所有的表只要出现消费金额都用Amount字段，不要出现money,pay等其他字段。\n建立公司内部或者团队内部的命名规范是非常必要的，可以采用「动词+名词」或者「名词+动词」的规则来命名，比如「加入购物车」事件，就可以命名为：addToCart。\n6.明确优先级\n数据埋点都是为数据应用做铺排，埋点之后分析师可能面临着搭建指标体系和数据报表体系的工作，可以根据报表的优先级、埋点的技术实现成本以及资源有限性为数据埋点确定优先级。\n以电商购物成交转化为例实现数据埋点设计\n（1）通过UJM模型拆分用户购买商品的路径：将用户购买路径拆解为注册-登录-商品曝光-商品点击-浏览页面详情-加入购物车-生成订单-订单支付步骤，根据产品或策划提的数据需求，确定每一个步骤学要看哪些字段才能实现数据需求。\n（2）确认触发机制：明确是在点击按钮时记录行为还是在用户完成该步骤时记录行为。\n（3）确认上报机制：明确数据上报机制，是实时上报还是异步上报，不同的上报机制采集到的字段可能不一样，或者说需要将字段拆分到不同表进行记录。\n（4）统一字段名：业务内同一变量在所有的数据表当中都用统一的字段，例如，用户编号用account_id,用户所属国家用region,用户所属地区用ip_region等等。\n（5）统一表层级结构：我们这里采用两层数据表结构，第一层存放用户基信息，第二层存放用户行为信息。这个根据团队内部的数据接入规范进行调整，只要是统一的结构，对于数据分析师的分析都是有利的。\n（6）明确数据优先级：根据埋点需求的紧急程度，给每一个买埋点任务标上优先级。\n根据上面的六个步骤，将每一个步骤需要记录的字段按照标准格式汇总到文档，即可完成初步的埋点设计。当然完成初版埋点设计之后，还需要与产品、策划、程序一遍一遍过文档内容，不断修改完善，直至三方会谈达成统一意见。\n\n","tags":["数据分析"]},{"title":"数据分析｜数据指标口径不统一","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E5%8F%A3%E5%BE%84%E4%B8%8D%E7%BB%9F%E4%B8%80/","content":"\nCase1: 统一指标在不同系统之间标口径不一致改进方方案是将各个系统都需要用的复合指标，统一由数据中台进行加工，并统一向外提供消费，同时数据中台需要确保指标的准确性、及时性，最好能提供自我质量检查机制，无需消费者自己再检查。\nCase2: 中间表的建立和维护\n目的：依托于基础的埋点表&#x2F;业务明细表&#x2F;用户明细表等，来提高数据提取与输出的效率。\n作用：在[dws层 (data warehouse service，汇总层，按照业务划分来提供后续的业务查询、数据分发等) ][https://blog.csdn.net/pmdream/article/details/113601956]，将**基础事实层**的数据，通过**与产品人员既定的规则**编写，用”case when”或syscode系统码表来将业务口径标签化，将数据固定在对应主题域中，方便日常的汇报或者用户分析使用。\n\n\n引自：https://zhuanlan.zhihu.com/p/145226500\n\n供给端创建一张中间表，将所有功能数据存储，对于有关需求可以随取随用。\n\n消耗端运营人员除了关注整体的文章阅读外，还会关注各个入口的转化情况，数据如下：\n\n与产品&#x2F;运营人员，开会商议，确定每个来源入口的规则，对埋点表进行处理，用case when使不同的来源入口标签化：\n\n总结通过建立中间表，确实可以提高数据的输出效率，但是光建立是远远不够的，后期的维护也同样重要。\n由于规则是与产品&#x2F;运营商定的，必须形成规范的数据字典文档，存储在公共平台上（wiki、SVN等），有改动或者新增，需及时调整。\n有新增的业务时，也要在发版之后，添加至中间表中，避免数据遗漏或者重跑。\nCase3: 可以彻底解决数据指标口径不统一的问题吗？\n参考：https://coffee.pmcaff.com/article/3203006749265024?newwindow=1#:~:text=指标口径不统一的,的把数据用起来%E3%80%82\n\nCase4: 如何在公司内部统一数据指标口径第一，规范数据埋点，先统一数据埋点的最小维度记录方式等，做好这一步是统一数据口径的基础，会为之后口径的统一提供极大便利；其次是，构建数据字典，定义每一个指标的最小维度和统计口径；然后就是通过数据指标体系等进行数据指标的展示。\n当然这只是数据层面的，对口径的统一还需要和各方沟通达成统一的认知。\n制定数据埋点规范","tags":["数据分析"]},{"title":"数据分析｜数据看板","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E6%95%B0%E6%8D%AE%E7%9C%8B%E6%9D%BF/","content":"什么是数据看板数据看板一般用作后台系统的首页，主要呈现公司当前业务相关或运营管理相关数据和图表，方便公司内部人员实时了解公司内情，掌握业务发展情况，并能够对数据变化做出业务决策。\n谁使用\n数据看板的应用场景\n应用场景 1：监控\n\n监控是数据看板主流的应用场景。通过看板大屏，公司可以实时获取数据，了解商业进程，洞察发展趋势，甚至发布业务预警。\n\n应用场景 2：分析\n\n数据看板需要具备下沉细节的能力，在实际数据与项目预期不一致时，帮助业务部门分析导致异常的细节点、直击核心问题。\n\n应用场景 3：协作\n\n在发现数据问题、找到数据原因后，公司需要采取行动解决问题。\n数据看板的搭建（4）主要分为以下四步：\n第一步，明确需求。搭建看板有三问：一问使用者的业务需求是什么？二问业务目标是什么？三问如何达到业务目标？\n清楚问题答案后，才能明确看板需求，聚焦具体的商业问题。无论是自建看板还是为他人搭建看板，首先需要明确需求。\n第二步，需求分析。在这一步，我们需要拆解业务需求目标，选择合适的维度将其抽象为数据指标体系，确定看板基础内容。\n第三步，可视化。可视化是创建过程的核心环节。可视化图表需要准确表达数据信息，并通过有序组合排列清晰传递业务事实。这里建议采用议论文的写作方法：重要信息在前，佐证数据在后。\n第四步，评估效果。完成基础搭建工作后，我们需要关注如下问题：\n\n看板是否只有一屏幕（最多不超过 1.5 个屏幕）？\n看板使用者能否通过看板完整讲述业务故事？\n创建的看板能否帮助使用者迅速发现趋势、规律和异常？\n\n","tags":["数据分析"]},{"title":"数据分析｜需求沟通","url":"/2023/03/08/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%EF%BD%9C%E9%9C%80%E6%B1%82%E6%B2%9F%E9%80%9A/","content":"\n参考：https://zhuanlan.zhihu.com/p/363896505\n\n\n需求沟通\n需求收集（5+2）\n明确：为什么、做什么、谁做、何时、何地、如何、多少。\n\n需求分析（8）\n\n需求产生的背景和原因\n需求的目的和程度（最好可以量化）\n使用人员和场景\n会影响到哪些业务领域\n统计指标和维度\n展现形式\n数据源\n交付形式和标准\n\n\n目标对齐\n梳理整个流程，把整个流程中的关键环节都列出来，这是其实运用了MVC（最小可行性成品）的思想，每个环节都和业务同学碰一下，保持信息共享，确保需求的进行没有跑偏就好，即使有问题影响也不大，及时发现进行调整。\n\n信息同步\n整个流程一定要数字化，保持信息畅通无阻。信息共享真的很重要，让业务同学清晰看见自己需求的进度，进行得如何了，会让业务同学具有安全感。\n\n\n","tags":["数据分析"]}]